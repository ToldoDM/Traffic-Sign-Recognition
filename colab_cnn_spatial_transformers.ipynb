{
  "cells": [
    {
      "metadata": {
        "id": "53bb43bad4c869b9"
      },
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "id": "53bb43bad4c869b9"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U albumentations"
      ],
      "metadata": {
        "id": "P_5LiGFJ3Akj",
        "outputId": "23d9927f-015f-45f0-9692-9698537266ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "P_5LiGFJ3Akj",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.3.1)\n",
            "Collecting albumentations\n",
            "  Downloading albumentations-1.4.7-py3-none-any.whl (155 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.11.4)\n",
            "Collecting scikit-image>=0.21.0 (from albumentations)\n",
            "  Downloading scikit_image-0.23.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.11.0)\n",
            "Collecting scikit-learn>=1.3.2 (from albumentations)\n",
            "  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.7.1)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.9.0.80)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.18.2)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (3.3)\n",
            "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (9.4.0)\n",
            "Collecting imageio>=2.33 (from scikit-image>=0.21.0->albumentations)\n",
            "  Downloading imageio-2.34.1-py3-none-any.whl (313 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.5/313.5 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (2024.5.3)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (24.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.2->albumentations) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.2->albumentations) (3.5.0)\n",
            "Installing collected packages: imageio, scikit-learn, scikit-image, albumentations\n",
            "  Attempting uninstall: imageio\n",
            "    Found existing installation: imageio 2.31.6\n",
            "    Uninstalling imageio-2.31.6:\n",
            "      Successfully uninstalled imageio-2.31.6\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: scikit-image\n",
            "    Found existing installation: scikit-image 0.19.3\n",
            "    Uninstalling scikit-image-0.19.3:\n",
            "      Successfully uninstalled scikit-image-0.19.3\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 1.3.1\n",
            "    Uninstalling albumentations-1.3.1:\n",
            "      Successfully uninstalled albumentations-1.3.1\n",
            "Successfully installed albumentations-1.4.7 imageio-2.34.1 scikit-image-0.23.2 scikit-learn-1.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d0a3a4e-da5d-46de-f808-fac4cff1a7de",
        "ExecuteTime": {
          "end_time": "2024-05-15T14:41:46.905296Z",
          "start_time": "2024-05-15T14:41:37.103047Z"
        }
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torch.nn.functional as F\n",
        "import torchvision.utils as vutils\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision.models import ResNet152_Weights, ResNet50_Weights\n",
        "import random\n",
        "import time\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install split-folders\n",
        "import splitfolders\n",
        "import data_augmenter"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting split-folders\n",
            "  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.5.1\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_classes_preds(images, labels, preds, probs):\n",
        "    # plot the images in the batch, along with predicted and true labels\n",
        "    fig = plt.figure(figsize=(15, 5))\n",
        "    for idx in np.arange(4):\n",
        "        ax = fig.add_subplot(1, 4, idx + 1, xticks=[], yticks=[])\n",
        "        norm_img = cv2.normalize(images[idx].cpu().numpy(), None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
        "        rgb_img = np.transpose(norm_img, (1, 2, 0)).astype(np.uint8)\n",
        "        plt.imshow(rgb_img)\n",
        "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
        "            preds[idx],\n",
        "            probs[idx] * 100.0,\n",
        "            labels[idx]),\n",
        "            color=(\"green\" if preds[idx] == labels[idx].item() else \"red\"))\n",
        "    return fig"
      ],
      "metadata": {
        "id": "6CCfxseGNAtf",
        "ExecuteTime": {
          "end_time": "2024-05-15T14:41:46.920925Z",
          "start_time": "2024-05-15T14:41:46.905296Z"
        }
      },
      "id": "6CCfxseGNAtf",
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess"
      ],
      "metadata": {
        "id": "9ax_0x5Pn-MF"
      },
      "id": "9ax_0x5Pn-MF"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-15T14:41:46.936547Z",
          "start_time": "2024-05-15T14:41:46.920925Z"
        },
        "id": "ba26bd6e55fb8bae"
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(123)  # for replication"
      ],
      "id": "ba26bd6e55fb8bae",
      "outputs": [],
      "execution_count": 7
    },
    {
      "metadata": {
        "id": "fe6a3582e4f42eab"
      },
      "cell_type": "markdown",
      "source": [
        "## Download the dataset"
      ],
      "id": "fe6a3582e4f42eab"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc01ba7b79c1896f",
        "outputId": "f48b6977-9a2a-4516-bd40-d40feb517037",
        "ExecuteTime": {
          "end_time": "2024-05-15T14:41:46.967799Z",
          "start_time": "2024-05-15T14:41:46.936547Z"
        }
      },
      "cell_type": "code",
      "source": [
        "def download_file(url, file_name):\n",
        "    if not os.path.exists('dataset/' + file_name):\n",
        "        with urllib.request.urlopen(url) as response, open('dataset/' + file_name, 'wb') as out_file:\n",
        "            content_length = int(response.headers['Content-Length'])\n",
        "            with tqdm(total=content_length, unit='B', unit_scale=True, desc=url.split('/')[-1]) as pbar:\n",
        "                while True:\n",
        "                    chunk = response.read(1024)\n",
        "                    if not chunk:\n",
        "                        break\n",
        "                    out_file.write(chunk)\n",
        "                    pbar.update(len(chunk))\n",
        "    else:\n",
        "        print(f\"{file_name} already exists.\")\n",
        "\n",
        "\n",
        "os.makedirs(\"dataset/\", exist_ok=True)\n",
        "# Training\n",
        "download_file('https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Training_Images.zip',\n",
        "              'GTSRB_Final_Training_Images.zip')\n",
        "# Testing\n",
        "download_file('https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_Images.zip',\n",
        "              'GTSRB_Final_Test_Images.zip')\n",
        "# Ground truth\n",
        "download_file('https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_GT.zip',\n",
        "              'GTSRB_Final_Test_GT.zip')"
      ],
      "id": "dc01ba7b79c1896f",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GTSRB_Final_Training_Images.zip: 100%|██████████| 276M/276M [00:23<00:00, 11.5MB/s]\n",
            "GTSRB_Final_Test_Images.zip: 100%|██████████| 89.0M/89.0M [00:06<00:00, 13.4MB/s]\n",
            "GTSRB_Final_Test_GT.zip: 100%|██████████| 99.6k/99.6k [00:00<00:00, 84.7MB/s]\n"
          ]
        }
      ],
      "execution_count": 8
    },
    {
      "metadata": {
        "id": "f905d089f305e5b7"
      },
      "cell_type": "markdown",
      "source": [
        "## Extract zip files"
      ],
      "id": "f905d089f305e5b7"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3de5a631fc79d888",
        "outputId": "25561910-af83-4194-b8e2-b7ba40c8e6ab",
        "ExecuteTime": {
          "end_time": "2024-05-15T14:43:15.635062Z",
          "start_time": "2024-05-15T14:41:46.967799Z"
        }
      },
      "cell_type": "code",
      "source": [
        "def extract_file(file_name):\n",
        "    with zipfile.ZipFile(f\"dataset/{file_name}\", 'r') as zip_ref:\n",
        "        file_list = zip_ref.namelist()\n",
        "        with tqdm(total=len(file_list), desc=\"Extracting\") as pbar:\n",
        "            for file in file_list:\n",
        "                zip_ref.extract(file, 'dataset/')\n",
        "                pbar.update(1)\n",
        "\n",
        "\n",
        "extract_file('GTSRB_Final_Training_Images.zip')\n",
        "extract_file('GTSRB_Final_Test_Images.zip')\n",
        "extract_file('GTSRB_Final_Test_GT.zip')"
      ],
      "id": "3de5a631fc79d888",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting: 100%|██████████| 39299/39299 [00:41<00:00, 957.82it/s] \n",
            "Extracting: 100%|██████████| 12635/12635 [00:07<00:00, 1675.69it/s]\n",
            "Extracting: 100%|██████████| 1/1 [00:00<00:00, 166.49it/s]\n"
          ]
        }
      ],
      "execution_count": 9
    },
    {
      "metadata": {
        "id": "323d7c1bb67c1550"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading CSV file"
      ],
      "id": "323d7c1bb67c1550"
    },
    {
      "metadata": {
        "id": "468db0ee5cdf9840",
        "ExecuteTime": {
          "end_time": "2024-05-15T14:43:15.697103Z",
          "start_time": "2024-05-15T14:43:15.638064Z"
        }
      },
      "cell_type": "code",
      "source": [
        "#IMAGES: './dataset/GTSRB/test_images'\n",
        "#CSV ANNOTATIONS: './dataset/GTSRB/test_images/GT-final_test.csv'\n",
        "def csv_loader(csv_path):\n",
        "    data = np.loadtxt(csv_path,\n",
        "                      delimiter=\";\", dtype=str, skiprows=1)\n",
        "    return data\n",
        "\n",
        "\n",
        "#You should download the testset ('GTSRB_Final_Test_Images.zip') from the website which contains only the images\n",
        "#Then you have to download the ground truth csv ('GTSRB_Final_Test_GT.zip') from the website and paste it into the testset images folder\n",
        "annotations = csv_loader('./dataset/GT-final_test.csv')\n",
        "#sort the annotations\n",
        "annotations = annotations[:, [0, 7]]\n",
        "num_samples = len(annotations)\n",
        "#Column 0: filename - Column 1: classid\n",
        "annotations = annotations[annotations[:, 1].astype(int).argsort()]"
      ],
      "id": "468db0ee5cdf9840",
      "outputs": [],
      "execution_count": 10
    },
    {
      "metadata": {
        "id": "b8b6074543013fbb"
      },
      "cell_type": "markdown",
      "source": [
        "## Making training data accordingly"
      ],
      "id": "b8b6074543013fbb"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61a83a9b0d1825e4",
        "outputId": "390f7b65-c07e-43f9-8eb4-9757b2597850",
        "ExecuteTime": {
          "end_time": "2024-05-15T14:43:15.793018Z",
          "start_time": "2024-05-15T14:43:15.698991Z"
        }
      },
      "cell_type": "code",
      "source": [
        "def move_directories(source, destination):\n",
        "    if not os.path.exists(destination):\n",
        "        os.makedirs(destination)\n",
        "    # Get a list of all directories in the source directory\n",
        "    directories = [d for d in os.listdir(source) if os.path.isdir(os.path.join(source, d))]\n",
        "\n",
        "    # Move each directory to the destination\n",
        "    for directory in tqdm(directories):\n",
        "        source_path = os.path.join(source, directory)\n",
        "        destination_path = os.path.join(destination, directory)\n",
        "        shutil.move(source_path, destination_path)\n",
        "\n",
        "\n",
        "# Move directories with contents\n",
        "move_directories(\"./dataset/GTSRB/Final_Training/Images\", \"./dataset/GTSRB/train\")\n",
        "shutil.rmtree(\"./dataset/GTSRB/Final_Training\")"
      ],
      "id": "61a83a9b0d1825e4",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 43/43 [00:00<00:00, 17981.56it/s]\n"
          ]
        }
      ],
      "execution_count": 11
    },
    {
      "metadata": {
        "id": "5172c68f192d26ae"
      },
      "cell_type": "markdown",
      "source": [
        "## Making test data accordingly"
      ],
      "id": "5172c68f192d26ae"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df556df8aaf39e91",
        "outputId": "3fd32837-8e79-4ead-8e40-41a052b633a3",
        "ExecuteTime": {
          "end_time": "2024-05-15T14:43:22.009798Z",
          "start_time": "2024-05-15T14:43:15.796998Z"
        }
      },
      "cell_type": "code",
      "source": [
        "for class_id in tqdm(np.unique(annotations[:, 1]), desc='Class_ID'):\n",
        "    newpath = './dataset/GTSRB/test/' + class_id.zfill(5)\n",
        "    if not os.path.exists(newpath):\n",
        "        os.makedirs(newpath)\n",
        "    for image_filename in annotations[annotations[:, 1] == class_id]:\n",
        "        shutil.move('./dataset/GTSRB/Final_Test/Images/' + image_filename[0], newpath + '/' + image_filename[0])\n",
        "\n",
        "shutil.rmtree(\"./dataset/GTSRB/Final_Test\")"
      ],
      "id": "df556df8aaf39e91",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Class_ID: 100%|██████████| 43/43 [00:00<00:00, 43.98it/s]\n"
          ]
        }
      ],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge the dataset and then split into train and test"
      ],
      "metadata": {
        "id": "JWpVwjN6nUFH"
      },
      "id": "JWpVwjN6nUFH"
    },
    {
      "cell_type": "code",
      "source": [
        "def merge(source_folder, destination_folder):\n",
        "    # Create the destination folder if it doesn't exist\n",
        "    if not os.path.exists(destination_folder):\n",
        "        os.makedirs(destination_folder)\n",
        "\n",
        "    # Get the total number of files and directories in the source folder\n",
        "    total_items = sum([len(files) + len(dirs) for root, dirs, files in os.walk(source_folder)])\n",
        "\n",
        "    # Initialize tqdm to show progress\n",
        "    progress = tqdm(total=total_items, desc='Moving: ' + source_folder + ' --> ' + destination_folder, position=0, leave=True)\n",
        "\n",
        "    # Iterate over all files and subdirectories in the source folder\n",
        "    for root, dirs, files in os.walk(source_folder):\n",
        "        for item in files + dirs:\n",
        "            source_item = os.path.join(root, item)\n",
        "            destination_item = os.path.join(destination_folder, os.path.relpath(source_item, source_folder))\n",
        "\n",
        "            # If the item is a file, copy it to the destination folder\n",
        "            if os.path.isfile(source_item):\n",
        "                shutil.move(source_item, destination_item)\n",
        "            # If the item is a directory, create it in the destination folder\n",
        "            elif os.path.isdir(source_item):\n",
        "                os.makedirs(destination_item, exist_ok=True)\n",
        "\n",
        "            progress.update(1)  # Update progress bar\n",
        "\n",
        "    progress.close()  # Close tqdm\n",
        "\n",
        "def merge_folders(source_folders, target_folder):\n",
        "    for sf in source_folders:\n",
        "        merge(sf, target_folder)\n",
        "        shutil.rmtree(sf)\n",
        "\n",
        "# Temporary directory to store the merged dataset\n",
        "merged_dir = \"./dataset/GTSRB/merged\"\n",
        "\n",
        "merge_folders(['./dataset/GTSRB/train','./dataset/GTSRB/test'], merged_dir)\n",
        "\n",
        "# Training 70\n",
        "# Testing 30\n",
        "splitfolders.ratio(merged_dir, output=\"./dataset/GTSRB/plain\", seed=123, ratio=(.7,0, 0.3),move=False)\n",
        "\n",
        "# Clear temporary files\n",
        "shutil.rmtree('./dataset/GTSRB/plain/val')\n",
        "os.remove('./dataset/GTSRB/Readme-Images-Final-test.txt')\n",
        "os.remove('./dataset/GTSRB/Readme-Images.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "690Ap50knZxO",
        "outputId": "a7d79372-bd4d-47e4-dbd0-ad6dbfdfef0c",
        "ExecuteTime": {
          "end_time": "2024-05-15T14:46:41.857267Z",
          "start_time": "2024-05-15T14:43:22.012798Z"
        }
      },
      "id": "690Ap50knZxO",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Moving: ./dataset/GTSRB/train --> ./dataset/GTSRB/merged: 100%|██████████| 39295/39295 [00:08<00:00, 4455.07it/s] \n",
            "Moving: ./dataset/GTSRB/test --> ./dataset/GTSRB/merged: 100%|██████████| 12673/12673 [00:02<00:00, 5007.43it/s]\n",
            "Copying files: 51882 files [00:24, 2110.10 files/s]\n"
          ]
        }
      ],
      "execution_count": 13
    },
    {
      "metadata": {
        "id": "5650b2ca72f0f486"
      },
      "cell_type": "markdown",
      "source": [
        "## Add weather conditions to the merged dataset and then split into train and test"
      ],
      "id": "5650b2ca72f0f486"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-15T14:53:12.820581Z",
          "start_time": "2024-05-15T14:46:41.862270Z"
        },
        "id": "b0fb0b549a431a3d",
        "outputId": "c74bc912-9029-49f7-c235-ecf0321edfcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "da = data_augmenter.DataAugmenter(dataset_path='./dataset/GTSRB/')\n",
        "da.load_images(folder_to_load='merged')\n",
        "da.add_weather_effects(prob_per_class=0.5)\n",
        "\n",
        "# Training 70\n",
        "# Testing 30\n",
        "splitfolders.ratio(merged_dir, output=\"./dataset/GTSRB/weather\", seed=123, ratio=(.7,0, 0.3),move=True)\n",
        "\n",
        "# Clear temporary files\n",
        "shutil.rmtree('./dataset/GTSRB/weather/val')\n",
        "shutil.rmtree(merged_dir)"
      ],
      "id": "b0fb0b549a431a3d",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes found: , 00023, 00006, 00020, 00026, 00002, 00019, 00035, 00018, 00036, 00003, 00028, 00041, 00042, 00021, 00038, 00032, 00025, 00040, 00039, 00024, 00000, 00010, 00007, 00015, 00027, 00031, 00030, 00029, 00008, 00016, 00014, 00011, 00004, 00022, 00037, 00033, 00009, 00005, 00034, 00001, 00017, 00012, 00013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading classes: 100%|██████████| 43/43 [00:14<00:00,  2.98it/s]\n",
            "Add weather effects with probability 0.5:   9%|▉         | 4/43 [00:18<02:34,  3.96s/it]"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "6dcd860b78a2678d"
      },
      "cell_type": "markdown",
      "source": [
        "## Set dataset paths"
      ],
      "id": "6dcd860b78a2678d"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-15T14:53:44.718874Z",
          "start_time": "2024-05-15T14:53:44.700871Z"
        },
        "id": "20f8c403f5b816ee"
      },
      "cell_type": "code",
      "source": [
        "plain_train_dir = './dataset/GTSRB/plain/train'\n",
        "plain_test_dir = './dataset/GTSRB/plain/test'\n",
        "\n",
        "weather_train_dir='./dataset/GTSRB/weather/train'\n",
        "weather_test_dir='./dataset/GTSRB/weather/test'"
      ],
      "id": "20f8c403f5b816ee",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "d2e92b663416be6f"
      },
      "cell_type": "markdown",
      "source": [
        "# Parameters setup"
      ],
      "id": "d2e92b663416be6f"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-15T15:00:56.425984Z",
          "start_time": "2024-05-15T15:00:56.412471Z"
        },
        "id": "e9b3ab9f505b0242"
      },
      "cell_type": "code",
      "source": [
        "# Setting device for the computation\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Change this to select between plain or weather dataset\n",
        "# train_dir = plain_train_dir/weather_train_dir\n",
        "# test_dir = plain_test_dir/weather_test_dir\n",
        "train_dir = weather_train_dir\n",
        "test_dir = weather_test_dir\n",
        "\n",
        "# Hyperparameters\n",
        "hyperparams = {\n",
        "    \"num_epochs\": 15,\n",
        "    \"batch_size\": 64,\n",
        "    #optimizer\n",
        "    \"opt\": \"adam\",\n",
        "    \"learning_rate\": 1e-2,\n",
        "    \"betas\":(0.9,0.999),\n",
        "    \"eps\":1e-8,\n",
        "    \"weight_decay\": 0,\n",
        "    \"momentum\":0,\n",
        "    #scheduler\n",
        "    \"decay_rate\": 0.5\n",
        "}"
      ],
      "id": "e9b3ab9f505b0242",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the training set"
      ],
      "metadata": {
        "id": "q-bE0596WTkR"
      },
      "id": "q-bE0596WTkR"
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./models', exist_ok=True)\n",
        "\n",
        "# Define your transformations\n",
        "custom_cnn_transform = transforms.Compose([\n",
        "    transforms.Resize((48,48)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Load the dataset using the torchvision.datasets.ImageFolder\n",
        "train_dataset = datasets.ImageFolder(train_dir, transform=custom_cnn_transform)\n",
        "\n",
        "# Concatenate all images into a single tensor\n",
        "images = torch.stack([img for img, _ in train_dataset], dim=0)\n",
        "\n",
        "# Calculate mean and std across all images and channels\n",
        "mean = torch.mean(images, dim=(0, 2, 3))\n",
        "std = torch.std(images, dim=(0, 2, 3))\n",
        "\n",
        "# Define your transformations\n",
        "custom_cnn_transform = transforms.Compose([\n",
        "    transforms.Resize((48,48)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "# Load the dataset using the torchvision.datasets.ImageFolder\n",
        "train_dataset = datasets.ImageFolder(train_dir, transform=custom_cnn_transform)\n",
        "\n",
        "train_size = len(train_dataset)\n",
        "class_names = train_dataset.classes\n",
        "number_of_classes = len(train_dataset.classes)\n",
        "\n",
        "print('Train size:', train_size)\n",
        "print('Class names:', class_names)\n",
        "\n",
        "# Use numpy to count occurrences of each class index in targets\n",
        "train_samples_per_class = np.bincount(train_dataset.targets)\n",
        "\n",
        "# Compute class weights\n",
        "class_weights=(train_size/(number_of_classes*train_samples_per_class))\n",
        "class_weights=torch.tensor(class_weights,dtype=torch.float)\n",
        "\n",
        "# Create histogram\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(class_names, train_samples_per_class, color='blue')\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Number of images')\n",
        "plt.title('Number of Images per Class')\n",
        "plt.xticks(rotation=90)  # Rotate class names for better visibility if needed\n",
        "plt.yticks(np.arange(0, max(train_samples_per_class) + 10, 250))  # Adjust the range and step size as needed\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gCFlJvJYWQqc",
        "ExecuteTime": {
          "end_time": "2024-05-15T14:54:57.010142Z",
          "start_time": "2024-05-15T14:53:48.931280Z"
        }
      },
      "id": "gCFlJvJYWQqc",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the test set"
      ],
      "metadata": {
        "id": "mvleDPehWW4u"
      },
      "id": "mvleDPehWW4u"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset using the torchvision.datasets.ImageFolder\n",
        "test_dataset = datasets.ImageFolder(test_dir, transform=custom_cnn_transform)\n",
        "test_size = len(test_dataset)\n",
        "class_names = test_dataset.classes\n",
        "\n",
        "print('Test size:', test_size)\n",
        "print('Class names:', class_names)\n",
        "\n",
        "# Use numpy to count occurrences of each class index in targets\n",
        "counts = np.bincount(test_dataset.targets)\n",
        "\n",
        "# Create histogram\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(class_names, counts, color='blue')\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Number of images')\n",
        "plt.title('Number of Images per Class')\n",
        "plt.xticks(rotation=90)  # Rotate class names for better visibility if needed\n",
        "plt.yticks(np.arange(0, max(counts) + 10, 50))  # Adjust the range and step size as needed\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TcSvqT4AWaxe",
        "ExecuteTime": {
          "end_time": "2024-05-15T14:54:58.532147Z",
          "start_time": "2024-05-15T14:54:57.014156Z"
        }
      },
      "id": "TcSvqT4AWaxe",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the CNN"
      ],
      "metadata": {
        "id": "G7CE0M9SWeZj"
      },
      "id": "G7CE0M9SWeZj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN Plain"
      ],
      "metadata": {
        "id": "waE26jiDoZ32"
      },
      "id": "waE26jiDoZ32"
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Expected input as 48x48\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=200, kernel_size=7, stride=1, padding=2)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.local_norm = nn.LocalResponseNorm(size=5)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=200, out_channels=250, kernel_size=4, stride=1, padding=2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels=250, out_channels=350, kernel_size=4, stride=1, padding=2)\n",
        "\n",
        "        self.fc1= nn.Linear(in_features=350*6*6, out_features=400)\n",
        "\n",
        "        self.fc2= nn.Linear(in_features=400, out_features=43)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x= self.pool1(x)\n",
        "\n",
        "        x = self.local_norm(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.local_norm(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.local_norm(x)\n",
        "\n",
        "        # Flatten the output from conv1\n",
        "        x = x.view(-1, 350*6*6)\n",
        "\n",
        "        x= self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "r56vEoqhogaG"
      },
      "id": "r56vEoqhogaG",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN with 3 spatial transformers"
      ],
      "metadata": {
        "id": "6jKXkY8lft6q"
      },
      "id": "6jKXkY8lft6q"
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_ST(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Expected input as 48x48\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=200, kernel_size=7, stride=1, padding=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=200, out_channels=250, kernel_size=4, stride=1, padding=2)\n",
        "        self.conv3 = nn.Conv2d(in_channels=250, out_channels=350, kernel_size=4, stride=1, padding=2)\n",
        "\n",
        "        self.local_norm = nn.LocalResponseNorm(size=5)\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.fc1= nn.Linear(in_features=350*6*6, out_features=400)\n",
        "        self.fc2= nn.Linear(in_features=400, out_features=43)\n",
        "\n",
        "        # Spatial transformer block 1\n",
        "        self.loc1=nn.Sequential(\n",
        "            nn.MaxPool2d(2,2),\n",
        "            nn.Conv2d(in_channels=3, out_channels=250, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(in_channels=250, out_channels=250, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        self.fc_loc1 = nn.Sequential(\n",
        "            nn.Linear(250 * 6 * 6, 250),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(250, 6)\n",
        "        )\n",
        "        # Initialize the weights/bias with identity transformation\n",
        "        self.fc_loc1[2].weight.data.zero_()\n",
        "        self.fc_loc1[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
        "\n",
        "        # Spatial transformer block 2\n",
        "        self.loc2=nn.Sequential(\n",
        "            nn.MaxPool2d(2,2),\n",
        "            nn.Conv2d(in_channels=200, out_channels=150, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(in_channels=150, out_channels=200, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        self.fc_loc2 = nn.Sequential(\n",
        "            nn.Linear(200 * 2 * 2, 300),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(300, 6)\n",
        "        )\n",
        "        # Initialize the weights/bias with identity transformation\n",
        "        self.fc_loc2[2].weight.data.zero_()\n",
        "        self.fc_loc2[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
        "\n",
        "        # Spatial transformer block 3\n",
        "        self.loc3=nn.Sequential(\n",
        "            nn.MaxPool2d(2,2),\n",
        "            nn.Conv2d(in_channels=250, out_channels=150, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(in_channels=150, out_channels=200, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        self.fc_loc3 = nn.Sequential(\n",
        "            nn.Linear(200 * 1 * 1, 300),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(300, 6)\n",
        "        )\n",
        "        # Initialize the weights/bias with identity transformation\n",
        "        self.fc_loc3[2].weight.data.zero_()\n",
        "        self.fc_loc3[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
        "\n",
        "\n",
        "    def stn1(self,x):\n",
        "        xs = self.loc1(x)\n",
        "        xs = xs.view(-1, 250 * 6 * 6)\n",
        "        theta = self.fc_loc1(xs)\n",
        "        theta = theta.view(-1, 2, 3)\n",
        "\n",
        "        grid = F.affine_grid(theta, x.size(), align_corners=False)\n",
        "        x = F.grid_sample(x, grid, align_corners=False)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def stn2(self,x):\n",
        "        xs = self.loc2(x)\n",
        "        xs = xs.view(-1, 200 * 2 * 2)\n",
        "        theta = self.fc_loc2(xs)\n",
        "        theta = theta.view(-1, 2, 3)\n",
        "\n",
        "        grid = F.affine_grid(theta, x.size(), align_corners=False)\n",
        "        x = F.grid_sample(x, grid, align_corners=False)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def stn3(self,x):\n",
        "        xs = self.loc3(x)\n",
        "        xs = xs.view(-1, 200 * 1 * 1)\n",
        "        theta = self.fc_loc3(xs)\n",
        "        theta = theta.view(-1, 2, 3)\n",
        "\n",
        "        grid = F.affine_grid(theta, x.size(), align_corners=False)\n",
        "        x = F.grid_sample(x, grid, align_corners=False)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Spatial transformer 1\n",
        "        x = self.stn1(x)\n",
        "\n",
        "        # CNN block 1\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.local_norm(x)\n",
        "\n",
        "        # Spatial transformer 2\n",
        "        x = self.stn2(x)\n",
        "\n",
        "        # CNN block 2\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.local_norm(x)\n",
        "\n",
        "        # Spatial transformer 3\n",
        "        x = self.stn3(x)\n",
        "\n",
        "        # CNN block 3\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.local_norm(x)\n",
        "\n",
        "        # Flatten the output for dense layers\n",
        "        x = x.view(-1, 350*6*6)\n",
        "        x= self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "eAwI5trgfxqg",
        "ExecuteTime": {
          "end_time": "2024-05-15T14:55:03.193468Z",
          "start_time": "2024-05-15T14:55:03.146583Z"
        }
      },
      "id": "eAwI5trgfxqg",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training function"
      ],
      "metadata": {
        "id": "6cpZCj8mowXh"
      },
      "id": "6cpZCj8mowXh"
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(trained_model, test_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        trained_model.eval()\n",
        "        for data in test_loader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = trained_model(images)\n",
        "            softmax_outputs = F.softmax(outputs, dim=1)\n",
        "            _, predicted = torch.max(softmax_outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_accuracy = float(correct / total)\n",
        "    print('{} Model accuracy: {:.4f}'.format('Test phase - ', test_accuracy))\n",
        "    writer.add_scalar('Training/Test Accuracy', test_accuracy)\n",
        "    return test_accuracy\n",
        "\n",
        "\n",
        "def train_model(device, model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=25,\n",
        "                model_name='trained_model'):\n",
        "    since = time.time()\n",
        "    time_train = 0\n",
        "    time_val = 0\n",
        "\n",
        "    # Save the initial model\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('-' * 10)\n",
        "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Choose the appropriate data loader\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "                data_total_steps = len(train_loader)\n",
        "                data_loader = train_loader\n",
        "            else:\n",
        "                model.eval()  # Set model to evaluate mode\n",
        "                data_total_steps = len(val_loader)\n",
        "                data_loader = val_loader\n",
        "\n",
        "            for i, (images, labels) in enumerate(data_loader):\n",
        "                # time_t = epoch * len(data_loader) * i + i\n",
        "\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward\n",
        "                # Track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(images)\n",
        "                    softmax_outputs = F.softmax(outputs, dim=1)\n",
        "                    probs, preds = torch.max(softmax_outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Statistics\n",
        "                running_loss += loss.item() * images.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "                #prints the stats every 20 steps (20 batches performed)\n",
        "                if (i + 1) % int(data_total_steps / 8) == 0:\n",
        "                    print(\n",
        "                        f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{data_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "                    # Log image predictions\n",
        "                    selected_indices = random.sample(range(len(images)), 4)  # Select 4 random indices\n",
        "                    selected_images = images[selected_indices]\n",
        "                    selected_labels = labels[selected_indices]\n",
        "                    selected_preds = preds[selected_indices]\n",
        "                    selected_probs = probs[selected_indices]\n",
        "                    if phase == 'train':\n",
        "                        writer.add_figure('Training/Training Predictions',\n",
        "                                          plot_classes_preds(selected_images, selected_labels, selected_preds,\n",
        "                                                             selected_probs),\n",
        "                                          global_step=time_train)\n",
        "                    else:\n",
        "                        writer.add_figure('Training/Validation Predictions',\n",
        "                                          plot_classes_preds(selected_images, selected_labels, selected_preds,\n",
        "                                                             selected_probs),\n",
        "                                          global_step=time_val)\n",
        "\n",
        "                # Log scalars\n",
        "                if phase == 'train':\n",
        "                    writer.add_scalar('Training/Training Loss',\n",
        "                                      loss.item(),\n",
        "                                      time_train)\n",
        "                    writer.add_scalar('Policy/Learning Rate',\n",
        "                                      np.array(scheduler.get_last_lr()),\n",
        "                                      time_train)\n",
        "                    time_train += 1\n",
        "                else:\n",
        "                    writer.add_scalar('Training/Validation Loss',\n",
        "                                      loss.item(),\n",
        "                                      time_val)\n",
        "                    time_val += 1\n",
        "\n",
        "            epoch_loss = running_loss / len(data_loader.dataset)\n",
        "            epoch_acc = running_corrects.double() / len(data_loader.dataset)\n",
        "\n",
        "            if phase == 'train':\n",
        "                print('{} Epoch {} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                    'Train phase - ', epoch + 1, epoch_loss, epoch_acc))\n",
        "                writer.add_scalar('Training/Training Accuracy',\n",
        "                                  epoch_acc,\n",
        "                                  epoch)\n",
        "                if (epoch + 1) % max(int(num_epochs / 5), 1) == 0:  # checkpoint the model\n",
        "                    print(\"----> model checkpoint...\")\n",
        "                    torch.save(model, f'./models/trained_model_{model_name}_epoch_{epoch + 1}.pth')\n",
        "            else:\n",
        "                if epoch_acc > best_acc:\n",
        "                    best_acc = epoch_acc\n",
        "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                print('{} Epoch {} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                    'Validation phase - ', epoch + 1, epoch_loss, epoch_acc))\n",
        "                writer.add_scalar('Training/Validation Accuracy',\n",
        "                                  epoch_acc,\n",
        "                                  epoch)\n",
        "                #scheduler.step(epoch_loss)\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc:{:.4f}'.format(best_acc))\n",
        "    # Return the model with the best accuracy in the validation\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "RIiC4t_jWeBV",
        "ExecuteTime": {
          "end_time": "2024-05-15T14:55:07.319912Z",
          "start_time": "2024-05-15T14:55:07.295064Z"
        }
      },
      "id": "RIiC4t_jWeBV",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model setting"
      ],
      "metadata": {
        "id": "trtHqdVcWrbV"
      },
      "id": "trtHqdVcWrbV"
    },
    {
      "cell_type": "code",
      "source": [
        "# default `log_dir` is \"runs\" - we'll be more specific here\n",
        "model_name = 'CNN_Sgd_Weather'\n",
        "writer = SummaryWriter(f'runs/{model_name}')\n",
        "\n",
        "test_abs = int(len(train_dataset) * 0.8)\n",
        "train_subset, val_subset = random_split(\n",
        "    train_dataset, [test_abs, len(train_dataset) - test_abs])\n",
        "\n",
        "train_data_size = len(train_subset)\n",
        "\n",
        "# Create DataLoader instances for training and validation\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_subset,\n",
        "    batch_size=hyperparams[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    num_workers=0)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_subset,\n",
        "    batch_size=hyperparams[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    num_workers=0)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=hyperparams[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    num_workers=0)\n",
        "\n",
        "# Model initialization\n",
        "model = CNN().to(device)\n",
        "\n",
        "# Define loss function, optimizer, etc.\n",
        "class_weights=class_weights.to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "# Set optimizers\n",
        "if hyperparams['opt'] == 'sgd':\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=hyperparams[\"learning_rate\"],momentum=hyperparams[\"momentum\"], weight_decay=hyperparams[\"weight_decay\"])\n",
        "elif hyperparams['opt'] == 'adam':\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=hyperparams[\"learning_rate\"], betas=hyperparams[\"betas\"], eps=hyperparams[\"eps\"], weight_decay=hyperparams[\"weight_decay\"])\n",
        "elif hyperparams['opt'] == 'rmsprop':\n",
        "    optimizer = torch.optim.RMSprop(model.parameters(), lr=hyperparams[\"learning_rate\"])\n",
        "else:\n",
        "    raise ValueError('Invalid optimizer provided')\n",
        "scheduler = lr_scheduler.LinearLR(optimizer)"
      ],
      "metadata": {
        "id": "SpS12uKCWvbE",
        "ExecuteTime": {
          "end_time": "2024-05-15T15:01:04.887998Z",
          "start_time": "2024-05-15T15:01:04.673151Z"
        }
      },
      "id": "SpS12uKCWvbE",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "tzJPf39GWxB9"
      },
      "id": "tzJPf39GWxB9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "trained_model = train_model(device=device, model=model, criterion=criterion, optimizer=optimizer, scheduler=scheduler,\n",
        "                            train_loader=train_loader, val_loader=val_loader, num_epochs=hyperparams[\"num_epochs\"],\n",
        "                            model_name=model_name)\n",
        "\n",
        "ta = test_model(trained_model=trained_model, test_loader=test_loader)\n",
        "writer.add_hparams({key: str(value) if isinstance(value, list) else value for key, value in hyperparams.items()},\n",
        "                   metric_dict={'Training/Test Accuracy': ta})"
      ],
      "metadata": {
        "id": "nNs3JQVJWzfg",
        "jupyter": {
          "is_executing": true
        },
        "ExecuteTime": {
          "start_time": "2024-05-15T15:01:07.116914Z"
        }
      },
      "id": "nNs3JQVJWzfg",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the trained model"
      ],
      "metadata": {
        "id": "9NUtJO4nW1Iu"
      },
      "id": "9NUtJO4nW1Iu"
    },
    {
      "cell_type": "code",
      "source": [
        "print('Finished Training')\n",
        "torch.save(trained_model, f'./models/trained_model_{model_name}_final.pth')"
      ],
      "metadata": {
        "id": "XEYZMVC1W7mH"
      },
      "id": "XEYZMVC1W7mH",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r runs.zip runs"
      ],
      "metadata": {
        "id": "OL41g_-KDYWe"
      },
      "id": "OL41g_-KDYWe",
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}