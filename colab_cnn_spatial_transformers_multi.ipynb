{
 "cells": [
  {
   "metadata": {
    "id": "53bb43bad4c869b9"
   },
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "id": "53bb43bad4c869b9"
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -U albumentations\n",
    "!pip install split-folders"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P_5LiGFJ3Akj",
    "outputId": "3f140c27-1fc6-458e-f504-2e9ba18ac4ff",
    "ExecuteTime": {
     "end_time": "2024-06-27T19:14:39.402678Z",
     "start_time": "2024-06-27T19:14:36.518573Z"
    }
   },
   "id": "P_5LiGFJ3Akj",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (1.4.10)\r\n",
      "Requirement already satisfied: numpy<2,>=1.24.4 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from albumentations) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.10.0 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from albumentations) (1.13.0)\r\n",
      "Requirement already satisfied: scikit-image>=0.21.0 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from albumentations) (0.23.2)\r\n",
      "Requirement already satisfied: PyYAML in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from albumentations) (6.0.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from albumentations) (4.11.0)\r\n",
      "Requirement already satisfied: scikit-learn>=1.3.2 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from albumentations) (1.4.2)\r\n",
      "Requirement already satisfied: pydantic>=2.7.0 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from albumentations) (2.7.1)\r\n",
      "Requirement already satisfied: albucore>=0.0.11 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from albumentations) (0.0.12)\r\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from albumentations) (4.9.0.80)\r\n",
      "Requirement already satisfied: tomli>=2.0.1 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from albucore>=0.0.11->albumentations) (2.0.1)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from pydantic>=2.7.0->albumentations) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from pydantic>=2.7.0->albumentations) (2.18.2)\r\n",
      "Requirement already satisfied: networkx>=2.8 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (3.3)\r\n",
      "Requirement already satisfied: pillow>=9.1 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (10.3.0)\r\n",
      "Requirement already satisfied: imageio>=2.33 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (2.34.1)\r\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (2024.5.3)\r\n",
      "Requirement already satisfied: packaging>=21 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (24.0)\r\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (0.4)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from scikit-learn>=1.3.2->albumentations) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from scikit-learn>=1.3.2->albumentations) (3.5.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: split-folders in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (0.5.1)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e7310ded-f8e0-4409-856d-570280543fd0",
    "ExecuteTime": {
     "end_time": "2024-06-27T19:14:43.189456Z",
     "start_time": "2024-06-27T19:14:39.404127Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.models import ResNet152_Weights, ResNet50_Weights\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import splitfolders\n",
    "import data_augmenter"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_classes_preds(images, labels, preds, probs):\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx + 1, xticks=[], yticks=[])\n",
    "        norm_img = cv2.normalize(images[idx].cpu().numpy(), None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "        rgb_img = np.transpose(norm_img, (1, 2, 0)).astype(np.uint8)\n",
    "        plt.imshow(rgb_img)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            preds[idx],\n",
    "            probs[idx] * 100.0,\n",
    "            labels[idx]),\n",
    "            color=(\"green\" if preds[idx] == labels[idx].item() else \"red\"))\n",
    "    return fig"
   ],
   "metadata": {
    "id": "6CCfxseGNAtf",
    "ExecuteTime": {
     "end_time": "2024-06-27T19:14:43.194029Z",
     "start_time": "2024-06-27T19:14:43.190531Z"
    }
   },
   "id": "6CCfxseGNAtf",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocess"
   ],
   "metadata": {
    "id": "9ax_0x5Pn-MF"
   },
   "id": "9ax_0x5Pn-MF"
  },
  {
   "metadata": {
    "id": "ba26bd6e55fb8bae",
    "ExecuteTime": {
     "end_time": "2024-06-27T19:14:43.200194Z",
     "start_time": "2024-06-27T19:14:43.194904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(123)  # for replication"
   ],
   "id": "ba26bd6e55fb8bae",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "id": "fe6a3582e4f42eab"
   },
   "cell_type": "markdown",
   "source": [
    "## Download the dataset"
   ],
   "id": "fe6a3582e4f42eab"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dc01ba7b79c1896f",
    "outputId": "49b1ba35-3fdc-4b67-bff7-1ab2395cc55d",
    "ExecuteTime": {
     "end_time": "2024-06-27T19:14:43.206639Z",
     "start_time": "2024-06-27T19:14:43.201856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def download_file(url, file_name):\n",
    "    if not os.path.exists('dataset/' + file_name):\n",
    "        with urllib.request.urlopen(url) as response, open('dataset/' + file_name, 'wb') as out_file:\n",
    "            content_length = int(response.headers['Content-Length'])\n",
    "            with tqdm(total=content_length, unit='B', unit_scale=True, desc=url.split('/')[-1]) as pbar:\n",
    "                while True:\n",
    "                    chunk = response.read(1024)\n",
    "                    if not chunk:\n",
    "                        break\n",
    "                    out_file.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "    else:\n",
    "        print(f\"{file_name} already exists.\")\n",
    "\n",
    "\n",
    "os.makedirs(\"dataset/\", exist_ok=True)\n",
    "# Training\n",
    "download_file('https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Training_Images.zip',\n",
    "              'GTSRB_Final_Training_Images.zip')\n",
    "# Testing\n",
    "download_file('https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_Images.zip',\n",
    "              'GTSRB_Final_Test_Images.zip')\n",
    "# Ground truth\n",
    "download_file('https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_GT.zip',\n",
    "              'GTSRB_Final_Test_GT.zip')"
   ],
   "id": "dc01ba7b79c1896f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GTSRB_Final_Training_Images.zip already exists.\n",
      "GTSRB_Final_Test_Images.zip already exists.\n",
      "GTSRB_Final_Test_GT.zip already exists.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "id": "f905d089f305e5b7"
   },
   "cell_type": "markdown",
   "source": [
    "## Extract zip files"
   ],
   "id": "f905d089f305e5b7"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3de5a631fc79d888",
    "outputId": "a47e1c83-caad-4385-c493-28c72f17ddb1",
    "ExecuteTime": {
     "end_time": "2024-06-27T19:14:50.977879Z",
     "start_time": "2024-06-27T19:14:43.207930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_file(file_name):\n",
    "    with zipfile.ZipFile(f\"dataset/{file_name}\", 'r') as zip_ref:\n",
    "        file_list = zip_ref.namelist()\n",
    "        with tqdm(total=len(file_list), desc=\"Extracting\") as pbar:\n",
    "            for file in file_list:\n",
    "                zip_ref.extract(file, 'dataset/')\n",
    "                pbar.update(1)\n",
    "\n",
    "\n",
    "extract_file('GTSRB_Final_Training_Images.zip')\n",
    "extract_file('GTSRB_Final_Test_Images.zip')\n",
    "extract_file('GTSRB_Final_Test_GT.zip')"
   ],
   "id": "3de5a631fc79d888",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting: 100%|██████████| 39299/39299 [00:05<00:00, 7052.97it/s]\n",
      "Extracting: 100%|██████████| 12635/12635 [00:01<00:00, 6777.62it/s]\n",
      "Extracting: 100%|██████████| 1/1 [00:00<00:00, 534.85it/s]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "id": "323d7c1bb67c1550"
   },
   "cell_type": "markdown",
   "source": [
    "## Loading CSV file"
   ],
   "id": "323d7c1bb67c1550"
  },
  {
   "metadata": {
    "id": "468db0ee5cdf9840",
    "ExecuteTime": {
     "end_time": "2024-06-27T19:14:50.999837Z",
     "start_time": "2024-06-27T19:14:50.978732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#IMAGES: './dataset/GTSRB/test_images'\n",
    "#CSV ANNOTATIONS: './dataset/GTSRB/test_images/GT-final_test.csv'\n",
    "def csv_loader(csv_path):\n",
    "    data = np.loadtxt(csv_path,\n",
    "                      delimiter=\";\", dtype=str, skiprows=1)\n",
    "    return data\n",
    "\n",
    "\n",
    "#You should download the testset ('GTSRB_Final_Test_Images.zip') from the website which contains only the images\n",
    "#Then you have to download the ground truth csv ('GTSRB_Final_Test_GT.zip') from the website and paste it into the testset images folder\n",
    "annotations = csv_loader('./dataset/GT-final_test.csv')\n",
    "#sort the annotations\n",
    "annotations = annotations[:, [0, 7]]\n",
    "num_samples = len(annotations)\n",
    "#Column 0: filename - Column 1: classid\n",
    "annotations = annotations[annotations[:, 1].astype(int).argsort()]"
   ],
   "id": "468db0ee5cdf9840",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "id": "b8b6074543013fbb"
   },
   "cell_type": "markdown",
   "source": [
    "## Making training data accordingly"
   ],
   "id": "b8b6074543013fbb"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61a83a9b0d1825e4",
    "outputId": "bb698c3b-ee90-4bdb-a9ec-7e91f0e0027b",
    "ExecuteTime": {
     "end_time": "2024-06-27T19:14:51.044610Z",
     "start_time": "2024-06-27T19:14:51.000946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def move_directories(source, destination):\n",
    "    if not os.path.exists(destination):\n",
    "        os.makedirs(destination)\n",
    "    # Get a list of all directories in the source directory\n",
    "    directories = [d for d in os.listdir(source) if os.path.isdir(os.path.join(source, d))]\n",
    "\n",
    "    # Move each directory to the destination\n",
    "    for directory in tqdm(directories):\n",
    "        source_path = os.path.join(source, directory)\n",
    "        destination_path = os.path.join(destination, directory)\n",
    "        shutil.move(source_path, destination_path)\n",
    "\n",
    "\n",
    "# Move directories with contents\n",
    "move_directories(\"./dataset/GTSRB/Final_Training/Images\", \"./dataset/GTSRB/train\")\n",
    "shutil.rmtree(\"./dataset/GTSRB/Final_Training\")"
   ],
   "id": "61a83a9b0d1825e4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:00<00:00, 29103.61it/s]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "id": "5172c68f192d26ae"
   },
   "cell_type": "markdown",
   "source": [
    "## Making test data accordingly"
   ],
   "id": "5172c68f192d26ae"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "df556df8aaf39e91",
    "outputId": "fb5ccd24-86c7-4c70-acff-be8f8cb0a4ff",
    "ExecuteTime": {
     "end_time": "2024-06-27T19:14:51.464682Z",
     "start_time": "2024-06-27T19:14:51.045602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for class_id in tqdm(np.unique(annotations[:, 1]), desc='Class_ID'):\n",
    "    newpath = './dataset/GTSRB/test/' + class_id.zfill(5)\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    for image_filename in annotations[annotations[:, 1] == class_id]:\n",
    "        shutil.move('./dataset/GTSRB/Final_Test/Images/' + image_filename[0], newpath + '/' + image_filename[0])\n",
    "\n",
    "shutil.rmtree(\"./dataset/GTSRB/Final_Test\")"
   ],
   "id": "df556df8aaf39e91",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class_ID: 100%|██████████| 43/43 [00:00<00:00, 104.37it/s]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Merge the dataset and then split into train and test"
   ],
   "metadata": {
    "id": "JWpVwjN6nUFH"
   },
   "id": "JWpVwjN6nUFH"
  },
  {
   "cell_type": "code",
   "source": [
    "def merge(source_folder, destination_folder):\n",
    "    # Create the destination folder if it doesn't exist\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "\n",
    "    # Get the total number of files and directories in the source folder\n",
    "    total_items = sum([len(files) + len(dirs) for root, dirs, files in os.walk(source_folder)])\n",
    "\n",
    "    # Initialize tqdm to show progress\n",
    "    progress = tqdm(total=total_items, desc='Moving: ' + source_folder + ' --> ' + destination_folder, position=0,\n",
    "                    leave=True)\n",
    "\n",
    "    # Iterate over all files and subdirectories in the source folder\n",
    "    for root, dirs, files in os.walk(source_folder):\n",
    "        for item in files + dirs:\n",
    "            source_item = os.path.join(root, item)\n",
    "            destination_item = os.path.join(destination_folder, os.path.relpath(source_item, source_folder))\n",
    "\n",
    "            # If the item is a file, copy it to the destination folder\n",
    "            if os.path.isfile(source_item):\n",
    "                shutil.move(source_item, destination_item)\n",
    "            # If the item is a directory, create it in the destination folder\n",
    "            elif os.path.isdir(source_item):\n",
    "                os.makedirs(destination_item, exist_ok=True)\n",
    "\n",
    "            progress.update(1)  # Update progress bar\n",
    "\n",
    "    progress.close()  # Close tqdm\n",
    "\n",
    "\n",
    "def merge_folders(source_folders, target_folder):\n",
    "    for sf in source_folders:\n",
    "        merge(sf, target_folder)\n",
    "        shutil.rmtree(sf)\n",
    "\n",
    "\n",
    "# Temporary directory to store the merged dataset\n",
    "merged_dir = \"./dataset/GTSRB/merged\"\n",
    "\n",
    "merge_folders(['./dataset/GTSRB/train', './dataset/GTSRB/test'], merged_dir)\n",
    "\n",
    "# Training 70\n",
    "# Testing 30\n",
    "splitfolders.ratio(merged_dir, output=\"./dataset/GTSRB/plain\", seed=123, ratio=(.7, 0, 0.3), move=False)\n",
    "\n",
    "# Clear temporary files\n",
    "shutil.rmtree('./dataset/GTSRB/plain/val')\n",
    "os.remove('./dataset/GTSRB/Readme-Images-Final-test.txt')\n",
    "os.remove('./dataset/GTSRB/Readme-Images.txt')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "690Ap50knZxO",
    "outputId": "f8edb2d3-28d1-4d0b-bf88-541ab4e04add",
    "ExecuteTime": {
     "end_time": "2024-06-27T19:15:00.145363Z",
     "start_time": "2024-06-27T19:14:51.465608Z"
    }
   },
   "id": "690Ap50knZxO",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moving: ./dataset/GTSRB/train --> ./dataset/GTSRB/merged: 100%|██████████| 39295/39295 [00:01<00:00, 20051.09it/s]\n",
      "Moving: ./dataset/GTSRB/test --> ./dataset/GTSRB/merged: 100%|██████████| 12673/12673 [00:00<00:00, 20737.78it/s]\n",
      "Copying files: 51882 files [00:06, 8567.78 files/s]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "id": "5650b2ca72f0f486"
   },
   "cell_type": "markdown",
   "source": [
    "## Add weather conditions to the merged dataset and then split into train and test"
   ],
   "id": "5650b2ca72f0f486"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0fb0b549a431a3d",
    "outputId": "0800184c-2b50-484c-ed37-7ff16d3fb9bd",
    "ExecuteTime": {
     "end_time": "2024-06-27T19:15:39.070877Z",
     "start_time": "2024-06-27T19:15:00.146112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "da = data_augmenter.DataAugmenter(dataset_path='./dataset/GTSRB/')\n",
    "da.load_images(folder_to_load='merged')\n",
    "da.add_weather_effects(prob_per_class=0.5)\n",
    "\n",
    "# Training 70\n",
    "# Testing 30\n",
    "splitfolders.ratio(merged_dir, output=\"./dataset/GTSRB/weather\", seed=123, ratio=(.7, 0, 0.3), move=True)\n",
    "\n",
    "# Clear temporary files\n",
    "shutil.rmtree('./dataset/GTSRB/weather/val')\n",
    "shutil.rmtree(merged_dir)"
   ],
   "id": "b0fb0b549a431a3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found: , 00016, 00033, 00022, 00011, 00003, 00039, 00034, 00035, 00028, 00036, 00037, 00015, 00027, 00038, 00014, 00026, 00019, 00007, 00029, 00005, 00008, 00021, 00025, 00032, 00002, 00000, 00024, 00009, 00012, 00023, 00013, 00041, 00030, 00017, 00006, 00042, 00018, 00040, 00020, 00010, 00031, 00004, 00001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading classes: 100%|██████████| 43/43 [00:01<00:00, 21.55it/s]\n",
      "Add weather effects with probability 0.5: 100%|██████████| 43/43 [00:34<00:00,  1.24it/s]\n",
      "Copying files: 51882 files [00:02, 22175.00 files/s]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "id": "6dcd860b78a2678d"
   },
   "cell_type": "markdown",
   "source": [
    "## Set dataset paths"
   ],
   "id": "6dcd860b78a2678d"
  },
  {
   "metadata": {
    "id": "20f8c403f5b816ee",
    "ExecuteTime": {
     "end_time": "2024-06-27T19:15:39.074374Z",
     "start_time": "2024-06-27T19:15:39.072049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plain_train_dir = './dataset/GTSRB/plain/train'\n",
    "plain_test_dir = './dataset/GTSRB/plain/test'\n",
    "\n",
    "weather_train_dir = './dataset/GTSRB/weather/train'\n",
    "weather_test_dir = './dataset/GTSRB/weather/test'"
   ],
   "id": "20f8c403f5b816ee",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "id": "d2e92b663416be6f"
   },
   "cell_type": "markdown",
   "source": [
    "# Parameters setup"
   ],
   "id": "d2e92b663416be6f"
  },
  {
   "metadata": {
    "id": "e9b3ab9f505b0242",
    "ExecuteTime": {
     "end_time": "2024-06-27T19:15:40.279179Z",
     "start_time": "2024-06-27T19:15:39.075161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setting device for the computation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Change this to select between plain or weather dataset\n",
    "# train_dir = plain_train_dir/weather_train_dir\n",
    "# test_dir = plain_test_dir/weather_test_dir\n",
    "# train_dir = plain_train_dir\n",
    "# test_dir = plain_test_dir\n",
    "\n",
    "# Hyperparameters\n",
    "sgd_hyperparams = {\n",
    "    \"num_epochs\": 15,\n",
    "    \"batch_size\": 64,\n",
    "    #optimizer\n",
    "    \"opt\": \"sgd\",\n",
    "    \"learning_rate\": 1e-2,\n",
    "    \"beta_1\": 0.9,\n",
    "    \"beta_2\": 0.999,\n",
    "    \"eps\": 1e-8,\n",
    "    \"weight_decay\": 0,\n",
    "    \"momentum\": 0,\n",
    "    #scheduler\n",
    "    \"decay_rate\": 0.5,\n",
    "}\n",
    "adam_hyperparams = {\n",
    "    \"num_epochs\": 15,\n",
    "    \"batch_size\": 64,\n",
    "    #optimizer\n",
    "    \"opt\": \"adam\",\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"beta_1\": 0.9,\n",
    "    \"beta_2\": 0.999,\n",
    "    \"eps\": 1e-8,\n",
    "    \"weight_decay\": 0,\n",
    "    \"momentum\": 0,\n",
    "    #scheduler\n",
    "    \"decay_rate\": 0.5,\n",
    "}\n",
    "\n",
    "runs_arguments = [\n",
    "    {'type': 'P', 'cnn': 'CNN'},\n",
    "    {'type': 'P', 'cnn': 'CNN_ST'},\n",
    "    {'type': 'W', 'cnn': 'CNN'},\n",
    "    {'type': 'W', 'cnn': 'CNN_ST'},\n",
    "]"
   ],
   "id": "e9b3ab9f505b0242",
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": "# Method for loading the training/testing set",
   "metadata": {
    "id": "q-bE0596WTkR"
   },
   "id": "q-bE0596WTkR"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T19:15:40.293590Z",
     "start_time": "2024-06-27T19:15:40.281179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_train_test_dir(train_dir, test_dir):\n",
    "    \n",
    "    # Define your transformations\n",
    "    custom_cnn_transform = transforms.Compose([\n",
    "        transforms.Resize((48, 48)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    # Load the dataset using the torchvision.datasets.ImageFolder\n",
    "    train_dataset = datasets.ImageFolder(train_dir, transform=custom_cnn_transform)\n",
    "    \n",
    "    # Concatenate all images into a single tensor\n",
    "    images = torch.stack([img for img, _ in train_dataset], dim=0)\n",
    "    \n",
    "    # Calculate mean and std across all images and channels\n",
    "    mean = torch.mean(images, dim=(0, 2, 3))\n",
    "    std = torch.std(images, dim=(0, 2, 3))\n",
    "    \n",
    "    # Define your transformations\n",
    "    custom_cnn_transform = transforms.Compose([\n",
    "        transforms.Resize((48, 48)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "    \n",
    "    # Load the dataset using the torchvision.datasets.ImageFolder\n",
    "    train_dataset = datasets.ImageFolder(train_dir, transform=custom_cnn_transform)\n",
    "    test_dataset = datasets.ImageFolder(test_dir, transform=custom_cnn_transform)\n",
    "    \n",
    "    return train_dataset, test_dataset"
   ],
   "id": "6f9bed2c473422e0",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Defining the CNN"
   ],
   "metadata": {
    "id": "G7CE0M9SWeZj"
   },
   "id": "G7CE0M9SWeZj"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CNN Plain"
   ],
   "metadata": {
    "id": "waE26jiDoZ32"
   },
   "id": "waE26jiDoZ32"
  },
  {
   "cell_type": "code",
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Expected input as 48x48\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=200, kernel_size=7, stride=1, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.local_norm = nn.LocalResponseNorm(size=5)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=200, out_channels=250, kernel_size=4, stride=1, padding=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=250, out_channels=350, kernel_size=4, stride=1, padding=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=350 * 6 * 6, out_features=400)\n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=400, out_features=43)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.local_norm(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.local_norm(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.local_norm(x)\n",
    "\n",
    "        # Flatten the output from conv1\n",
    "        x = x.view(-1, 350 * 6 * 6)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "metadata": {
    "id": "r56vEoqhogaG",
    "ExecuteTime": {
     "end_time": "2024-06-27T19:15:40.302693Z",
     "start_time": "2024-06-27T19:15:40.294484Z"
    }
   },
   "id": "r56vEoqhogaG",
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CNN with 3 spatial transformers"
   ],
   "metadata": {
    "id": "6jKXkY8lft6q"
   },
   "id": "6jKXkY8lft6q"
  },
  {
   "cell_type": "code",
   "source": [
    "class CNN_ST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Expected input as 48x48\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=200, kernel_size=7, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=200, out_channels=250, kernel_size=4, stride=1, padding=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=250, out_channels=350, kernel_size=4, stride=1, padding=2)\n",
    "\n",
    "        self.local_norm = nn.LocalResponseNorm(size=5)\n",
    "\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=350 * 6 * 6, out_features=400)\n",
    "        self.fc2 = nn.Linear(in_features=400, out_features=43)\n",
    "\n",
    "        # Spatial transformer block 1\n",
    "        self.loc1 = nn.Sequential(\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(in_channels=3, out_channels=250, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(in_channels=250, out_channels=250, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.fc_loc1 = nn.Sequential(\n",
    "            nn.Linear(250 * 6 * 6, 250),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(250, 6)\n",
    "        )\n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc1[2].weight.data.zero_()\n",
    "        self.fc_loc1[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "        # Spatial transformer block 2\n",
    "        self.loc2 = nn.Sequential(\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(in_channels=200, out_channels=150, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(in_channels=150, out_channels=200, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.fc_loc2 = nn.Sequential(\n",
    "            nn.Linear(200 * 2 * 2, 300),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(300, 6)\n",
    "        )\n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc2[2].weight.data.zero_()\n",
    "        self.fc_loc2[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "        # Spatial transformer block 3\n",
    "        self.loc3 = nn.Sequential(\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(in_channels=250, out_channels=150, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(in_channels=150, out_channels=200, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.fc_loc3 = nn.Sequential(\n",
    "            nn.Linear(200 * 1 * 1, 300),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(300, 6)\n",
    "        )\n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc3[2].weight.data.zero_()\n",
    "        self.fc_loc3[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "    def stn1(self, x):\n",
    "        xs = self.loc1(x)\n",
    "        xs = xs.view(-1, 250 * 6 * 6)\n",
    "        theta = self.fc_loc1(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "\n",
    "        grid = F.affine_grid(theta, x.size(), align_corners=False)\n",
    "        x = F.grid_sample(x, grid, align_corners=False)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def stn2(self, x):\n",
    "        xs = self.loc2(x)\n",
    "        xs = xs.view(-1, 200 * 2 * 2)\n",
    "        theta = self.fc_loc2(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "\n",
    "        grid = F.affine_grid(theta, x.size(), align_corners=False)\n",
    "        x = F.grid_sample(x, grid, align_corners=False)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def stn3(self, x):\n",
    "        xs = self.loc3(x)\n",
    "        xs = xs.view(-1, 200 * 1 * 1)\n",
    "        theta = self.fc_loc3(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "\n",
    "        grid = F.affine_grid(theta, x.size(), align_corners=False)\n",
    "        x = F.grid_sample(x, grid, align_corners=False)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Spatial transformer 1\n",
    "        x = self.stn1(x)\n",
    "\n",
    "        # CNN block 1\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.local_norm(x)\n",
    "\n",
    "        # Spatial transformer 2\n",
    "        x = self.stn2(x)\n",
    "\n",
    "        # CNN block 2\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.local_norm(x)\n",
    "\n",
    "        # Spatial transformer 3\n",
    "        x = self.stn3(x)\n",
    "\n",
    "        # CNN block 3\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.local_norm(x)\n",
    "\n",
    "        # Flatten the output for dense layers\n",
    "        x = x.view(-1, 350 * 6 * 6)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "id": "eAwI5trgfxqg",
    "ExecuteTime": {
     "end_time": "2024-06-27T19:15:40.315024Z",
     "start_time": "2024-06-27T19:15:40.303589Z"
    }
   },
   "id": "eAwI5trgfxqg",
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training function"
   ],
   "metadata": {
    "id": "6cpZCj8mowXh"
   },
   "id": "6cpZCj8mowXh"
  },
  {
   "cell_type": "code",
   "source": [
    "def test_model(trained_model, test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        trained_model.eval()\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = trained_model(images)\n",
    "            softmax_outputs = F.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(softmax_outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = float(correct / total)\n",
    "    print('{} Model accuracy: {:.4f}'.format('Test phase - ', test_accuracy))\n",
    "    writer.add_scalar('Training/Test Accuracy', test_accuracy)\n",
    "    return test_accuracy\n",
    "\n",
    "\n",
    "def train_model(device, model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=25,\n",
    "                model_name='trained_model'):\n",
    "    since = time.time()\n",
    "    time_train = 0\n",
    "    time_val = 0\n",
    "\n",
    "    # Save the initial model\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('-' * 10)\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Choose the appropriate data loader\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                data_total_steps = len(train_loader)\n",
    "                data_loader = train_loader\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "                data_total_steps = len(val_loader)\n",
    "                data_loader = val_loader\n",
    "\n",
    "            for i, (images, labels) in enumerate(data_loader):\n",
    "                # time_t = epoch * len(data_loader) * i + i\n",
    "\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward\n",
    "                # Track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(images)\n",
    "                    softmax_outputs = F.softmax(outputs, dim=1)\n",
    "                    probs, preds = torch.max(softmax_outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * images.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                #prints the stats every 20 steps (20 batches performed)\n",
    "                if (i + 1) % int(data_total_steps / 8) == 0:\n",
    "                    print(\n",
    "                        f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{data_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "                    # Log image predictions\n",
    "                    selected_indices = random.sample(range(len(images)), 4)  # Select 4 random indices\n",
    "                    selected_images = images[selected_indices]\n",
    "                    selected_labels = labels[selected_indices]\n",
    "                    selected_preds = preds[selected_indices]\n",
    "                    selected_probs = probs[selected_indices]\n",
    "                    if phase == 'train':\n",
    "                        writer.add_figure('Training/Training Predictions',\n",
    "                                          plot_classes_preds(selected_images, selected_labels, selected_preds,\n",
    "                                                             selected_probs),\n",
    "                                          global_step=time_train)\n",
    "                    else:\n",
    "                        writer.add_figure('Training/Validation Predictions',\n",
    "                                          plot_classes_preds(selected_images, selected_labels, selected_preds,\n",
    "                                                             selected_probs),\n",
    "                                          global_step=time_val)\n",
    "\n",
    "                # Log scalars\n",
    "                if phase == 'train':\n",
    "                    writer.add_scalar('Training/Training Loss',\n",
    "                                      loss.item(),\n",
    "                                      time_train)\n",
    "                    writer.add_scalar('Policy/Learning Rate',\n",
    "                                      np.array(scheduler.get_last_lr()),\n",
    "                                      time_train)\n",
    "                    time_train += 1\n",
    "                else:\n",
    "                    writer.add_scalar('Training/Validation Loss',\n",
    "                                      loss.item(),\n",
    "                                      time_val)\n",
    "                    time_val += 1\n",
    "\n",
    "            epoch_loss = running_loss / len(data_loader.dataset)\n",
    "            epoch_acc = running_corrects.double() / len(data_loader.dataset)\n",
    "\n",
    "            if phase == 'train':\n",
    "                print('{} Epoch {} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                    'Train phase - ', epoch + 1, epoch_loss, epoch_acc))\n",
    "                writer.add_scalar('Training/Training Accuracy',\n",
    "                                  epoch_acc,\n",
    "                                  epoch)\n",
    "                # if (epoch + 1) % max(int(num_epochs / 5), 1) == 0:  # checkpoint the model\n",
    "                #     print(\"----> model checkpoint...\")\n",
    "                #     torch.save(model, f'./models/trained_model_{model_name}_epoch_{epoch + 1}.pth')\n",
    "            else:\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                print('{} Epoch {} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                    'Validation phase - ', epoch + 1, epoch_loss, epoch_acc))\n",
    "                writer.add_scalar('Training/Validation Accuracy',\n",
    "                                  epoch_acc,\n",
    "                                  epoch)\n",
    "                #scheduler.step(epoch_loss)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc:{:.4f}'.format(best_acc))\n",
    "    # Return the model with the best accuracy in the validation\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "id": "RIiC4t_jWeBV",
    "ExecuteTime": {
     "end_time": "2024-06-27T19:15:40.326203Z",
     "start_time": "2024-06-27T19:15:40.316202Z"
    }
   },
   "id": "RIiC4t_jWeBV",
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model setting"
   ],
   "metadata": {
    "id": "trtHqdVcWrbV"
   },
   "id": "trtHqdVcWrbV"
  },
  {
   "cell_type": "code",
   "source": [
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "# runs_arguments = [\n",
    "#     {'type': 'P', 'cnn': 'CNN'},\n",
    "#     {'type': 'P', 'cnn': 'CNN_ST'},\n",
    "#     {'type': 'W', 'cnn': 'CNN'},\n",
    "#     {'type': 'W', 'cnn': 'CNN_ST'},\n",
    "# ]\n",
    "for r_args in runs_arguments:\n",
    "    dataset_type = r_args['type']\n",
    "    model_architecture = r_args['cnn']\n",
    "    \n",
    "    train_dir = plain_train_dir if dataset_type == 'P' else weather_train_dir   \n",
    "    test_dir = plain_test_dir if dataset_type == 'P' else weather_test_dir   \n",
    "    train_dataset, test_dataset = load_train_test_dir(train_dir, test_dir)\n",
    "    \n",
    "    # Compute class weights\n",
    "    train_size = len(train_dataset)\n",
    "    number_of_classes = len(train_dataset.classes)\n",
    "    train_samples_per_class = np.bincount(train_dataset.targets)\n",
    "    class_weights=(train_size/(number_of_classes*train_samples_per_class))\n",
    "    class_weights=torch.tensor(class_weights,dtype=torch.float)\n",
    "    \n",
    "    for sel_optim in ['sgd', 'adam']:\n",
    "        hyperparams = sgd_hyperparams if sel_optim == 'sgd' else adam_hyperparams\n",
    "        model_name = f'{dataset_type}_{model_architecture}_{hyperparams[\"opt\"]}'\n",
    "        writer = SummaryWriter(f'runs/{model_name}')\n",
    "    \n",
    "        test_abs = int(len(train_dataset) * 0.8)\n",
    "        train_subset, val_subset = random_split(\n",
    "            train_dataset, [test_abs, len(train_dataset) - test_abs])\n",
    "        \n",
    "        train_data_size = len(train_subset)\n",
    "        \n",
    "        # Create DataLoader instances for training and validation\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_subset,\n",
    "            batch_size=hyperparams[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            num_workers=0)\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            val_subset,\n",
    "            batch_size=hyperparams[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            num_workers=0)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=hyperparams[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            num_workers=0)\n",
    "        \n",
    "        # Model initialization\n",
    "        model = CNN() if model_architecture == 'CNN' else CNN_ST()\n",
    "        model.to(device)\n",
    "        \n",
    "        # Define loss function, optimizer, etc.\n",
    "        class_weights = class_weights.to(device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        \n",
    "        # Set optimizers\n",
    "        if hyperparams['opt'] == 'sgd':\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=hyperparams[\"learning_rate\"], momentum=hyperparams[\"momentum\"],\n",
    "                                        weight_decay=hyperparams[\"weight_decay\"])\n",
    "        elif hyperparams['opt'] == 'adam':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=hyperparams[\"learning_rate\"],\n",
    "                                         betas=(hyperparams[\"beta_1\"], hyperparams[\"beta_2\"]), eps=hyperparams[\"eps\"],\n",
    "                                         weight_decay=hyperparams[\"weight_decay\"])\n",
    "        elif hyperparams['opt'] == 'rmsprop':\n",
    "            optimizer = torch.optim.RMSprop(model.parameters(), lr=hyperparams[\"learning_rate\"])\n",
    "        else:\n",
    "            raise ValueError('Invalid optimizer provided')\n",
    "        scheduler = lr_scheduler.LinearLR(optimizer)\n",
    "        \n",
    "        # Train model\n",
    "        trained_model = train_model(device=device, model=model, criterion=criterion, optimizer=optimizer, scheduler=scheduler,\n",
    "                                    train_loader=train_loader, val_loader=val_loader, num_epochs=hyperparams[\"num_epochs\"],\n",
    "                                    model_name=model_name)\n",
    "        \n",
    "        ta = test_model(trained_model=trained_model, test_loader=test_loader)\n",
    "        \n",
    "        writer.add_hparams({key: str(value) if isinstance(value, list) else value for key, value in hyperparams.items()},\n",
    "                       metric_dict={'Training/Test Accuracy': ta})\n",
    "        print(f'Finished Training {model_name}')\n",
    "        torch.save(trained_model, f'./models/trained_model_{model_name}_final.pth')"
   ],
   "metadata": {
    "id": "SpS12uKCWvbE",
    "ExecuteTime": {
     "end_time": "2024-06-27T19:17:53.887912Z",
     "start_time": "2024-06-27T19:16:58.035706Z"
    }
   },
   "id": "SpS12uKCWvbE",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Epoch 1/15\n",
      "----------\n",
      "Epoch [1/15], Step [56/454], Loss: 3.7503\n",
      "Epoch [1/15], Step [112/454], Loss: 3.7645\n",
      "Epoch [1/15], Step [168/454], Loss: 3.7586\n",
      "Epoch [1/15], Step [224/454], Loss: 3.7127\n",
      "Epoch [1/15], Step [280/454], Loss: 3.6436\n",
      "Epoch [1/15], Step [336/454], Loss: 3.7280\n",
      "Epoch [1/15], Step [392/454], Loss: 3.6872\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 74\u001B[0m\n\u001B[1;32m     71\u001B[0m scheduler \u001B[38;5;241m=\u001B[39m lr_scheduler\u001B[38;5;241m.\u001B[39mLinearLR(optimizer)\n\u001B[1;32m     73\u001B[0m \u001B[38;5;66;03m# Train model\u001B[39;00m\n\u001B[0;32m---> 74\u001B[0m trained_model \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     75\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhyperparams\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnum_epochs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     76\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     78\u001B[0m ta \u001B[38;5;241m=\u001B[39m test_model(trained_model\u001B[38;5;241m=\u001B[39mtrained_model, test_loader\u001B[38;5;241m=\u001B[39mtest_loader)\n\u001B[1;32m     80\u001B[0m writer\u001B[38;5;241m.\u001B[39madd_hparams({key: \u001B[38;5;28mstr\u001B[39m(value) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(value, \u001B[38;5;28mlist\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m value \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m hyperparams\u001B[38;5;241m.\u001B[39mitems()},\n\u001B[1;32m     81\u001B[0m                metric_dict\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTraining/Test Accuracy\u001B[39m\u001B[38;5;124m'\u001B[39m: ta})\n",
      "Cell \u001B[0;32mIn[17], line 51\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(device, model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs, model_name)\u001B[0m\n\u001B[1;32m     48\u001B[0m     data_total_steps \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(val_loader)\n\u001B[1;32m     49\u001B[0m     data_loader \u001B[38;5;241m=\u001B[39m val_loader\n\u001B[0;32m---> 51\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (images, labels) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(data_loader):\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;66;03m# time_t = epoch * len(data_loader) * i + i\u001B[39;00m\n\u001B[1;32m     54\u001B[0m     images \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     55\u001B[0m     labels \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m~/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__getitems__\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__:\n\u001B[0;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__getitems__\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n",
      "File \u001B[0;32m~/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages/torch/utils/data/dataset.py:419\u001B[0m, in \u001B[0;36mSubset.__getitems__\u001B[0;34m(self, indices)\u001B[0m\n\u001B[1;32m    417\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices])  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[1;32m    418\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 419\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx]] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices]\n",
      "File \u001B[0;32m~/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages/torch/utils/data/dataset.py:419\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    417\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices])  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[1;32m    418\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 419\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices]\n",
      "File \u001B[0;32m~/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages/torchvision/datasets/folder.py:247\u001B[0m, in \u001B[0;36mDatasetFolder.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m    245\u001B[0m sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader(path)\n\u001B[1;32m    246\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 247\u001B[0m     sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43msample\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    248\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    249\u001B[0m     target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform(target)\n",
      "File \u001B[0;32m~/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[1;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[0;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[0;32m~/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages/torchvision/transforms/transforms.py:277\u001B[0m, in \u001B[0;36mNormalize.forward\u001B[0;34m(self, tensor)\u001B[0m\n\u001B[1;32m    269\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, tensor: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m    270\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    271\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m    272\u001B[0m \u001B[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    275\u001B[0m \u001B[38;5;124;03m        Tensor: Normalized Tensor image.\u001B[39;00m\n\u001B[1;32m    276\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 277\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormalize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages/torchvision/transforms/functional.py:350\u001B[0m, in \u001B[0;36mnormalize\u001B[0;34m(tensor, mean, std, inplace)\u001B[0m\n\u001B[1;32m    347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tensor, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[1;32m    348\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimg should be Tensor Image. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(tensor)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 350\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF_t\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormalize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmean\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstd\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py:920\u001B[0m, in \u001B[0;36mnormalize\u001B[0;34m(tensor, mean, std, inplace)\u001B[0m\n\u001B[1;32m    918\u001B[0m mean \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mas_tensor(mean, dtype\u001B[38;5;241m=\u001B[39mdtype, device\u001B[38;5;241m=\u001B[39mtensor\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m    919\u001B[0m std \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mas_tensor(std, dtype\u001B[38;5;241m=\u001B[39mdtype, device\u001B[38;5;241m=\u001B[39mtensor\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m--> 920\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43m(\u001B[49m\u001B[43mstd\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43many\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    921\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstd evaluated to zero after conversion to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdtype\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, leading to division by zero.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    922\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mean\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": "# Zip files",
   "metadata": {
    "id": "tzJPf39GWxB9"
   },
   "id": "tzJPf39GWxB9"
  },
  {
   "cell_type": "code",
   "source": [
    "!zip -r runs.zip runs\n",
    "!zip -r models.zip models"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OL41g_-KDYWe",
    "outputId": "88276013-7cd0-49f9-a20a-3a7e5391a36d",
    "ExecuteTime": {
     "end_time": "2024-06-27T19:16:21.422785Z",
     "start_time": "2024-06-27T19:16:21.422695Z"
    }
   },
   "id": "OL41g_-KDYWe",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
