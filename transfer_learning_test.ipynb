{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "95feb99553e5f381"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-27T21:52:21.354379Z",
     "start_time": "2024-04-27T21:52:21.343086Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n"
   ],
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Set device",
   "id": "f26a8210e0afc41d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T21:52:21.378122Z",
     "start_time": "2024-04-27T21:52:21.363388Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
   "id": "2c3071b3341a7e3e",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Loading the training dataset",
   "id": "179809bcd90eb62a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T21:52:21.388282Z",
     "start_time": "2024-04-27T21:52:21.379130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_transform = transforms.Compose([\n",
    "        #TODO:think a better transformation pipeline\n",
    "        #naive transformation\n",
    "        transforms.Resize((64,64)),\n",
    "        transforms.ToTensor()\n",
    "    ])"
   ],
   "id": "50d7d9e3cbf71956",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T21:52:21.671252Z",
     "start_time": "2024-04-27T21:52:21.388282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dir='./dataset/GTSRB/train/original'\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir,data_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=200,shuffle=True, num_workers=0)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "train_size = len(train_dataset)\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "print(class_names)"
   ],
   "id": "233a264dd2428bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00000', '00001', '00002', '00003', '00004', '00005', '00006', '00007', '00008', '00009', '00010', '00011', '00012', '00013', '00014', '00015', '00016', '00017', '00018', '00019', '00020', '00021', '00022', '00023', '00024', '00025', '00026', '00027', '00028', '00029', '00030', '00031', '00032', '00033', '00034', '00035', '00036', '00037', '00038', '00039', '00040', '00041', '00042']\n"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8199467383be87c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T21:52:21.687638Z",
     "start_time": "2024-04-27T21:52:21.671252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "    \n",
    "            # forward\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(images)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "    \n",
    "                # backward + optimize only if in training phase\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                if (i+1) % 200 == 0:\n",
    "                    print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "            # statistics\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            #to update weights\n",
    "            scheduler.step()\n",
    "    \n",
    "        epoch_loss = running_loss / train_size\n",
    "        epoch_acc = running_corrects.double() / train_size\n",
    "    \n",
    "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "            'train', epoch_loss, epoch_acc))\n",
    "    \n",
    "    print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ],
   "id": "d1388a73ca0b9760",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ResNet18 backbone",
   "id": "6a9c4fc45c415b8f"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-04-27T21:52:21.687638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### ConvNet as fixed feature extractor ####\n",
    "# freeze all the network parameters except the final layer settings, to not compute the gradients backward\n",
    "model_conv = torchvision.models.resnet18(weights=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 43)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "\n",
    "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=20)"
   ],
   "id": "731771455515444c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save the model",
   "id": "e4d9605ef361a0d3"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "print('Finished Training')\n",
    "PATH = './first_trained_model.pth'\n",
    "torch.save(model_conv.state_dict(), PATH)"
   ],
   "id": "9f5f26cdefec21bb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
