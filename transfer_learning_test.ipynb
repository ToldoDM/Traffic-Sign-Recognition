{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "95feb99553e5f381"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-01T15:31:15.751816Z",
     "start_time": "2024-05-01T15:31:15.748272Z"
    }
   },
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.models import ResNet152_Weights"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Helper functions",
   "id": "920e26d20bf8c3c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T15:31:15.757834Z",
     "start_time": "2024-05-01T15:31:15.752911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_classes_preds(images, labels, preds, probs):\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx + 1, xticks=[], yticks=[])\n",
    "        plt.imshow(np.transpose(images[idx].cpu().numpy(), (1, 2, 0)))  # because is a tensor \n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            preds[idx],\n",
    "            probs[idx] * 100.0,\n",
    "            labels[idx]),\n",
    "            color=(\"green\" if preds[idx] == labels[idx].item() else \"red\"))\n",
    "    return fig"
   ],
   "id": "5b5b2a8dbc382e69",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Loading the train dataset",
   "id": "1e0e1c4761d44bcd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T15:31:15.846151Z",
     "start_time": "2024-05-01T15:31:15.772505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_transform = transforms.Compose([\n",
    "    #TODO:think a better transformation pipeline\n",
    "    #naive transformation\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dir = './dataset/GTSRB/train'\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, train_transform)\n",
    "train_size = len(train_dataset)\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "print('Train size:', train_size)\n",
    "print('Class names:', class_names)"
   ],
   "id": "233a264dd2428bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 39209\n",
      "Class names: ['00000', '00001', '00002', '00003', '00004', '00005', '00006', '00007', '00008', '00009', '00010', '00011', '00012', '00013', '00014', '00015', '00016', '00017', '00018', '00019', '00020', '00021', '00022', '00023', '00024', '00025', '00026', '00027', '00028', '00029', '00030', '00031', '00032', '00033', '00034', '00035', '00036', '00037', '00038', '00039', '00040', '00041', '00042']\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1239b390bd17bb84"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Loading the test dataset",
   "id": "8199467383be87c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T15:31:15.874268Z",
     "start_time": "2024-05-01T15:31:15.847416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_transform = transforms.Compose([\n",
    "    #TODO:think a better transformation pipeline\n",
    "    #naive transformation\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_dir = './dataset/GTSRB/test'\n",
    "\n",
    "test_dataset = datasets.ImageFolder(test_dir, test_transform)\n",
    "test_size = len(test_dataset)\n",
    "class_names = test_dataset.classes\n",
    "\n",
    "print('Test size:', train_size)\n",
    "print('Class names:', class_names)"
   ],
   "id": "4757b88147ac3fbf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: 39209\n",
      "Class names: ['00000', '00001', '00002', '00003', '00004', '00005', '00006', '00007', '00008', '00009', '00010', '00011', '00012', '00013', '00014', '00015', '00016', '00017', '00018', '00019', '00020', '00021', '00022', '00023', '00024', '00025', '00026', '00027', '00028', '00029', '00030', '00031', '00032', '00033', '00034', '00035', '00036', '00037', '00038', '00039', '00040', '00041', '00042']\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Defining the training phase",
   "id": "3191a2bde55202e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T15:31:15.886990Z",
     "start_time": "2024-05-01T15:31:15.875464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(device, model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('-' * 10)\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Choose the appropriate data loader\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                data_total_steps = len(train_loader)\n",
    "                data_loader = train_loader\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "                data_total_steps = len(val_loader)\n",
    "                data_loader = val_loader\n",
    "\n",
    "            for i, (images, labels) in enumerate(data_loader):\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward\n",
    "                # Track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(images)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    probs = [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, outputs)]\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * images.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                # Calculate entropy with epsilon\n",
    "                softmax_outputs = F.softmax(outputs, dim=1)\n",
    "                epsilon = 1e-10  # Small epsilon value to avoid zero probabilities\n",
    "                entropy = -torch.sum(softmax_outputs * torch.log2(softmax_outputs + epsilon), dim=1).mean()\n",
    "\n",
    "                # Log scalars\n",
    "                if phase == 'train':\n",
    "                    writer.add_scalar('Training/Training Loss',\n",
    "                                      loss.item(),\n",
    "                                      epoch * len(data_loader) + i)\n",
    "                    writer.add_scalar('Policy/Entropy',\n",
    "                                      entropy.item(),\n",
    "                                      epoch * len(data_loader) + i)\n",
    "                    writer.add_scalar('Policy/Learning Rate',\n",
    "                                      np.array(scheduler.get_last_lr()),\n",
    "                                      epoch * len(data_loader) + i)\n",
    "                else:\n",
    "                    writer.add_scalar('Training/Validation Loss',\n",
    "                                      loss.item(),\n",
    "                                      epoch * len(data_loader) + i)\n",
    "\n",
    "                #prints the stats every 20 steps (20 batches performed)\n",
    "                if (i + 1) % int(data_total_steps / 8) == 0:\n",
    "                    print(\n",
    "                        f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{data_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "                    # Log image predictions\n",
    "                    selected_indices = random.sample(range(len(images)), 4)  # Select 4 random indices\n",
    "                    selected_images = images[selected_indices]\n",
    "                    selected_labels = labels[selected_indices]\n",
    "                    selected_preds = preds[selected_indices]\n",
    "                    selected_probs = [probs[i] for i in selected_indices]\n",
    "                    if phase == 'train':\n",
    "                        writer.add_figure('Training/Training Predictions',\n",
    "                                          plot_classes_preds(selected_images, selected_labels, selected_preds,\n",
    "                                                             selected_probs),\n",
    "                                          global_step=epoch * len(data_loader) + i)\n",
    "                    else:\n",
    "                        writer.add_figure('Training/Validation Predictions',\n",
    "                                          plot_classes_preds(selected_images, selected_labels, selected_preds,\n",
    "                                                             selected_probs),\n",
    "                                          global_step=epoch * len(data_loader) + i)\n",
    "\n",
    "            epoch_loss = running_loss / len(data_loader.dataset)\n",
    "            epoch_acc = running_corrects.double() / len(data_loader.dataset)\n",
    "\n",
    "            if phase == 'train':\n",
    "                print('{} Epoch {} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                    'Train phase - ', epoch + 1, epoch_loss, epoch_acc))\n",
    "                writer.add_scalar('Training/Training Accuracy',\n",
    "                                  running_corrects.double() / len(data_loader.dataset),\n",
    "                                  epoch * len(data_loader))\n",
    "                scheduler.step()\n",
    "            else:\n",
    "                print('{} Epoch {} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                    'Validation phase - ', epoch + 1, epoch_loss, epoch_acc))\n",
    "                writer.add_scalar('Training/Validation Accuracy',\n",
    "                                  running_corrects.double() / len(data_loader.dataset),\n",
    "                                  epoch * len(data_loader))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_dynamic_network(num_features, num_classes, num_layers=0, num_neurons=1):\n",
    "    layers = []\n",
    "    # Input layer to first hidden layer\n",
    "    if num_layers > 0:\n",
    "        layers.append(nn.Linear(num_features, num_neurons))\n",
    "        layers.append(nn.ELU())\n",
    "\n",
    "    # Additional hidden layers\n",
    "    for _ in range(1, num_layers):\n",
    "        layers.append(nn.Linear(num_neurons, num_neurons))\n",
    "        layers.append(nn.ELU())\n",
    "\n",
    "    # Always include the final specified layer\n",
    "    layers.append(nn.Linear(num_neurons if num_layers > 0 else num_features, num_classes))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ],
   "id": "d1388a73ca0b9760",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training Setup",
   "id": "596398e0642aca56"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T15:31:15.895512Z",
     "start_time": "2024-05-01T15:31:15.888855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setting device for the computation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 32\n",
    "batch_size = 100\n",
    "learning_rate = 0.05\n",
    "step_size = 2  # After how many epochs to apply the decay rate\n",
    "decay_rate = 0.9  # new_lr = Decay rate * learning rate\n",
    "\n",
    "num_layers = 1  # 0 layers means no hidden layers, just one layer from conv to classes: conv -> layer -> softmax\n",
    "num_neurons = 100"
   ],
   "id": "50d7d9e3cbf71956",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setting up the model using ResNet152 as backbone",
   "id": "7d53679acd744b28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T15:31:17.298650Z",
     "start_time": "2024-05-01T15:31:15.896550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/TSR-SGD')\n",
    "\n",
    "# Assuming train_dataset is your full dataset\n",
    "indices = list(range(train_size))\n",
    "split = int(np.floor(0.7 * train_size))  # 70% train, 30% validation\n",
    "np.random.shuffle(indices)  # Shuffle the indices if needed\n",
    "\n",
    "# Split indices into training and validation sets\n",
    "train_indices, val_indices = indices[:split], indices[split:]\n",
    "\n",
    "# Create sampler objects using the subset indices\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "# Create DataLoader instances for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "val_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "\n",
    "# Model initialization\n",
    "model = torchvision.models.resnet152(weights=ResNet152_Weights.IMAGENET1K_V2)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "# Define the layers you want to add\n",
    "model.fc = create_dynamic_network(model.fc.in_features, 43, num_layers=num_layers, num_neurons=num_neurons)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function, optimizer, etc.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.fc.parameters(), lr=learning_rate)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=decay_rate)"
   ],
   "id": "d53ebf690ba17e92",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train the model",
   "id": "6a9c4fc45c415b8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T15:57:20.657895Z",
     "start_time": "2024-05-01T15:31:17.299686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train model\n",
    "trained_model = train_model(device=device, model=model, criterion=criterion, optimizer=optimizer, scheduler=scheduler,\n",
    "                            train_loader=train_loader, val_loader=val_loader, num_epochs=num_epochs)"
   ],
   "id": "731771455515444c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Epoch 1/32\n",
      "----------\n",
      "Epoch [1/32], Step [34/275], Loss: 3.1675\n",
      "Epoch [1/32], Step [68/275], Loss: 2.6375\n",
      "Epoch [1/32], Step [102/275], Loss: 2.2682\n",
      "Epoch [1/32], Step [136/275], Loss: 2.2304\n",
      "Epoch [1/32], Step [170/275], Loss: 2.1089\n",
      "Epoch [1/32], Step [204/275], Loss: 1.6610\n",
      "Epoch [1/32], Step [238/275], Loss: 1.5974\n",
      "Epoch [1/32], Step [272/275], Loss: 1.5289\n",
      "Train phase -  Epoch 1 Loss: 1.5789 Acc: 0.3060\n",
      "Epoch [1/32], Step [14/118], Loss: 1.5068\n",
      "Epoch [1/32], Step [28/118], Loss: 1.4412\n",
      "Epoch [1/32], Step [42/118], Loss: 1.6295\n",
      "Epoch [1/32], Step [56/118], Loss: 1.6437\n",
      "Epoch [1/32], Step [70/118], Loss: 1.2916\n",
      "Epoch [1/32], Step [84/118], Loss: 1.7792\n",
      "Epoch [1/32], Step [98/118], Loss: 1.4294\n",
      "Epoch [1/32], Step [112/118], Loss: 1.4893\n",
      "Validation phase -  Epoch 1 Loss: 0.4685 Acc: 0.1738\n",
      "----------\n",
      "Epoch 2/32\n",
      "----------\n",
      "Epoch [2/32], Step [34/275], Loss: 1.3856\n",
      "Epoch [2/32], Step [68/275], Loss: 1.3685\n",
      "Epoch [2/32], Step [102/275], Loss: 1.3270\n",
      "Epoch [2/32], Step [136/275], Loss: 1.2091\n",
      "Epoch [2/32], Step [170/275], Loss: 1.2774\n",
      "Epoch [2/32], Step [204/275], Loss: 1.2437\n",
      "Epoch [2/32], Step [238/275], Loss: 1.1436\n",
      "Epoch [2/32], Step [272/275], Loss: 1.1814\n",
      "Train phase -  Epoch 2 Loss: 0.9196 Acc: 0.4465\n",
      "Epoch [2/32], Step [14/118], Loss: 1.2781\n",
      "Epoch [2/32], Step [28/118], Loss: 1.2091\n",
      "Epoch [2/32], Step [42/118], Loss: 1.0350\n",
      "Epoch [2/32], Step [56/118], Loss: 1.0853\n",
      "Epoch [2/32], Step [70/118], Loss: 1.2388\n",
      "Epoch [2/32], Step [84/118], Loss: 1.2961\n",
      "Epoch [2/32], Step [98/118], Loss: 1.2397\n",
      "Epoch [2/32], Step [112/118], Loss: 1.1230\n",
      "Validation phase -  Epoch 2 Loss: 0.3460 Acc: 0.2007\n",
      "----------\n",
      "Epoch 3/32\n",
      "----------\n",
      "Epoch [3/32], Step [34/275], Loss: 0.9524\n",
      "Epoch [3/32], Step [68/275], Loss: 1.0023\n",
      "Epoch [3/32], Step [102/275], Loss: 1.0004\n",
      "Epoch [3/32], Step [136/275], Loss: 0.9452\n",
      "Epoch [3/32], Step [170/275], Loss: 1.0207\n",
      "Epoch [3/32], Step [204/275], Loss: 0.8433\n",
      "Epoch [3/32], Step [238/275], Loss: 1.0058\n",
      "Epoch [3/32], Step [272/275], Loss: 0.9741\n",
      "Train phase -  Epoch 3 Loss: 0.7009 Acc: 0.5028\n",
      "Epoch [3/32], Step [14/118], Loss: 1.0057\n",
      "Epoch [3/32], Step [28/118], Loss: 1.0847\n",
      "Epoch [3/32], Step [42/118], Loss: 0.9449\n",
      "Epoch [3/32], Step [56/118], Loss: 0.8787\n",
      "Epoch [3/32], Step [70/118], Loss: 0.9813\n",
      "Epoch [3/32], Step [84/118], Loss: 1.2345\n",
      "Epoch [3/32], Step [98/118], Loss: 0.8743\n",
      "Epoch [3/32], Step [112/118], Loss: 1.0395\n",
      "Validation phase -  Epoch 3 Loss: 0.2921 Acc: 0.2133\n",
      "----------\n",
      "Epoch 4/32\n",
      "----------\n",
      "Epoch [4/32], Step [34/275], Loss: 0.9277\n",
      "Epoch [4/32], Step [68/275], Loss: 0.8707\n",
      "Epoch [4/32], Step [102/275], Loss: 0.8910\n",
      "Epoch [4/32], Step [136/275], Loss: 0.7586\n",
      "Epoch [4/32], Step [170/275], Loss: 0.8337\n",
      "Epoch [4/32], Step [204/275], Loss: 0.7850\n",
      "Epoch [4/32], Step [238/275], Loss: 0.6802\n",
      "Epoch [4/32], Step [272/275], Loss: 0.8224\n",
      "Train phase -  Epoch 4 Loss: 0.5868 Acc: 0.5309\n",
      "Epoch [4/32], Step [14/118], Loss: 1.0358\n",
      "Epoch [4/32], Step [28/118], Loss: 0.7283\n",
      "Epoch [4/32], Step [42/118], Loss: 1.0436\n",
      "Epoch [4/32], Step [56/118], Loss: 0.9063\n",
      "Epoch [4/32], Step [70/118], Loss: 1.0004\n",
      "Epoch [4/32], Step [84/118], Loss: 0.8730\n",
      "Epoch [4/32], Step [98/118], Loss: 0.8629\n",
      "Epoch [4/32], Step [112/118], Loss: 0.8732\n",
      "Validation phase -  Epoch 4 Loss: 0.2652 Acc: 0.2217\n",
      "----------\n",
      "Epoch 5/32\n",
      "----------\n",
      "Epoch [5/32], Step [34/275], Loss: 0.6704\n",
      "Epoch [5/32], Step [68/275], Loss: 0.7022\n",
      "Epoch [5/32], Step [102/275], Loss: 0.8492\n",
      "Epoch [5/32], Step [136/275], Loss: 0.7118\n",
      "Epoch [5/32], Step [170/275], Loss: 0.5582\n",
      "Epoch [5/32], Step [204/275], Loss: 0.6077\n",
      "Epoch [5/32], Step [238/275], Loss: 0.6420\n",
      "Epoch [5/32], Step [272/275], Loss: 0.7593\n",
      "Train phase -  Epoch 5 Loss: 0.5096 Acc: 0.5528\n",
      "Epoch [5/32], Step [14/118], Loss: 0.8725\n",
      "Epoch [5/32], Step [28/118], Loss: 0.8443\n",
      "Epoch [5/32], Step [42/118], Loss: 0.8914\n",
      "Epoch [5/32], Step [56/118], Loss: 1.0100\n",
      "Epoch [5/32], Step [70/118], Loss: 0.7353\n",
      "Epoch [5/32], Step [84/118], Loss: 0.8245\n",
      "Epoch [5/32], Step [98/118], Loss: 0.9615\n",
      "Epoch [5/32], Step [112/118], Loss: 0.7392\n",
      "Validation phase -  Epoch 5 Loss: 0.2398 Acc: 0.2292\n",
      "----------\n",
      "Epoch 6/32\n",
      "----------\n",
      "Epoch [6/32], Step [34/275], Loss: 0.6921\n",
      "Epoch [6/32], Step [68/275], Loss: 0.5307\n",
      "Epoch [6/32], Step [102/275], Loss: 0.6084\n",
      "Epoch [6/32], Step [136/275], Loss: 0.7802\n",
      "Epoch [6/32], Step [170/275], Loss: 0.5240\n",
      "Epoch [6/32], Step [204/275], Loss: 0.5790\n",
      "Epoch [6/32], Step [238/275], Loss: 0.6115\n",
      "Epoch [6/32], Step [272/275], Loss: 0.5638\n",
      "Train phase -  Epoch 6 Loss: 0.4580 Acc: 0.5683\n",
      "Epoch [6/32], Step [14/118], Loss: 0.9041\n",
      "Epoch [6/32], Step [28/118], Loss: 0.7608\n",
      "Epoch [6/32], Step [42/118], Loss: 0.6169\n",
      "Epoch [6/32], Step [56/118], Loss: 0.8133\n",
      "Epoch [6/32], Step [70/118], Loss: 0.8685\n",
      "Epoch [6/32], Step [84/118], Loss: 0.7562\n",
      "Epoch [6/32], Step [98/118], Loss: 0.7240\n",
      "Epoch [6/32], Step [112/118], Loss: 0.6631\n",
      "Validation phase -  Epoch 6 Loss: 0.2308 Acc: 0.2297\n",
      "----------\n",
      "Epoch 7/32\n",
      "----------\n",
      "Epoch [7/32], Step [34/275], Loss: 0.6757\n",
      "Epoch [7/32], Step [68/275], Loss: 0.5123\n",
      "Epoch [7/32], Step [102/275], Loss: 0.5915\n",
      "Epoch [7/32], Step [136/275], Loss: 0.6344\n",
      "Epoch [7/32], Step [170/275], Loss: 0.6290\n",
      "Epoch [7/32], Step [204/275], Loss: 0.6128\n",
      "Epoch [7/32], Step [238/275], Loss: 0.5041\n",
      "Epoch [7/32], Step [272/275], Loss: 0.5417\n",
      "Train phase -  Epoch 7 Loss: 0.4167 Acc: 0.5804\n",
      "Epoch [7/32], Step [14/118], Loss: 0.8656\n",
      "Epoch [7/32], Step [28/118], Loss: 0.6820\n",
      "Epoch [7/32], Step [42/118], Loss: 0.7918\n",
      "Epoch [7/32], Step [56/118], Loss: 0.9125\n",
      "Epoch [7/32], Step [70/118], Loss: 0.7797\n",
      "Epoch [7/32], Step [84/118], Loss: 0.8722\n",
      "Epoch [7/32], Step [98/118], Loss: 0.7494\n",
      "Epoch [7/32], Step [112/118], Loss: 0.8241\n",
      "Validation phase -  Epoch 7 Loss: 0.2212 Acc: 0.2342\n",
      "----------\n",
      "Epoch 8/32\n",
      "----------\n",
      "Epoch [8/32], Step [34/275], Loss: 0.5944\n",
      "Epoch [8/32], Step [68/275], Loss: 0.4966\n",
      "Epoch [8/32], Step [102/275], Loss: 0.5443\n",
      "Epoch [8/32], Step [136/275], Loss: 0.5808\n",
      "Epoch [8/32], Step [170/275], Loss: 0.6443\n",
      "Epoch [8/32], Step [204/275], Loss: 0.5663\n",
      "Epoch [8/32], Step [238/275], Loss: 0.4740\n",
      "Epoch [8/32], Step [272/275], Loss: 0.5865\n",
      "Train phase -  Epoch 8 Loss: 0.3835 Acc: 0.5899\n",
      "Epoch [8/32], Step [14/118], Loss: 0.7098\n",
      "Epoch [8/32], Step [28/118], Loss: 0.6454\n",
      "Epoch [8/32], Step [42/118], Loss: 0.6675\n",
      "Epoch [8/32], Step [56/118], Loss: 0.7250\n",
      "Epoch [8/32], Step [70/118], Loss: 0.5819\n",
      "Epoch [8/32], Step [84/118], Loss: 0.6241\n",
      "Epoch [8/32], Step [98/118], Loss: 0.6924\n",
      "Epoch [8/32], Step [112/118], Loss: 0.5743\n",
      "Validation phase -  Epoch 8 Loss: 0.2105 Acc: 0.2363\n",
      "----------\n",
      "Epoch 9/32\n",
      "----------\n",
      "Epoch [9/32], Step [34/275], Loss: 0.5868\n",
      "Epoch [9/32], Step [68/275], Loss: 0.5802\n",
      "Epoch [9/32], Step [102/275], Loss: 0.3942\n",
      "Epoch [9/32], Step [136/275], Loss: 0.8564\n",
      "Epoch [9/32], Step [170/275], Loss: 0.5479\n",
      "Epoch [9/32], Step [204/275], Loss: 0.3697\n",
      "Epoch [9/32], Step [238/275], Loss: 0.4703\n",
      "Epoch [9/32], Step [272/275], Loss: 0.4842\n",
      "Train phase -  Epoch 9 Loss: 0.3587 Acc: 0.5960\n",
      "Epoch [9/32], Step [14/118], Loss: 0.5965\n",
      "Epoch [9/32], Step [28/118], Loss: 0.7241\n",
      "Epoch [9/32], Step [42/118], Loss: 1.0354\n",
      "Epoch [9/32], Step [56/118], Loss: 0.6773\n",
      "Epoch [9/32], Step [70/118], Loss: 0.5725\n",
      "Epoch [9/32], Step [84/118], Loss: 0.6508\n",
      "Epoch [9/32], Step [98/118], Loss: 0.7342\n",
      "Epoch [9/32], Step [112/118], Loss: 0.4900\n",
      "Validation phase -  Epoch 9 Loss: 0.2031 Acc: 0.2377\n",
      "----------\n",
      "Epoch 10/32\n",
      "----------\n",
      "Epoch [10/32], Step [34/275], Loss: 0.4407\n",
      "Epoch [10/32], Step [68/275], Loss: 0.6378\n",
      "Epoch [10/32], Step [102/275], Loss: 0.4047\n",
      "Epoch [10/32], Step [136/275], Loss: 0.5149\n",
      "Epoch [10/32], Step [170/275], Loss: 0.3567\n",
      "Epoch [10/32], Step [204/275], Loss: 0.4430\n",
      "Epoch [10/32], Step [238/275], Loss: 0.4604\n",
      "Epoch [10/32], Step [272/275], Loss: 0.4971\n",
      "Train phase -  Epoch 10 Loss: 0.3413 Acc: 0.5997\n",
      "Epoch [10/32], Step [14/118], Loss: 0.7538\n",
      "Epoch [10/32], Step [28/118], Loss: 0.7334\n",
      "Epoch [10/32], Step [42/118], Loss: 0.9047\n",
      "Epoch [10/32], Step [56/118], Loss: 0.5960\n",
      "Epoch [10/32], Step [70/118], Loss: 0.5854\n",
      "Epoch [10/32], Step [84/118], Loss: 0.6540\n",
      "Epoch [10/32], Step [98/118], Loss: 0.5865\n",
      "Epoch [10/32], Step [112/118], Loss: 0.8672\n",
      "Validation phase -  Epoch 10 Loss: 0.2029 Acc: 0.2379\n",
      "----------\n",
      "Epoch 11/32\n",
      "----------\n",
      "Epoch [11/32], Step [34/275], Loss: 0.4734\n",
      "Epoch [11/32], Step [68/275], Loss: 0.4077\n",
      "Epoch [11/32], Step [102/275], Loss: 0.4471\n",
      "Epoch [11/32], Step [136/275], Loss: 0.4568\n",
      "Epoch [11/32], Step [170/275], Loss: 0.4226\n",
      "Epoch [11/32], Step [204/275], Loss: 0.4651\n",
      "Epoch [11/32], Step [238/275], Loss: 0.4764\n",
      "Epoch [11/32], Step [272/275], Loss: 0.3948\n",
      "Train phase -  Epoch 11 Loss: 0.3233 Acc: 0.6057\n",
      "Epoch [11/32], Step [14/118], Loss: 0.4478\n",
      "Epoch [11/32], Step [28/118], Loss: 0.7563\n",
      "Epoch [11/32], Step [42/118], Loss: 0.6463\n",
      "Epoch [11/32], Step [56/118], Loss: 0.5188\n",
      "Epoch [11/32], Step [70/118], Loss: 0.7639\n",
      "Epoch [11/32], Step [84/118], Loss: 0.7634\n",
      "Epoch [11/32], Step [98/118], Loss: 0.6081\n",
      "Epoch [11/32], Step [112/118], Loss: 0.7543\n",
      "Validation phase -  Epoch 11 Loss: 0.1986 Acc: 0.2411\n",
      "----------\n",
      "Epoch 12/32\n",
      "----------\n",
      "Epoch [12/32], Step [34/275], Loss: 0.5599\n",
      "Epoch [12/32], Step [68/275], Loss: 0.3551\n",
      "Epoch [12/32], Step [102/275], Loss: 0.4525\n",
      "Epoch [12/32], Step [136/275], Loss: 0.3946\n",
      "Epoch [12/32], Step [170/275], Loss: 0.6903\n",
      "Epoch [12/32], Step [204/275], Loss: 0.4116\n",
      "Epoch [12/32], Step [238/275], Loss: 0.4010\n",
      "Epoch [12/32], Step [272/275], Loss: 0.3628\n",
      "Train phase -  Epoch 12 Loss: 0.3088 Acc: 0.6102\n",
      "Epoch [12/32], Step [14/118], Loss: 0.4566\n",
      "Epoch [12/32], Step [28/118], Loss: 0.5409\n",
      "Epoch [12/32], Step [42/118], Loss: 0.7227\n",
      "Epoch [12/32], Step [56/118], Loss: 0.5502\n",
      "Epoch [12/32], Step [70/118], Loss: 0.9202\n",
      "Epoch [12/32], Step [84/118], Loss: 0.6442\n",
      "Epoch [12/32], Step [98/118], Loss: 0.6142\n",
      "Epoch [12/32], Step [112/118], Loss: 0.5305\n",
      "Validation phase -  Epoch 12 Loss: 0.1907 Acc: 0.2415\n",
      "----------\n",
      "Epoch 13/32\n",
      "----------\n",
      "Epoch [13/32], Step [34/275], Loss: 0.3753\n",
      "Epoch [13/32], Step [68/275], Loss: 0.3434\n",
      "Epoch [13/32], Step [102/275], Loss: 0.5299\n",
      "Epoch [13/32], Step [136/275], Loss: 0.4923\n",
      "Epoch [13/32], Step [170/275], Loss: 0.4514\n",
      "Epoch [13/32], Step [204/275], Loss: 0.2975\n",
      "Epoch [13/32], Step [238/275], Loss: 0.4225\n",
      "Epoch [13/32], Step [272/275], Loss: 0.4217\n",
      "Train phase -  Epoch 13 Loss: 0.2985 Acc: 0.6135\n",
      "Epoch [13/32], Step [14/118], Loss: 0.6379\n",
      "Epoch [13/32], Step [28/118], Loss: 0.7873\n",
      "Epoch [13/32], Step [42/118], Loss: 0.9350\n",
      "Epoch [13/32], Step [56/118], Loss: 0.6609\n",
      "Epoch [13/32], Step [70/118], Loss: 0.7605\n",
      "Epoch [13/32], Step [84/118], Loss: 0.4530\n",
      "Epoch [13/32], Step [98/118], Loss: 0.6522\n",
      "Epoch [13/32], Step [112/118], Loss: 0.7094\n",
      "Validation phase -  Epoch 13 Loss: 0.1877 Acc: 0.2427\n",
      "----------\n",
      "Epoch 14/32\n",
      "----------\n",
      "Epoch [14/32], Step [34/275], Loss: 0.3265\n",
      "Epoch [14/32], Step [68/275], Loss: 0.4562\n",
      "Epoch [14/32], Step [102/275], Loss: 0.3841\n",
      "Epoch [14/32], Step [136/275], Loss: 0.4472\n",
      "Epoch [14/32], Step [170/275], Loss: 0.4975\n",
      "Epoch [14/32], Step [204/275], Loss: 0.3741\n",
      "Epoch [14/32], Step [238/275], Loss: 0.5486\n",
      "Epoch [14/32], Step [272/275], Loss: 0.3851\n",
      "Train phase -  Epoch 14 Loss: 0.2900 Acc: 0.6152\n",
      "Epoch [14/32], Step [14/118], Loss: 0.8262\n",
      "Epoch [14/32], Step [28/118], Loss: 0.6337\n",
      "Epoch [14/32], Step [42/118], Loss: 0.6186\n",
      "Epoch [14/32], Step [56/118], Loss: 0.5138\n",
      "Epoch [14/32], Step [70/118], Loss: 0.4682\n",
      "Epoch [14/32], Step [84/118], Loss: 0.6180\n",
      "Epoch [14/32], Step [98/118], Loss: 0.8872\n",
      "Epoch [14/32], Step [112/118], Loss: 0.7623\n",
      "Validation phase -  Epoch 14 Loss: 0.1862 Acc: 0.2432\n",
      "----------\n",
      "Epoch 15/32\n",
      "----------\n",
      "Epoch [15/32], Step [34/275], Loss: 0.3503\n",
      "Epoch [15/32], Step [68/275], Loss: 0.3587\n",
      "Epoch [15/32], Step [102/275], Loss: 0.5940\n",
      "Epoch [15/32], Step [136/275], Loss: 0.2958\n",
      "Epoch [15/32], Step [170/275], Loss: 0.3733\n",
      "Epoch [15/32], Step [204/275], Loss: 0.5414\n",
      "Epoch [15/32], Step [238/275], Loss: 0.2659\n",
      "Epoch [15/32], Step [272/275], Loss: 0.4055\n",
      "Train phase -  Epoch 15 Loss: 0.2777 Acc: 0.6174\n",
      "Epoch [15/32], Step [14/118], Loss: 0.5926\n",
      "Epoch [15/32], Step [28/118], Loss: 0.6450\n",
      "Epoch [15/32], Step [42/118], Loss: 0.5658\n",
      "Epoch [15/32], Step [56/118], Loss: 0.8321\n",
      "Epoch [15/32], Step [70/118], Loss: 0.5592\n",
      "Epoch [15/32], Step [84/118], Loss: 0.7591\n",
      "Epoch [15/32], Step [98/118], Loss: 0.5997\n",
      "Epoch [15/32], Step [112/118], Loss: 0.6987\n",
      "Validation phase -  Epoch 15 Loss: 0.1835 Acc: 0.2436\n",
      "----------\n",
      "Epoch 16/32\n",
      "----------\n",
      "Epoch [16/32], Step [34/275], Loss: 0.3644\n",
      "Epoch [16/32], Step [68/275], Loss: 0.3470\n",
      "Epoch [16/32], Step [102/275], Loss: 0.3686\n",
      "Epoch [16/32], Step [136/275], Loss: 0.2857\n",
      "Epoch [16/32], Step [170/275], Loss: 0.4055\n",
      "Epoch [16/32], Step [204/275], Loss: 0.4627\n",
      "Epoch [16/32], Step [238/275], Loss: 0.2704\n",
      "Epoch [16/32], Step [272/275], Loss: 0.3548\n",
      "Train phase -  Epoch 16 Loss: 0.2667 Acc: 0.6214\n",
      "Epoch [16/32], Step [14/118], Loss: 0.4244\n",
      "Epoch [16/32], Step [28/118], Loss: 0.6197\n",
      "Epoch [16/32], Step [42/118], Loss: 0.3834\n",
      "Epoch [16/32], Step [56/118], Loss: 0.5365\n",
      "Epoch [16/32], Step [70/118], Loss: 0.5338\n",
      "Epoch [16/32], Step [84/118], Loss: 0.7468\n",
      "Epoch [16/32], Step [98/118], Loss: 0.6442\n",
      "Epoch [16/32], Step [112/118], Loss: 0.5906\n",
      "Validation phase -  Epoch 16 Loss: 0.1796 Acc: 0.2446\n",
      "----------\n",
      "Epoch 17/32\n",
      "----------\n",
      "Epoch [17/32], Step [34/275], Loss: 0.3561\n",
      "Epoch [17/32], Step [68/275], Loss: 0.4105\n",
      "Epoch [17/32], Step [102/275], Loss: 0.3960\n",
      "Epoch [17/32], Step [136/275], Loss: 0.4248\n",
      "Epoch [17/32], Step [170/275], Loss: 0.4257\n",
      "Epoch [17/32], Step [204/275], Loss: 0.3958\n",
      "Epoch [17/32], Step [238/275], Loss: 0.4877\n",
      "Epoch [17/32], Step [272/275], Loss: 0.4704\n",
      "Train phase -  Epoch 17 Loss: 0.2596 Acc: 0.6234\n",
      "Epoch [17/32], Step [14/118], Loss: 0.6696\n",
      "Epoch [17/32], Step [28/118], Loss: 0.6005\n",
      "Epoch [17/32], Step [42/118], Loss: 0.8076\n",
      "Epoch [17/32], Step [56/118], Loss: 0.5525\n",
      "Epoch [17/32], Step [70/118], Loss: 0.5914\n",
      "Epoch [17/32], Step [84/118], Loss: 0.6306\n",
      "Epoch [17/32], Step [98/118], Loss: 0.4625\n",
      "Epoch [17/32], Step [112/118], Loss: 0.7013\n",
      "Validation phase -  Epoch 17 Loss: 0.1809 Acc: 0.2444\n",
      "----------\n",
      "Epoch 18/32\n",
      "----------\n",
      "Epoch [18/32], Step [34/275], Loss: 0.2827\n",
      "Epoch [18/32], Step [68/275], Loss: 0.4038\n",
      "Epoch [18/32], Step [102/275], Loss: 0.4299\n",
      "Epoch [18/32], Step [136/275], Loss: 0.2227\n",
      "Epoch [18/32], Step [170/275], Loss: 0.3519\n",
      "Epoch [18/32], Step [204/275], Loss: 0.3695\n",
      "Epoch [18/32], Step [238/275], Loss: 0.3846\n",
      "Epoch [18/32], Step [272/275], Loss: 0.2323\n",
      "Train phase -  Epoch 18 Loss: 0.2520 Acc: 0.6262\n",
      "Epoch [18/32], Step [14/118], Loss: 0.5294\n",
      "Epoch [18/32], Step [28/118], Loss: 0.6483\n",
      "Epoch [18/32], Step [42/118], Loss: 0.6187\n",
      "Epoch [18/32], Step [56/118], Loss: 0.6129\n",
      "Epoch [18/32], Step [70/118], Loss: 0.6155\n",
      "Epoch [18/32], Step [84/118], Loss: 0.8564\n",
      "Epoch [18/32], Step [98/118], Loss: 0.5842\n",
      "Epoch [18/32], Step [112/118], Loss: 0.5801\n",
      "Validation phase -  Epoch 18 Loss: 0.1819 Acc: 0.2446\n",
      "----------\n",
      "Epoch 19/32\n",
      "----------\n",
      "Epoch [19/32], Step [34/275], Loss: 0.1980\n",
      "Epoch [19/32], Step [68/275], Loss: 0.4100\n",
      "Epoch [19/32], Step [102/275], Loss: 0.3393\n",
      "Epoch [19/32], Step [136/275], Loss: 0.3729\n",
      "Epoch [19/32], Step [170/275], Loss: 0.3211\n",
      "Epoch [19/32], Step [204/275], Loss: 0.2369\n",
      "Epoch [19/32], Step [238/275], Loss: 0.3667\n",
      "Epoch [19/32], Step [272/275], Loss: 0.3094\n",
      "Train phase -  Epoch 19 Loss: 0.2441 Acc: 0.6276\n",
      "Epoch [19/32], Step [14/118], Loss: 0.7676\n",
      "Epoch [19/32], Step [28/118], Loss: 0.6444\n",
      "Epoch [19/32], Step [42/118], Loss: 0.5236\n",
      "Epoch [19/32], Step [56/118], Loss: 0.5369\n",
      "Epoch [19/32], Step [70/118], Loss: 0.5251\n",
      "Epoch [19/32], Step [84/118], Loss: 0.4952\n",
      "Epoch [19/32], Step [98/118], Loss: 0.5897\n",
      "Epoch [19/32], Step [112/118], Loss: 0.6836\n",
      "Validation phase -  Epoch 19 Loss: 0.1793 Acc: 0.2455\n",
      "----------\n",
      "Epoch 20/32\n",
      "----------\n",
      "Epoch [20/32], Step [34/275], Loss: 0.2688\n",
      "Epoch [20/32], Step [68/275], Loss: 0.3013\n",
      "Epoch [20/32], Step [102/275], Loss: 0.2900\n",
      "Epoch [20/32], Step [136/275], Loss: 0.4036\n",
      "Epoch [20/32], Step [170/275], Loss: 0.3284\n",
      "Epoch [20/32], Step [204/275], Loss: 0.3819\n",
      "Epoch [20/32], Step [238/275], Loss: 0.2424\n",
      "Epoch [20/32], Step [272/275], Loss: 0.3749\n",
      "Train phase -  Epoch 20 Loss: 0.2415 Acc: 0.6296\n",
      "Epoch [20/32], Step [14/118], Loss: 0.7914\n",
      "Epoch [20/32], Step [28/118], Loss: 0.5797\n",
      "Epoch [20/32], Step [42/118], Loss: 0.5726\n",
      "Epoch [20/32], Step [56/118], Loss: 0.5169\n",
      "Epoch [20/32], Step [70/118], Loss: 0.5251\n",
      "Epoch [20/32], Step [84/118], Loss: 0.4675\n",
      "Epoch [20/32], Step [98/118], Loss: 0.5533\n",
      "Epoch [20/32], Step [112/118], Loss: 0.5677\n",
      "Validation phase -  Epoch 20 Loss: 0.1778 Acc: 0.2459\n",
      "----------\n",
      "Epoch 21/32\n",
      "----------\n",
      "Epoch [21/32], Step [34/275], Loss: 0.3398\n",
      "Epoch [21/32], Step [68/275], Loss: 0.3351\n",
      "Epoch [21/32], Step [102/275], Loss: 0.2404\n",
      "Epoch [21/32], Step [136/275], Loss: 0.3274\n",
      "Epoch [21/32], Step [170/275], Loss: 0.3469\n",
      "Epoch [21/32], Step [204/275], Loss: 0.2692\n",
      "Epoch [21/32], Step [238/275], Loss: 0.2693\n",
      "Epoch [21/32], Step [272/275], Loss: 0.3462\n",
      "Train phase -  Epoch 21 Loss: 0.2378 Acc: 0.6301\n",
      "Epoch [21/32], Step [14/118], Loss: 0.5971\n",
      "Epoch [21/32], Step [28/118], Loss: 0.5316\n",
      "Epoch [21/32], Step [42/118], Loss: 0.5601\n",
      "Epoch [21/32], Step [56/118], Loss: 0.6327\n",
      "Epoch [21/32], Step [70/118], Loss: 0.5314\n",
      "Epoch [21/32], Step [84/118], Loss: 0.6079\n",
      "Epoch [21/32], Step [98/118], Loss: 0.5838\n",
      "Epoch [21/32], Step [112/118], Loss: 0.6407\n",
      "Validation phase -  Epoch 21 Loss: 0.1766 Acc: 0.2461\n",
      "----------\n",
      "Epoch 22/32\n",
      "----------\n",
      "Epoch [22/32], Step [34/275], Loss: 0.3128\n",
      "Epoch [22/32], Step [68/275], Loss: 0.4017\n",
      "Epoch [22/32], Step [102/275], Loss: 0.3048\n",
      "Epoch [22/32], Step [136/275], Loss: 0.2953\n",
      "Epoch [22/32], Step [170/275], Loss: 0.3760\n",
      "Epoch [22/32], Step [204/275], Loss: 0.3577\n",
      "Epoch [22/32], Step [238/275], Loss: 0.4188\n",
      "Epoch [22/32], Step [272/275], Loss: 0.3429\n",
      "Train phase -  Epoch 22 Loss: 0.2309 Acc: 0.6315\n",
      "Epoch [22/32], Step [14/118], Loss: 0.4098\n",
      "Epoch [22/32], Step [28/118], Loss: 0.5547\n",
      "Epoch [22/32], Step [42/118], Loss: 0.7312\n",
      "Epoch [22/32], Step [56/118], Loss: 0.5135\n",
      "Epoch [22/32], Step [70/118], Loss: 0.7268\n",
      "Epoch [22/32], Step [84/118], Loss: 0.7051\n",
      "Epoch [22/32], Step [98/118], Loss: 0.5610\n",
      "Epoch [22/32], Step [112/118], Loss: 0.5670\n",
      "Validation phase -  Epoch 22 Loss: 0.1805 Acc: 0.2456\n",
      "----------\n",
      "Epoch 23/32\n",
      "----------\n",
      "Epoch [23/32], Step [34/275], Loss: 0.3205\n",
      "Epoch [23/32], Step [68/275], Loss: 0.4947\n",
      "Epoch [23/32], Step [102/275], Loss: 0.3699\n",
      "Epoch [23/32], Step [136/275], Loss: 0.2077\n",
      "Epoch [23/32], Step [170/275], Loss: 0.3684\n",
      "Epoch [23/32], Step [204/275], Loss: 0.2818\n",
      "Epoch [23/32], Step [238/275], Loss: 0.2627\n",
      "Epoch [23/32], Step [272/275], Loss: 0.3331\n",
      "Train phase -  Epoch 23 Loss: 0.2245 Acc: 0.6343\n",
      "Epoch [23/32], Step [14/118], Loss: 0.8533\n",
      "Epoch [23/32], Step [28/118], Loss: 0.6664\n",
      "Epoch [23/32], Step [42/118], Loss: 0.4667\n",
      "Epoch [23/32], Step [56/118], Loss: 0.6630\n",
      "Epoch [23/32], Step [70/118], Loss: 0.7214\n",
      "Epoch [23/32], Step [84/118], Loss: 0.5006\n",
      "Epoch [23/32], Step [98/118], Loss: 0.8143\n",
      "Epoch [23/32], Step [112/118], Loss: 0.5776\n",
      "Validation phase -  Epoch 23 Loss: 0.1780 Acc: 0.2459\n",
      "----------\n",
      "Epoch 24/32\n",
      "----------\n",
      "Epoch [24/32], Step [34/275], Loss: 0.3083\n",
      "Epoch [24/32], Step [68/275], Loss: 0.3865\n",
      "Epoch [24/32], Step [102/275], Loss: 0.3856\n",
      "Epoch [24/32], Step [136/275], Loss: 0.3591\n",
      "Epoch [24/32], Step [170/275], Loss: 0.3656\n",
      "Epoch [24/32], Step [204/275], Loss: 0.2187\n",
      "Epoch [24/32], Step [238/275], Loss: 0.2368\n",
      "Epoch [24/32], Step [272/275], Loss: 0.3214\n",
      "Train phase -  Epoch 24 Loss: 0.2253 Acc: 0.6336\n",
      "Epoch [24/32], Step [14/118], Loss: 0.5749\n",
      "Epoch [24/32], Step [28/118], Loss: 0.8142\n",
      "Epoch [24/32], Step [42/118], Loss: 0.5579\n",
      "Epoch [24/32], Step [56/118], Loss: 0.7076\n",
      "Epoch [24/32], Step [70/118], Loss: 0.4345\n",
      "Epoch [24/32], Step [84/118], Loss: 0.5268\n",
      "Epoch [24/32], Step [98/118], Loss: 0.5815\n",
      "Epoch [24/32], Step [112/118], Loss: 0.7342\n",
      "Validation phase -  Epoch 24 Loss: 0.1787 Acc: 0.2462\n",
      "----------\n",
      "Epoch 25/32\n",
      "----------\n",
      "Epoch [25/32], Step [34/275], Loss: 0.4712\n",
      "Epoch [25/32], Step [68/275], Loss: 0.2961\n",
      "Epoch [25/32], Step [102/275], Loss: 0.3287\n",
      "Epoch [25/32], Step [136/275], Loss: 0.3105\n",
      "Epoch [25/32], Step [170/275], Loss: 0.2721\n",
      "Epoch [25/32], Step [204/275], Loss: 0.3237\n",
      "Epoch [25/32], Step [238/275], Loss: 0.4375\n",
      "Epoch [25/32], Step [272/275], Loss: 0.2871\n",
      "Train phase -  Epoch 25 Loss: 0.2275 Acc: 0.6326\n",
      "Epoch [25/32], Step [14/118], Loss: 0.7562\n",
      "Epoch [25/32], Step [28/118], Loss: 0.5730\n",
      "Epoch [25/32], Step [42/118], Loss: 0.6100\n",
      "Epoch [25/32], Step [56/118], Loss: 0.3991\n",
      "Epoch [25/32], Step [70/118], Loss: 0.7537\n",
      "Epoch [25/32], Step [84/118], Loss: 0.6760\n",
      "Epoch [25/32], Step [98/118], Loss: 0.5367\n",
      "Epoch [25/32], Step [112/118], Loss: 0.8925\n",
      "Validation phase -  Epoch 25 Loss: 0.1817 Acc: 0.2458\n",
      "----------\n",
      "Epoch 26/32\n",
      "----------\n",
      "Epoch [26/32], Step [34/275], Loss: 0.2021\n",
      "Epoch [26/32], Step [68/275], Loss: 0.2138\n",
      "Epoch [26/32], Step [102/275], Loss: 0.3053\n",
      "Epoch [26/32], Step [136/275], Loss: 0.3202\n",
      "Epoch [26/32], Step [170/275], Loss: 0.2338\n",
      "Epoch [26/32], Step [204/275], Loss: 0.3333\n",
      "Epoch [26/32], Step [238/275], Loss: 0.2924\n",
      "Epoch [26/32], Step [272/275], Loss: 0.3039\n",
      "Train phase -  Epoch 26 Loss: 0.2177 Acc: 0.6364\n",
      "Epoch [26/32], Step [14/118], Loss: 0.6227\n",
      "Epoch [26/32], Step [28/118], Loss: 0.3843\n",
      "Epoch [26/32], Step [42/118], Loss: 0.7250\n",
      "Epoch [26/32], Step [56/118], Loss: 0.5572\n",
      "Epoch [26/32], Step [70/118], Loss: 0.6415\n",
      "Epoch [26/32], Step [84/118], Loss: 0.5638\n",
      "Epoch [26/32], Step [98/118], Loss: 0.4285\n",
      "Epoch [26/32], Step [112/118], Loss: 0.4729\n",
      "Validation phase -  Epoch 26 Loss: 0.1723 Acc: 0.2478\n",
      "----------\n",
      "Epoch 27/32\n",
      "----------\n",
      "Epoch [27/32], Step [34/275], Loss: 0.3979\n",
      "Epoch [27/32], Step [68/275], Loss: 0.2641\n",
      "Epoch [27/32], Step [102/275], Loss: 0.2466\n",
      "Epoch [27/32], Step [136/275], Loss: 0.2949\n",
      "Epoch [27/32], Step [170/275], Loss: 0.2885\n",
      "Epoch [27/32], Step [204/275], Loss: 0.3073\n",
      "Epoch [27/32], Step [238/275], Loss: 0.3453\n",
      "Epoch [27/32], Step [272/275], Loss: 0.3726\n",
      "Train phase -  Epoch 27 Loss: 0.2165 Acc: 0.6370\n",
      "Epoch [27/32], Step [14/118], Loss: 0.6947\n",
      "Epoch [27/32], Step [28/118], Loss: 0.6310\n",
      "Epoch [27/32], Step [42/118], Loss: 0.7813\n",
      "Epoch [27/32], Step [56/118], Loss: 0.4480\n",
      "Epoch [27/32], Step [70/118], Loss: 0.5444\n",
      "Epoch [27/32], Step [84/118], Loss: 0.5412\n",
      "Epoch [27/32], Step [98/118], Loss: 0.6098\n",
      "Epoch [27/32], Step [112/118], Loss: 0.6067\n",
      "Validation phase -  Epoch 27 Loss: 0.1791 Acc: 0.2469\n",
      "----------\n",
      "Epoch 28/32\n",
      "----------\n",
      "Epoch [28/32], Step [34/275], Loss: 0.1845\n",
      "Epoch [28/32], Step [68/275], Loss: 0.2752\n",
      "Epoch [28/32], Step [102/275], Loss: 0.3459\n",
      "Epoch [28/32], Step [136/275], Loss: 0.2376\n",
      "Epoch [28/32], Step [170/275], Loss: 0.2832\n",
      "Epoch [28/32], Step [204/275], Loss: 0.2889\n",
      "Epoch [28/32], Step [238/275], Loss: 0.3705\n",
      "Epoch [28/32], Step [272/275], Loss: 0.2679\n",
      "Train phase -  Epoch 28 Loss: 0.2141 Acc: 0.6376\n",
      "Epoch [28/32], Step [14/118], Loss: 0.5008\n",
      "Epoch [28/32], Step [28/118], Loss: 0.5779\n",
      "Epoch [28/32], Step [42/118], Loss: 0.6031\n",
      "Epoch [28/32], Step [56/118], Loss: 0.7224\n",
      "Epoch [28/32], Step [70/118], Loss: 0.5424\n",
      "Epoch [28/32], Step [84/118], Loss: 0.6615\n",
      "Epoch [28/32], Step [98/118], Loss: 0.7741\n",
      "Epoch [28/32], Step [112/118], Loss: 0.4886\n",
      "Validation phase -  Epoch 28 Loss: 0.1725 Acc: 0.2468\n",
      "----------\n",
      "Epoch 29/32\n",
      "----------\n",
      "Epoch [29/32], Step [34/275], Loss: 0.2901\n",
      "Epoch [29/32], Step [68/275], Loss: 0.2522\n",
      "Epoch [29/32], Step [102/275], Loss: 0.3309\n",
      "Epoch [29/32], Step [136/275], Loss: 0.3204\n",
      "Epoch [29/32], Step [170/275], Loss: 0.3228\n",
      "Epoch [29/32], Step [204/275], Loss: 0.2274\n",
      "Epoch [29/32], Step [238/275], Loss: 0.2684\n",
      "Epoch [29/32], Step [272/275], Loss: 0.3019\n",
      "Train phase -  Epoch 29 Loss: 0.2066 Acc: 0.6398\n",
      "Epoch [29/32], Step [14/118], Loss: 0.4673\n",
      "Epoch [29/32], Step [28/118], Loss: 0.6015\n",
      "Epoch [29/32], Step [42/118], Loss: 0.3146\n",
      "Epoch [29/32], Step [56/118], Loss: 0.6110\n",
      "Epoch [29/32], Step [70/118], Loss: 0.5784\n",
      "Epoch [29/32], Step [84/118], Loss: 0.3658\n",
      "Epoch [29/32], Step [98/118], Loss: 0.4704\n",
      "Epoch [29/32], Step [112/118], Loss: 0.6150\n",
      "Validation phase -  Epoch 29 Loss: 0.1714 Acc: 0.2480\n",
      "----------\n",
      "Epoch 30/32\n",
      "----------\n",
      "Epoch [30/32], Step [34/275], Loss: 0.3970\n",
      "Epoch [30/32], Step [68/275], Loss: 0.4099\n",
      "Epoch [30/32], Step [102/275], Loss: 0.3360\n",
      "Epoch [30/32], Step [136/275], Loss: 0.2385\n",
      "Epoch [30/32], Step [170/275], Loss: 0.3184\n",
      "Epoch [30/32], Step [204/275], Loss: 0.2502\n",
      "Epoch [30/32], Step [238/275], Loss: 0.3506\n",
      "Epoch [30/32], Step [272/275], Loss: 0.2733\n",
      "Train phase -  Epoch 30 Loss: 0.2125 Acc: 0.6377\n",
      "Epoch [30/32], Step [14/118], Loss: 0.5850\n",
      "Epoch [30/32], Step [28/118], Loss: 0.6676\n",
      "Epoch [30/32], Step [42/118], Loss: 0.6135\n",
      "Epoch [30/32], Step [56/118], Loss: 0.4717\n",
      "Epoch [30/32], Step [70/118], Loss: 0.7157\n",
      "Epoch [30/32], Step [84/118], Loss: 0.4921\n",
      "Epoch [30/32], Step [98/118], Loss: 0.5303\n",
      "Epoch [30/32], Step [112/118], Loss: 0.6175\n",
      "Validation phase -  Epoch 30 Loss: 0.1688 Acc: 0.2479\n",
      "----------\n",
      "Epoch 31/32\n",
      "----------\n",
      "Epoch [31/32], Step [34/275], Loss: 0.2600\n",
      "Epoch [31/32], Step [68/275], Loss: 0.2386\n",
      "Epoch [31/32], Step [102/275], Loss: 0.3140\n",
      "Epoch [31/32], Step [136/275], Loss: 0.3033\n",
      "Epoch [31/32], Step [170/275], Loss: 0.3016\n",
      "Epoch [31/32], Step [204/275], Loss: 0.2725\n",
      "Epoch [31/32], Step [238/275], Loss: 0.2826\n",
      "Epoch [31/32], Step [272/275], Loss: 0.2668\n",
      "Train phase -  Epoch 31 Loss: 0.2061 Acc: 0.6398\n",
      "Epoch [31/32], Step [14/118], Loss: 0.3307\n",
      "Epoch [31/32], Step [28/118], Loss: 0.4942\n",
      "Epoch [31/32], Step [42/118], Loss: 0.4659\n",
      "Epoch [31/32], Step [56/118], Loss: 0.6087\n",
      "Epoch [31/32], Step [70/118], Loss: 0.5701\n",
      "Epoch [31/32], Step [84/118], Loss: 0.7597\n",
      "Epoch [31/32], Step [98/118], Loss: 0.6732\n",
      "Epoch [31/32], Step [112/118], Loss: 0.8097\n",
      "Validation phase -  Epoch 31 Loss: 0.1706 Acc: 0.2482\n",
      "----------\n",
      "Epoch 32/32\n",
      "----------\n",
      "Epoch [32/32], Step [34/275], Loss: 0.2899\n",
      "Epoch [32/32], Step [68/275], Loss: 0.2634\n",
      "Epoch [32/32], Step [102/275], Loss: 0.2722\n",
      "Epoch [32/32], Step [136/275], Loss: 0.3581\n",
      "Epoch [32/32], Step [170/275], Loss: 0.1861\n",
      "Epoch [32/32], Step [204/275], Loss: 0.4360\n",
      "Epoch [32/32], Step [238/275], Loss: 0.3045\n",
      "Epoch [32/32], Step [272/275], Loss: 0.3997\n",
      "Train phase -  Epoch 32 Loss: 0.2038 Acc: 0.6396\n",
      "Epoch [32/32], Step [14/118], Loss: 0.5849\n",
      "Epoch [32/32], Step [28/118], Loss: 0.6403\n",
      "Epoch [32/32], Step [42/118], Loss: 0.6626\n",
      "Epoch [32/32], Step [56/118], Loss: 0.6799\n",
      "Epoch [32/32], Step [70/118], Loss: 0.5621\n",
      "Epoch [32/32], Step [84/118], Loss: 0.5509\n",
      "Epoch [32/32], Step [98/118], Loss: 0.4899\n",
      "Epoch [32/32], Step [112/118], Loss: 0.6404\n",
      "Validation phase -  Epoch 32 Loss: 0.1723 Acc: 0.2469\n",
      "Training complete in 26m 3s\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Saving the trained model",
   "id": "e4d9605ef361a0d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T15:57:21.074574Z",
     "start_time": "2024-05-01T15:57:20.659090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('Finished Training')\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "PATH = './models/trained_model.pth'\n",
    "torch.save(trained_model, PATH)"
   ],
   "id": "9f5f26cdefec21bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Loading the model",
   "id": "6f3fea444bec58bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T15:57:21.242960Z",
     "start_time": "2024-05-01T15:57:21.075639Z"
    }
   },
   "cell_type": "code",
   "source": "trained_model = torch.load('./models/trained_model.pth', map_location=device)",
   "id": "fb1aa946a282ac0f",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluating the model",
   "id": "9e3a434c2806a2e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T15:57:21.247586Z",
     "start_time": "2024-05-01T15:57:21.243945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_model(model, dataloader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_corrects = 0\n",
    "\n",
    "    # Disable gradient calculation to speed up the process and reduce memory usage\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass to get output/logits\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Get predictions from the maximum value\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # Increment the correct predictions count\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # Optionally print progress every 250 batches\n",
    "            if (i + 1) % 250 == 0:\n",
    "                print(f'Evaluating: [{i + 1}/{len(dataloader)}],  Correct classified: {running_corrects}/{i + 1}')\n",
    "\n",
    "    # Calculate the accuracy by dividing the number of correct predictions by the dataset size\n",
    "    test_acc = running_corrects.double() / len(dataloader)\n",
    "    print(f'Test Acc: {test_acc:.4f}, Correct classified: {running_corrects}/{len(dataloader)}')\n",
    "\n",
    "    return test_acc"
   ],
   "id": "6488b43f7e58c5d",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T15:59:58.693160Z",
     "start_time": "2024-05-01T15:57:21.249269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_loader = DataLoader(test_dataset, shuffle=True)\n",
    "\n",
    "test_model(trained_model, test_loader, device)"
   ],
   "id": "5b3a78850fc2ef92",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: [250/12630],  Correct classified: 162/250\n",
      "Evaluating: [500/12630],  Correct classified: 329/500\n",
      "Evaluating: [750/12630],  Correct classified: 483/750\n",
      "Evaluating: [1000/12630],  Correct classified: 625/1000\n",
      "Evaluating: [1250/12630],  Correct classified: 781/1250\n",
      "Evaluating: [1500/12630],  Correct classified: 916/1500\n",
      "Evaluating: [1750/12630],  Correct classified: 1079/1750\n",
      "Evaluating: [2000/12630],  Correct classified: 1216/2000\n",
      "Evaluating: [2250/12630],  Correct classified: 1355/2250\n",
      "Evaluating: [2500/12630],  Correct classified: 1502/2500\n",
      "Evaluating: [2750/12630],  Correct classified: 1648/2750\n",
      "Evaluating: [3000/12630],  Correct classified: 1803/3000\n",
      "Evaluating: [3250/12630],  Correct classified: 1941/3250\n",
      "Evaluating: [3500/12630],  Correct classified: 2073/3500\n",
      "Evaluating: [3750/12630],  Correct classified: 2236/3750\n",
      "Evaluating: [4000/12630],  Correct classified: 2382/4000\n",
      "Evaluating: [4250/12630],  Correct classified: 2524/4250\n",
      "Evaluating: [4500/12630],  Correct classified: 2665/4500\n",
      "Evaluating: [4750/12630],  Correct classified: 2815/4750\n",
      "Evaluating: [5000/12630],  Correct classified: 2964/5000\n",
      "Evaluating: [5250/12630],  Correct classified: 3122/5250\n",
      "Evaluating: [5500/12630],  Correct classified: 3278/5500\n",
      "Evaluating: [5750/12630],  Correct classified: 3423/5750\n",
      "Evaluating: [6000/12630],  Correct classified: 3563/6000\n",
      "Evaluating: [6250/12630],  Correct classified: 3717/6250\n",
      "Evaluating: [6500/12630],  Correct classified: 3863/6500\n",
      "Evaluating: [6750/12630],  Correct classified: 4009/6750\n",
      "Evaluating: [7000/12630],  Correct classified: 4154/7000\n",
      "Evaluating: [7250/12630],  Correct classified: 4290/7250\n",
      "Evaluating: [7500/12630],  Correct classified: 4436/7500\n",
      "Evaluating: [7750/12630],  Correct classified: 4596/7750\n",
      "Evaluating: [8000/12630],  Correct classified: 4744/8000\n",
      "Evaluating: [8250/12630],  Correct classified: 4887/8250\n",
      "Evaluating: [8500/12630],  Correct classified: 5042/8500\n",
      "Evaluating: [8750/12630],  Correct classified: 5193/8750\n",
      "Evaluating: [9000/12630],  Correct classified: 5346/9000\n",
      "Evaluating: [9250/12630],  Correct classified: 5484/9250\n",
      "Evaluating: [9500/12630],  Correct classified: 5635/9500\n",
      "Evaluating: [9750/12630],  Correct classified: 5790/9750\n",
      "Evaluating: [10000/12630],  Correct classified: 5944/10000\n",
      "Evaluating: [10250/12630],  Correct classified: 6088/10250\n",
      "Evaluating: [10500/12630],  Correct classified: 6235/10500\n",
      "Evaluating: [10750/12630],  Correct classified: 6384/10750\n",
      "Evaluating: [11000/12630],  Correct classified: 6545/11000\n",
      "Evaluating: [11250/12630],  Correct classified: 6696/11250\n",
      "Evaluating: [11500/12630],  Correct classified: 6856/11500\n",
      "Evaluating: [11750/12630],  Correct classified: 7004/11750\n",
      "Evaluating: [12000/12630],  Correct classified: 7144/12000\n",
      "Evaluating: [12250/12630],  Correct classified: 7295/12250\n",
      "Evaluating: [12500/12630],  Correct classified: 7450/12500\n",
      "Test Acc: 0.5960, Correct classified: 7528/12630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.5960, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
