{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "95feb99553e5f381"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-30T10:42:31.839719Z",
     "start_time": "2024-04-30T10:42:27.907026Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Setting device for the computation",
   "id": "f26a8210e0afc41d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T10:42:31.847442Z",
     "start_time": "2024-04-30T10:42:31.839719Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
   "id": "2c3071b3341a7e3e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Resizing all the images to the same size",
   "id": "34b616576f3daeb7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T10:59:45.666153Z",
     "start_time": "2024-04-30T10:59:45.646738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_transform = transforms.Compose([\n",
    "        #TODO:think a better transformation pipeline\n",
    "        #naive transformation\n",
    "        transforms.Resize((64,64)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "test_transform = transforms.Compose([\n",
    "        #TODO:think a better transformation pipeline\n",
    "        #naive transformation\n",
    "        transforms.Resize((64,64)),\n",
    "        transforms.ToTensor()\n",
    "    ])"
   ],
   "id": "50d7d9e3cbf71956",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Loading the train dataset",
   "id": "1e0e1c4761d44bcd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:11:20.981814Z",
     "start_time": "2024-04-30T11:11:20.747438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dir='./dataset/GTSRB/train'\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir,train_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=200,shuffle=True, num_workers=4)\n",
    "\n",
    "train_total_steps = len(train_loader)\n",
    "train_size = len(train_dataset)\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "print('Total steps for training:',train_total_steps)\n",
    "print('Train size:',train_size)\n",
    "print('Class names:',class_names)"
   ],
   "id": "233a264dd2428bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps for training: 393\n",
      "Train size: 39209\n",
      "Class names: ['00000', '00001', '00002', '00003', '00004', '00005', '00006', '00007', '00008', '00009', '00010', '00011', '00012', '00013', '00014', '00015', '00016', '00017', '00018', '00019', '00020', '00021', '00022', '00023', '00024', '00025', '00026', '00027', '00028', '00029', '00030', '00031', '00032', '00033', '00034', '00035', '00036', '00037', '00038', '00039', '00040', '00041', '00042']\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1239b390bd17bb84"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Loading the test dataset",
   "id": "8199467383be87c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:11:24.485685Z",
     "start_time": "2024-04-30T11:11:24.310274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_dir='./dataset/GTSRB/test'\n",
    "\n",
    "test_dataset = datasets.ImageFolder(test_dir,test_transform)\n",
    "\n",
    "test_loader = DataLoader(test_dataset,batch_size=200, shuffle=True, num_workers=0)\n",
    "\n",
    "test_size = len(test_dataset)\n",
    "class_names = test_dataset.classes\n",
    "\n",
    "print('Test size:',train_size)\n",
    "print('Class names:',class_names)"
   ],
   "id": "4757b88147ac3fbf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: 39209\n",
      "Class names: ['00000', '00001', '00002', '00003', '00004', '00005', '00006', '00007', '00008', '00009', '00010', '00011', '00012', '00013', '00014', '00015', '00016', '00017', '00018', '00019', '00020', '00021', '00022', '00023', '00024', '00025', '00026', '00027', '00028', '00029', '00030', '00031', '00032', '00033', '00034', '00035', '00036', '00037', '00038', '00039', '00040', '00041', '00042']\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Defining the training phase",
   "id": "3191a2bde55202e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:29:04.951429Z",
     "start_time": "2024-04-30T11:29:04.930624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    #best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        #iterates over the batches\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "    \n",
    "            # forward pass\n",
    "            outputs = model(images) #compute predictions\n",
    "            _, preds = torch.max(outputs, 1) #maximum value along dimension 1\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backward + optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #prints the stats every 20 steps (20 batches performed)\n",
    "            if (i+1) % 20 == 0:\n",
    "                print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{train_total_steps}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "            # statistics\n",
    "            running_loss += loss.item() * images.size(0) # scalar loss * n.of samples in the batch: necessary to average the loss over the batch\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            #to update weights\n",
    "            scheduler.step()\n",
    "    \n",
    "        epoch_loss = running_loss / train_size\n",
    "        epoch_acc = running_corrects.double() / train_size\n",
    "    \n",
    "        print('{} Epoch {} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "            'Train phase - ',epoch, epoch_loss, epoch_acc))\n",
    "    \n",
    "    print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    return model"
   ],
   "id": "d1388a73ca0b9760",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Defining the model using ResNet18 as backbone",
   "id": "6a9c4fc45c415b8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T11:33:16.510506Z",
     "start_time": "2024-04-30T11:29:08.679943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "#### ConvNet as fixed feature extractor ####\n",
    "# freeze all the network parameters except the final layer settings, to not compute the gradients backward\n",
    "model_conv = torchvision.models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 43)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized\n",
    "optimizer_conv = torch.optim.Adam(model_conv.fc.parameters(), lr=0.01)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "\n",
    "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                        exp_lr_scheduler, num_epochs=20)"
   ],
   "id": "731771455515444c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n",
      "Epoch [1/20], Step [20/393], Loss: 3.0241\n",
      "Epoch [1/20], Step [40/393], Loss: 3.0280\n",
      "Epoch [1/20], Step [60/393], Loss: 2.9532\n",
      "Epoch [1/20], Step [80/393], Loss: 2.7694\n",
      "Epoch [1/20], Step [100/393], Loss: 2.9545\n",
      "Epoch [1/20], Step [120/393], Loss: 3.3594\n",
      "Epoch [1/20], Step [140/393], Loss: 2.8523\n",
      "Epoch [1/20], Step [160/393], Loss: 3.0209\n",
      "Epoch [1/20], Step [180/393], Loss: 2.4268\n",
      "Epoch [1/20], Step [200/393], Loss: 3.0993\n",
      "Epoch [1/20], Step [220/393], Loss: 2.6250\n",
      "Epoch [1/20], Step [240/393], Loss: 2.2967\n",
      "Epoch [1/20], Step [260/393], Loss: 2.7128\n",
      "Epoch [1/20], Step [280/393], Loss: 2.8641\n",
      "Epoch [1/20], Step [300/393], Loss: 3.0498\n",
      "Epoch [1/20], Step [320/393], Loss: 3.0260\n",
      "Epoch [1/20], Step [340/393], Loss: 3.2138\n",
      "Epoch [1/20], Step [360/393], Loss: 2.9020\n",
      "Epoch [1/20], Step [380/393], Loss: 3.4047\n",
      "train Epoch 0 Loss: 2.9393 Acc: 0.3288\n",
      "Epoch 1/19\n",
      "----------\n",
      "Epoch [2/20], Step [20/393], Loss: 3.2393\n",
      "Epoch [2/20], Step [40/393], Loss: 3.0915\n",
      "Epoch [2/20], Step [60/393], Loss: 2.6384\n",
      "Epoch [2/20], Step [80/393], Loss: 3.4383\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[69], line 23\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# Decay LR by a factor of 0.1 every 7 epochs\u001B[39;00m\n\u001B[0;32m     21\u001B[0m exp_lr_scheduler \u001B[38;5;241m=\u001B[39m lr_scheduler\u001B[38;5;241m.\u001B[39mStepLR(optimizer_conv, step_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m7\u001B[39m, gamma\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m)\n\u001B[1;32m---> 23\u001B[0m model_conv \u001B[38;5;241m=\u001B[39m train_model(model_conv, criterion, optimizer_conv,\n\u001B[0;32m     24\u001B[0m                         exp_lr_scheduler, num_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m20\u001B[39m)\n",
      "Cell \u001B[1;32mIn[68], line 19\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, criterion, optimizer, scheduler, num_epochs)\u001B[0m\n\u001B[0;32m     16\u001B[0m labels \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# forward pass\u001B[39;00m\n\u001B[1;32m---> 19\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(images) \u001B[38;5;66;03m#compute predictions\u001B[39;00m\n\u001B[0;32m     20\u001B[0m _, preds \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(outputs, \u001B[38;5;241m1\u001B[39m) \u001B[38;5;66;03m#maximum value along dimension 1\u001B[39;00m\n\u001B[0;32m     21\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Traffic-Sign-Recognition\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Traffic-Sign-Recognition\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Traffic-Sign-Recognition\\Lib\\site-packages\\torchvision\\models\\resnet.py:285\u001B[0m, in \u001B[0;36mResNet.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 285\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_impl(x)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Traffic-Sign-Recognition\\Lib\\site-packages\\torchvision\\models\\resnet.py:274\u001B[0m, in \u001B[0;36mResNet._forward_impl\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    271\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmaxpool(x)\n\u001B[0;32m    273\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer1(x)\n\u001B[1;32m--> 274\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer2(x)\n\u001B[0;32m    275\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer3(x)\n\u001B[0;32m    276\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer4(x)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Traffic-Sign-Recognition\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1507\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1504\u001B[0m             tracing_state\u001B[38;5;241m.\u001B[39mpop_scope()\n\u001B[0;32m   1505\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[1;32m-> 1507\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_wrapped_call_impl\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m   1508\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1509\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Saving the trained model",
   "id": "e4d9605ef361a0d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T14:49:05.218210Z",
     "start_time": "2024-04-28T14:49:05.134333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('Finished Training')\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "PATH = './models/trained_model.pth'\n",
    "torch.save(model_conv, PATH)"
   ],
   "id": "9f5f26cdefec21bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "execution_count": 172
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Loading the model",
   "id": "6f3fea444bec58bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T09:25:22.630003Z",
     "start_time": "2024-04-30T09:25:22.543779Z"
    }
   },
   "cell_type": "code",
   "source": "trained_model=torch.load('./models/trained_model.pth',map_location=device)",
   "id": "fb1aa946a282ac0f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluating the model",
   "id": "9e3a434c2806a2e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T09:29:03.751274Z",
     "start_time": "2024-04-30T09:29:03.732161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_model(model, dataloader, dataset_size):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    running_corrects = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for i, (images, labels) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "    \n",
    "            # Statistics\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            if (i+1) % 50 == 0:\n",
    "                print ( f'Evaluating: [{i+1}/{test_size}]')\n",
    "\n",
    "    # Calculate accuracy\n",
    "    test_acc = running_corrects.double() / dataset_size\n",
    "\n",
    "    print('Test Acc: {:.4f}'.format( test_acc))\n",
    "    "
   ],
   "id": "6488b43f7e58c5d",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T09:30:00.121109Z",
     "start_time": "2024-04-30T09:29:05.614173Z"
    }
   },
   "cell_type": "code",
   "source": "test_model(trained_model, test_loader, test_size)",
   "id": "5b3a78850fc2ef92",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: [50/12630]\n",
      "Test Acc: 0.3157\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
