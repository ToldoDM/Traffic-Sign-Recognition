{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "95feb99553e5f381"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from collections import OrderedDict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.random.seed(123)  # for replication\n",
    "os.makedirs('./models', exist_ok=True)"
   ],
   "id": "625f7f9d2019b024",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Helper functions",
   "id": "920e26d20bf8c3c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_classes_preds(images, labels, preds, probs):\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx + 1, xticks=[], yticks=[])\n",
    "        norm_img = cv2.normalize(images[idx].cpu().numpy(), None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "        rgb_img = np.transpose(norm_img, (1, 2, 0)).astype(np.uint8)\n",
    "        plt.imshow(rgb_img)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            preds[idx],\n",
    "            probs[idx] * 100.0,\n",
    "            labels[idx]),\n",
    "            color=(\"green\" if preds[idx] == labels[idx].item() else \"red\"))\n",
    "    return fig\n",
    "\n",
    "\n",
    "def summary(model, input_size, batch_size=-1, device=\"cuda\"):\n",
    "    output_str = ''\n",
    "\n",
    "    def register_hook(module):\n",
    "\n",
    "        def hook(module, input, output):\n",
    "            class_name = str(module.__class__).split(\".\")[-1].split(\"'\")[0]\n",
    "            module_idx = len(summary)\n",
    "\n",
    "            m_key = \"%s-%i\" % (class_name, module_idx + 1)\n",
    "            summary[m_key] = OrderedDict()\n",
    "            summary[m_key][\"input_shape\"] = list(input[0].size())\n",
    "            summary[m_key][\"input_shape\"][0] = batch_size\n",
    "            if isinstance(output, (list, tuple)):\n",
    "                summary[m_key][\"output_shape\"] = [\n",
    "                    [-1] + list(o.size())[1:] for o in output\n",
    "                ]\n",
    "            else:\n",
    "                summary[m_key][\"output_shape\"] = list(output.size())\n",
    "                summary[m_key][\"output_shape\"][0] = batch_size\n",
    "\n",
    "            params = 0\n",
    "            if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.weight.size())))\n",
    "                summary[m_key][\"trainable\"] = module.weight.requires_grad\n",
    "            if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.bias.size())))\n",
    "            summary[m_key][\"nb_params\"] = params\n",
    "\n",
    "        if (\n",
    "                not isinstance(module, nn.Sequential)\n",
    "                and not isinstance(module, nn.ModuleList)\n",
    "                and not (module == model)\n",
    "        ):\n",
    "            hooks.append(module.register_forward_hook(hook))\n",
    "\n",
    "    device = device.lower()\n",
    "    assert device in [\n",
    "        \"cuda\",\n",
    "        \"cpu\",\n",
    "    ], \"Input device is not valid, please specify 'cuda' or 'cpu'\"\n",
    "\n",
    "    if device == \"cuda\" and torch.cuda.is_available():\n",
    "        dtype = torch.cuda.FloatTensor\n",
    "    else:\n",
    "        dtype = torch.FloatTensor\n",
    "\n",
    "    # multiple inputs to the network\n",
    "    if isinstance(input_size, tuple):\n",
    "        input_size = [input_size]\n",
    "\n",
    "    # batch_size of 2 for batchnorm\n",
    "    x = [torch.rand(2, *in_size).type(dtype) for in_size in input_size]\n",
    "    # print(type(x[0]))\n",
    "\n",
    "    # create properties\n",
    "    summary = OrderedDict()\n",
    "    hooks = []\n",
    "\n",
    "    # register hook\n",
    "    model.apply(register_hook)\n",
    "\n",
    "    # make a forward pass\n",
    "    # print(x.shape)\n",
    "    model(*x)\n",
    "\n",
    "    # remove these hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    output_str += \"----------------------------------------------------------------\\n\"\n",
    "    line_new = \"{:>20}  {:>25} {:>15}\".format(\"Layer (type)\", \"Output Shape\", \"Param #\\n\")\n",
    "    output_str += line_new\n",
    "    output_str += \"================================================================\\n\"\n",
    "    total_params = 0\n",
    "    total_output = 0\n",
    "    trainable_params = 0\n",
    "    for layer in summary:\n",
    "        # input_shape, output_shape, trainable, nb_params\n",
    "        line_new = \"{:>20}  {:>25} {:>15}\\n\".format(\n",
    "            layer,\n",
    "            str(summary[layer][\"output_shape\"]),\n",
    "            \"{0:,}\".format(summary[layer][\"nb_params\"]),\n",
    "        )\n",
    "        total_params += summary[layer][\"nb_params\"]\n",
    "        total_output += np.prod(summary[layer][\"output_shape\"])\n",
    "        if \"trainable\" in summary[layer]:\n",
    "            if summary[layer][\"trainable\"] == True:\n",
    "                trainable_params += summary[layer][\"nb_params\"]\n",
    "        output_str += line_new\n",
    "\n",
    "    # assume 4 bytes/number (float on cuda).\n",
    "    total_input_size = abs(np.prod(input_size) * batch_size * 4. / (1024 ** 2.))\n",
    "    total_output_size = abs(2. * total_output * 4. / (1024 ** 2.))  # x2 for gradients\n",
    "    total_params_size = abs(total_params.numpy() * 4. / (1024 ** 2.))\n",
    "    total_size = total_params_size + total_output_size + total_input_size\n",
    "\n",
    "    output_str += \"================================================================\\n\"\n",
    "    output_str += (\"Total params: {0:,}\\n\".format(total_params))\n",
    "    output_str += (\"Trainable params: {0:,}\\n\".format(trainable_params))\n",
    "    output_str += (\"Non-trainable params: {0:,}\\n\".format(total_params - trainable_params))\n",
    "    output_str += \"----------------------------------------------------------------\\n\"\n",
    "    output_str += (\"Input size (MB): %0.2f\\n\" % total_input_size)\n",
    "    output_str += (\"Forward/backward pass size (MB): %0.2f\\n\" % total_output_size)\n",
    "    output_str += (\"Params size (MB): %0.2f\\n\" % total_params_size)\n",
    "    output_str += (\"Estimated Total Size (MB): %0.2f\\n\" % total_size)\n",
    "    output_str += \"----------------------------------------------------------------\\n\"\n",
    "    print(output_str)\n",
    "    return output_str"
   ],
   "id": "5b5b2a8dbc382e69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Loading the train dataset",
   "id": "1e0e1c4761d44bcd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_dir = './dataset/GTSRB/train'\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, ResNet50_Weights.IMAGENET1K_V2.transforms())\n",
    "train_size = len(train_dataset)\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "print('Train size:', train_size)\n",
    "print('Class names:', class_names)"
   ],
   "id": "233a264dd2428bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1239b390bd17bb84"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Loading the test dataset",
   "id": "8199467383be87c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_dir = './dataset/GTSRB/test'\n",
    "\n",
    "test_dataset = datasets.ImageFolder(test_dir, ResNet50_Weights.IMAGENET1K_V2.transforms())\n",
    "test_size = len(test_dataset)\n",
    "class_names = test_dataset.classes\n",
    "\n",
    "print('Test size:', train_size)\n",
    "print('Class names:', class_names)"
   ],
   "id": "4757b88147ac3fbf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Defining the training phase",
   "id": "3191a2bde55202e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_model(device, model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=25,\n",
    "                model_name='trained_model'):\n",
    "    since = time.time()\n",
    "    time_train = 0\n",
    "    time_val = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('-' * 10)\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Choose the appropriate data loader\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                data_total_steps = len(train_loader)\n",
    "                data_loader = train_loader\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "                data_total_steps = len(val_loader)\n",
    "                data_loader = val_loader\n",
    "\n",
    "            for i, (images, labels) in enumerate(data_loader):\n",
    "                # time_t = epoch * len(data_loader) * i + i\n",
    "\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward\n",
    "                # Track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(images)\n",
    "                    softmax_outputs = F.softmax(outputs, dim=1)\n",
    "                    probs, preds = torch.max(softmax_outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * images.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                # Calculate entropy with epsilon\n",
    "                epsilon = 1e-10  # Small epsilon value to avoid zero probabilities\n",
    "                entropy = -torch.sum(softmax_outputs * torch.log2(softmax_outputs + epsilon), dim=1).mean()\n",
    "\n",
    "                #prints the stats every 20 steps (20 batches performed)\n",
    "                if (i + 1) % int(data_total_steps / 8) == 0:\n",
    "                    print(\n",
    "                        f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{data_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "                    # Log image predictions\n",
    "                    selected_indices = random.sample(range(len(images)), 4)  # Select 4 random indices\n",
    "                    selected_images = images[selected_indices]\n",
    "                    selected_labels = labels[selected_indices]\n",
    "                    selected_preds = preds[selected_indices]\n",
    "                    selected_probs = probs[selected_indices]\n",
    "                    if phase == 'train':\n",
    "                        writer.add_figure('Training/Training Predictions',\n",
    "                                          plot_classes_preds(selected_images, selected_labels, selected_preds,\n",
    "                                                             selected_probs),\n",
    "                                          global_step=time_train)\n",
    "                    else:\n",
    "                        writer.add_figure('Training/Validation Predictions',\n",
    "                                          plot_classes_preds(selected_images, selected_labels, selected_preds,\n",
    "                                                             selected_probs),\n",
    "                                          global_step=time_val)\n",
    "\n",
    "                # Log scalars\n",
    "                if phase == 'train':\n",
    "                    writer.add_scalar('Training/Training Loss',\n",
    "                                      loss.item(),\n",
    "                                      time_train)\n",
    "                    writer.add_scalar('Policy/Entropy',\n",
    "                                      entropy.item(),\n",
    "                                      time_train)\n",
    "                    writer.add_scalar('Policy/Learning Rate',\n",
    "                                      np.array(scheduler.get_last_lr()),\n",
    "                                      time_train)\n",
    "                    time_train += 1\n",
    "                else:\n",
    "                    writer.add_scalar('Training/Validation Loss',\n",
    "                                      loss.item(),\n",
    "                                      time_val)\n",
    "                    time_val += 1\n",
    "\n",
    "            epoch_loss = running_loss / len(data_loader.dataset)\n",
    "            epoch_acc = running_corrects.double() / len(data_loader.dataset)\n",
    "\n",
    "            if phase == 'train':\n",
    "                print('{} Epoch {} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                    'Train phase - ', epoch + 1, epoch_loss, epoch_acc))\n",
    "                writer.add_scalar('Training/Training Accuracy',\n",
    "                                  epoch_acc,\n",
    "                                  epoch)\n",
    "                if (epoch + 1) % max(int(num_epochs / 5), 1) == 0:  # checkpoint the model\n",
    "                    print(\"----> model checkpoint...\")\n",
    "                    torch.save(model, f'./models/trained_model_{model_name}_epoch_{epoch + 1}.pth')\n",
    "            else:\n",
    "                print('{} Epoch {} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                    'Validation phase - ', epoch + 1, epoch_loss, epoch_acc))\n",
    "                writer.add_scalar('Training/Validation Accuracy',\n",
    "                                  epoch_acc,\n",
    "                                  epoch)\n",
    "                scheduler.step(epoch_loss)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_dynamic_network(num_features, num_classes, neuron_list=None, dropout_values=None):\n",
    "    if neuron_list is None:\n",
    "        neuron_list = []\n",
    "    layers = []\n",
    "    num_layers = len(neuron_list)\n",
    "    # Input layer to first hidden layer\n",
    "    if num_layers > 0:\n",
    "        layers.append(nn.Linear(num_features, neuron_list[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        if dropout_values[0] != 0:\n",
    "            layers.append(nn.Dropout(dropout_values[0]))\n",
    "\n",
    "    # Additional hidden layers\n",
    "    for i in range(1, num_layers):\n",
    "        layers.append(nn.Linear(neuron_list[i - 1], neuron_list[i]))\n",
    "        layers.append(nn.ReLU())\n",
    "        if dropout_values[i] != 0:\n",
    "            layers.append(nn.Dropout(dropout_values[i]))\n",
    "\n",
    "    # Always include the final specified layer\n",
    "    layers.append(nn.Linear(neuron_list[-1] if num_layers > 0 else num_features, num_classes))\n",
    "    # layers.append(nn.Softmax(dim=1)) not needed cause cross entropy criterion\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ],
   "id": "d1388a73ca0b9760",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training Setup",
   "id": "596398e0642aca56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Setting device for the computation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "hyperparams = {\n",
    "    \"num_epochs\": 15,\n",
    "    \"batch_size\": 100,\n",
    "    #optimizer\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"momentum\": 0.0,\n",
    "    \"alpha\": 0.99,\n",
    "    \"beta1\": 0.9,\n",
    "    \"beta2\": 0.999,\n",
    "    \"epsilon\": 1e-07,\n",
    "    \"weight_decay\": 0,\n",
    "    #scheduler\n",
    "    \"decay_rate\": 0.5,\n",
    "    #nnet\n",
    "    # None layers means no hidden layers, just one layer from conv to classes: conv -> layer -> softmax\n",
    "    # Multiple layers can just be se as a list [500,400,300]\n",
    "    \"neuron_layer_list\": [512, 256, 128],\n",
    "    \"dropout_values\": [0.25, 0.25, 0.5],\n",
    "}"
   ],
   "id": "50d7d9e3cbf71956",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setting up the model using ResNet50 as backbone",
   "id": "7d53679acd744b28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "model_name = 'testfix'\n",
    "writer = SummaryWriter(f'runs/{model_name}')\n",
    "\n",
    "# Convert config dictionary to a formatted string\n",
    "hyper_str = \"\\n\".join(f\"{key}: {value}\\n\" for key, value in hyperparams.items())\n",
    "writer.add_text('Configuration', hyper_str)\n",
    "\n",
    "# Create DataLoader instances for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=hyperparams[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=hyperparams[\"batch_size\"], shuffle=True)\n",
    "\n",
    "# Model initialization\n",
    "model = torchvision.models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "# Define the layers you want to add\n",
    "model.fc = create_dynamic_network(model.fc.in_features, num_classes=43, neuron_list=hyperparams[\"neuron_layer_list\"],\n",
    "                                  dropout_values=hyperparams[\"dropout_values\"])\n",
    "model = model.to(device)\n",
    "writer.add_text(\"Model Summary\", summary(model, input_size=(3, 224, 224)))\n",
    "\n",
    "# Define loss function, optimizer, etc.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=hyperparams[\"learning_rate\"])\n",
    "# optimizer = torch.optim.SGD(model.fc.parameters(), lr=hyperparams[\"learning_rate\"], momentum=hyperparams[\"momentum\"],\n",
    "#                             weight_decay=hyperparams[\"weight_decay\"])\n",
    "# optimizer = torch.optim.SGD(model.fc.parameters(), lr=hyperparams[\"learning_rate\"], momentum=hyperparams[\"momentum\"],\n",
    "#                             weight_decay=hyperparams[\"weight_decay\"], nesterov=True)\n",
    "# optimizer = torch.optim.RMSprop(model.fc.parameters(), lr=hyperparams[\"learning_rate\"],\n",
    "#                                 weight_decay=hyperparams[\"weight_decay\"], alpha=hyperparams[\"alpha\"],\n",
    "#                                 eps=hyperparams[\"epsilon\"])\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, patience=1000, factor=hyperparams[\"decay_rate\"], min_lr=1e-4,\n",
    "                                           mode='min', threshold=1e-4)"
   ],
   "id": "d53ebf690ba17e92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train the model",
   "id": "6a9c4fc45c415b8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train model\n",
    "trained_model = train_model(device=device, model=model, criterion=criterion, optimizer=optimizer, scheduler=scheduler,\n",
    "                            train_loader=train_loader, val_loader=val_loader, num_epochs=hyperparams[\"num_epochs\"],\n",
    "                            model_name=model_name)"
   ],
   "id": "731771455515444c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Saving the trained model",
   "id": "e4d9605ef361a0d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print('Finished Training')\n",
    "PATH = f'./models/trained_model_{model_name}_final.pth'\n",
    "torch.save(trained_model, PATH)"
   ],
   "id": "9f5f26cdefec21bb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
