{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "95feb99553e5f381"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-03T17:39:49.848665Z",
     "start_time": "2024-05-03T17:39:49.839488Z"
    }
   },
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.models import ResNet152_Weights\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "import torch_directml"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:39:49.910165Z",
     "start_time": "2024-05-03T17:39:49.905165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(123)  # for replication\n",
    "os.makedirs('./models', exist_ok=True)"
   ],
   "id": "625f7f9d2019b024",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Helper functions",
   "id": "920e26d20bf8c3c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:39:49.926177Z",
     "start_time": "2024-05-03T17:39:49.914692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_classes_preds(images, labels, preds, probs):\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx + 1, xticks=[], yticks=[])\n",
    "        plt.imshow(np.transpose(images[idx].cpu().numpy(), (1, 2, 0)))  # because is a tensor \n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            preds[idx],\n",
    "            probs[idx] * 100.0,\n",
    "            labels[idx]),\n",
    "            color=(\"green\" if preds[idx] == labels[idx].item() else \"red\"))\n",
    "    return fig"
   ],
   "id": "5b5b2a8dbc382e69",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Loading the train dataset",
   "id": "1e0e1c4761d44bcd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:39:50.338270Z",
     "start_time": "2024-05-03T17:39:49.931461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Useless now since we use the required transformations for the ResNet50\n",
    "# train_transform = transforms.Compose([\n",
    "#     #naive transformation\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "train_dir = './dataset/GTSRB/train'\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, ResNet50_Weights.IMAGENET1K_V2.transforms())\n",
    "train_size = len(train_dataset)\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "print('Train size:', train_size)\n",
    "print('Class names:', class_names)"
   ],
   "id": "233a264dd2428bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 39209\n",
      "Class names: ['00000', '00001', '00002', '00003', '00004', '00005', '00006', '00007', '00008', '00009', '00010', '00011', '00012', '00013', '00014', '00015', '00016', '00017', '00018', '00019', '00020', '00021', '00022', '00023', '00024', '00025', '00026', '00027', '00028', '00029', '00030', '00031', '00032', '00033', '00034', '00035', '00036', '00037', '00038', '00039', '00040', '00041', '00042']\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1239b390bd17bb84"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Loading the test dataset",
   "id": "8199467383be87c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:39:50.445566Z",
     "start_time": "2024-05-03T17:39:50.340450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Useless now since we use the required transformations for the ResNet50\n",
    "# test_transform = transforms.Compose([\n",
    "#     #naive transformation\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "test_dir = './dataset/GTSRB/test'\n",
    "\n",
    "test_dataset = datasets.ImageFolder(test_dir, ResNet50_Weights.IMAGENET1K_V2.transforms())\n",
    "test_size = len(test_dataset)\n",
    "class_names = test_dataset.classes\n",
    "\n",
    "print('Test size:', train_size)\n",
    "print('Class names:', class_names)"
   ],
   "id": "4757b88147ac3fbf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: 39209\n",
      "Class names: ['00000', '00001', '00002', '00003', '00004', '00005', '00006', '00007', '00008', '00009', '00010', '00011', '00012', '00013', '00014', '00015', '00016', '00017', '00018', '00019', '00020', '00021', '00022', '00023', '00024', '00025', '00026', '00027', '00028', '00029', '00030', '00031', '00032', '00033', '00034', '00035', '00036', '00037', '00038', '00039', '00040', '00041', '00042']\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Defining the training phase",
   "id": "3191a2bde55202e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:39:50.491395Z",
     "start_time": "2024-05-03T17:39:50.447562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(device, model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=25,\n",
    "                model_name='trained_model'):\n",
    "    since = time.time()\n",
    "    time_train = 0\n",
    "    time_val = 0\n",
    "    \n",
    "    # Save the initial model\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('-' * 10)\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "\n",
    "            # Choose the appropriate data loader\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                data_total_steps = len(train_loader)\n",
    "                data_loader = train_loader\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "                data_total_steps = len(val_loader)\n",
    "                data_loader = val_loader\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            for i, (images, labels) in enumerate(data_loader):\n",
    "                # time_t = epoch * len(data_loader) * i + i\n",
    "\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                # Track history if only in train    \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(images)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    probs = [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, outputs)]\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * images.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "\n",
    "                # Calculate entropy with epsilon\n",
    "                softmax_outputs = F.softmax(outputs, dim=1)\n",
    "                epsilon = 1e-10  # Small epsilon value to avoid zero probabilities\n",
    "                entropy = -torch.sum(softmax_outputs * torch.log2(softmax_outputs + epsilon), dim=1).mean()\n",
    "\n",
    "                #prints the stats every 20 steps (20 batches performed)\n",
    "                if (i + 1) % int(data_total_steps / 8) == 0:\n",
    "                    print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{data_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "                    # Log image predictions\n",
    "                    selected_indices = random.sample(range(len(images)), 4)  # Select 4 random indices\n",
    "                    selected_images = images[selected_indices]\n",
    "                    selected_labels = labels[selected_indices]\n",
    "                    selected_preds = preds[selected_indices]\n",
    "                    selected_probs = [probs[i] for i in selected_indices]\n",
    "                    if phase == 'train':\n",
    "                        writer.add_figure('Training/Training Predictions',\n",
    "                                          plot_classes_preds(selected_images, selected_labels, selected_preds,\n",
    "                                                             selected_probs),\n",
    "                                          global_step=time_train)\n",
    "                    if phase == 'val':\n",
    "                        writer.add_figure('Training/Validation Predictions',\n",
    "                                          plot_classes_preds(selected_images, selected_labels, selected_preds,\n",
    "                                                             selected_probs),\n",
    "                                          global_step=time_val)\n",
    "\n",
    "                # Log scalars\n",
    "                if phase == 'train':\n",
    "                    writer.add_scalar('Training/Training Loss',\n",
    "                                      loss.item(),\n",
    "                                      time_train)\n",
    "                    writer.add_scalar('Policy/Entropy',\n",
    "                                      entropy.item(),\n",
    "                                      time_train)\n",
    "                    writer.add_scalar('Policy/Learning Rate',\n",
    "                                      np.array(optimizer.param_groups[0][\"lr\"]),\n",
    "                                      time_train)\n",
    "                    time_train += 1\n",
    "                if phase == 'val':\n",
    "                    writer.add_scalar('Training/Validation Loss',\n",
    "                                      loss.item(),\n",
    "                                      time_val)\n",
    "                    time_val += 1\n",
    "\n",
    "            epoch_loss = running_loss / len(data_loader.dataset)\n",
    "            epoch_acc = running_corrects.double() / len(data_loader.dataset)             \n",
    "            \n",
    "            if phase == 'train':\n",
    "                scheduler.step(epoch_loss)\n",
    "                \n",
    "                print('{} Epoch {} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                    'Train phase - ', epoch + 1, epoch_loss, epoch_acc))\n",
    "                writer.add_scalar('Training/Training Accuracy',\n",
    "                                  epoch_acc,\n",
    "                                  epoch)\n",
    "                if (epoch + 1) % max(int(num_epochs / 5), 1) == 0:  # checkpoint the model\n",
    "                    print(\"----> model checkpoint...\")\n",
    "                    torch.save(model, f'./models/trained_model_{model_name}_epoch_{epoch + 1}.pth')\n",
    "            if phase =='val':\n",
    "                # If the best accuracy is reached then save this model\n",
    "                if epoch_acc>best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts=copy.deepcopy(model.state.dict())\n",
    "                print('{} Epoch {} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                    'Validation phase - ', epoch + 1, epoch_loss, epoch_acc))\n",
    "                writer.add_scalar('Training/Validation Accuracy',\n",
    "                                  epoch_acc,\n",
    "                                  epoch)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    print('Best val Acc:{:.4f}'.format(best_acc))\n",
    "    # Return the model with the best accuracy in the validation\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_dynamic_network(num_features, num_classes, num_layers=0, num_neurons=1):\n",
    "    layers = []\n",
    "    # Input layer to first hidden layer\n",
    "    if num_layers > 0:\n",
    "        layers.append(nn.Linear(num_features, num_neurons))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "    # Additional hidden layers\n",
    "    for _ in range(1, num_layers):\n",
    "        layers.append(nn.Linear(num_neurons, num_neurons))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "    # Always include the final specified layer\n",
    "    layers.append(nn.Linear(num_neurons if num_layers > 0 else num_features, num_classes))\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def kaggle_nn(num_features,num_classes):\n",
    "    layers = [nn.Flatten(),\n",
    "              \n",
    "              nn.Linear(num_features, 512),\n",
    "              nn.ReLU(),\n",
    "              \n",
    "              nn.Linear(512, 256),\n",
    "              nn.ReLU(),\n",
    "              \n",
    "              nn.Linear(256, 128),\n",
    "              nn.ReLU(),\n",
    "              \n",
    "              nn.Linear(128, num_classes)]\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ],
   "id": "d1388a73ca0b9760",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training Setup",
   "id": "596398e0642aca56"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:39:50.506571Z",
     "start_time": "2024-05-03T17:39:50.496399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setting device for the computation\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch_directml.device()\n",
    "\n",
    "# Hyperparameters\n",
    "hyperparams = {\n",
    "    \"num_epochs\": 10,\n",
    "    \"batch_size\": 100,\n",
    "    #optimizer\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"momentum\": 0.9,\n",
    "    \"alpha\": 0.99,\n",
    "    \"beta1\": 0.9,\n",
    "    \"beta2\": 0.999,\n",
    "    \"epsilon\": 1e-08,\n",
    "    \"weight_decay\": 0,\n",
    "    #scheduler\n",
    "    \"decay_rate\": 0.5,\n",
    "    #nnet\n",
    "    \"num_layers\": 1,  # 0 layers means no hidden layers, just one layer from conv to classes: conv -> layer -> softmax\n",
    "    \"num_neurons\": 400,\n",
    "}"
   ],
   "id": "50d7d9e3cbf71956",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setting up the model using ResNet50 as backbone",
   "id": "7d53679acd744b28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:39:51.401511Z",
     "start_time": "2024-05-03T17:39:50.508572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "model_name = 'RMSprop'\n",
    "writer = SummaryWriter(f'runs/{model_name}')\n",
    "\n",
    "# Convert config dictionary to a formatted string\n",
    "hyper_str = \"\\n\".join(f\"{key}: {value}\\n\" for key, value in hyperparams.items())\n",
    "writer.add_text('Configuration', hyper_str)\n",
    "\n",
    "# Create DataLoader instances for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=hyperparams[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=hyperparams[\"batch_size\"], shuffle=True)\n",
    "\n",
    "# Model initialization\n",
    "model = torchvision.models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "# Define the layers you want to add\n",
    "#model.fc = create_dynamic_network(model.fc.in_features, 43, num_layers=hyperparams[\"num_layers\"],\n",
    "#                                  num_neurons=hyperparams[\"num_neurons\"])\n",
    "model.fc = kaggle_nn(model.fc.in_features,43)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function, optimizer, etc.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RMSprop(model.fc.parameters(),lr = hyperparams[\"learning_rate\"]) \n",
    "#optimizer = torch.optim.Adam(model.fc.parameters(), lr=hyperparams[\"learning_rate\"],\n",
    "#                             betas=(hyperparams[\"beta1\"], hyperparams[\"beta2\"]),\n",
    "#                             weight_decay=hyperparams[\"weight_decay\"], eps=hyperparams[\"epsilon\"])\n",
    "# optimizer = torch.optim.SGD(model.fc.parameters(), lr=hyperparams[\"learning_rate\"], momentum=hyperparams[\"momentum\"],\n",
    "#                             weight_decay=hyperparams[\"weight_decay\"])\n",
    "# optimizer = torch.optim.SGD(model.fc.parameters(), lr=hyperparams[\"learning_rate\"], momentum=hyperparams[\"momentum\"],\n",
    "#                             weight_decay=hyperparams[\"weight_decay\"], nesterov=True)\n",
    "# optimizer = torch.optim.RMSprop(model.fc.parameters(), lr=hyperparams[\"learning_rate\"],\n",
    "#                                 weight_decay=hyperparams[\"weight_decay\"], alpha=hyperparams[\"alpha\"],\n",
    "#                                 eps=hyperparams[\"epsilon\"])\n",
    "scheduler = lr_scheduler.LinearLR(optimizer)"
   ],
   "id": "d53ebf690ba17e92",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train the model",
   "id": "6a9c4fc45c415b8f"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-05-03T17:39:51.404668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train model\n",
    "trained_model = train_model(device=device, model=model, criterion=criterion, optimizer=optimizer, scheduler=scheduler,\n",
    "                            train_loader=train_loader, val_loader=val_loader, num_epochs=hyperparams[\"num_epochs\"],\n",
    "                            model_name=model_name)"
   ],
   "id": "731771455515444c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Epoch 1/10\n",
      "----------\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Saving the trained model",
   "id": "e4d9605ef361a0d3"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "print('Finished Training')\n",
    "PATH = f'./models/trained_model_{model_name}_final.pth'\n",
    "torch.save(trained_model, PATH)"
   ],
   "id": "9f5f26cdefec21bb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
