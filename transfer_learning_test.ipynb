{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "95feb99553e5f381"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-30T23:25:12.967844Z",
     "start_time": "2024-04-30T23:25:12.963251Z"
    }
   },
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.models import ResNet152_Weights"
   ],
   "outputs": [],
   "execution_count": 98
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Helper functions",
   "id": "920e26d20bf8c3c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T23:25:13.009301Z",
     "start_time": "2024-04-30T23:25:13.004713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_classes_preds(images, labels, preds, probs):\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx + 1, xticks=[], yticks=[])\n",
    "        plt.imshow(np.transpose(images[idx].T.cpu().numpy(), (1, 0, 2)))  # because is a tensor\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            preds[idx],\n",
    "            probs[idx] * 100.0,\n",
    "            labels[idx]),\n",
    "            color=(\"green\" if preds[idx] == labels[idx].item() else \"red\"))\n",
    "    return fig"
   ],
   "id": "5b5b2a8dbc382e69",
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Loading the train dataset",
   "id": "1e0e1c4761d44bcd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T23:25:13.164996Z",
     "start_time": "2024-04-30T23:25:13.010738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_transform = transforms.Compose([\n",
    "    #TODO:think a better transformation pipeline\n",
    "    #naive transformation\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dir = './dataset/GTSRB/train'\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, train_transform)\n",
    "train_size = len(train_dataset)\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "print('Train size:', train_size)\n",
    "print('Class names:', class_names)"
   ],
   "id": "233a264dd2428bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 39209\n",
      "Class names: ['00000', '00001', '00002', '00003', '00004', '00005', '00006', '00007', '00008', '00009', '00010', '00011', '00012', '00013', '00014', '00015', '00016', '00017', '00018', '00019', '00020', '00021', '00022', '00023', '00024', '00025', '00026', '00027', '00028', '00029', '00030', '00031', '00032', '00033', '00034', '00035', '00036', '00037', '00038', '00039', '00040', '00041', '00042']\n"
     ]
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1239b390bd17bb84"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Loading the test dataset",
   "id": "8199467383be87c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T23:25:13.209922Z",
     "start_time": "2024-04-30T23:25:13.166414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_transform = transforms.Compose([\n",
    "    #TODO:think a better transformation pipeline\n",
    "    #naive transformation\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_dir = './dataset/GTSRB/test'\n",
    "\n",
    "test_dataset = datasets.ImageFolder(test_dir, test_transform)\n",
    "test_size = len(test_dataset)\n",
    "class_names = test_dataset.classes\n",
    "\n",
    "print('Test size:', train_size)\n",
    "print('Class names:', class_names)"
   ],
   "id": "4757b88147ac3fbf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: 39209\n",
      "Class names: ['00000', '00001', '00002', '00003', '00004', '00005', '00006', '00007', '00008', '00009', '00010', '00011', '00012', '00013', '00014', '00015', '00016', '00017', '00018', '00019', '00020', '00021', '00022', '00023', '00024', '00025', '00026', '00027', '00028', '00029', '00030', '00031', '00032', '00033', '00034', '00035', '00036', '00037', '00038', '00039', '00040', '00041', '00042']\n"
     ]
    }
   ],
   "execution_count": 101
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Defining the training phase",
   "id": "3191a2bde55202e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T23:25:13.221261Z",
     "start_time": "2024-04-30T23:25:13.211067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(device, model, criterion, optimizer, scheduler, train_loader, num_epochs=25):\n",
    "    since = time.time()\n",
    "    train_total_steps = len(train_loader)\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('-' * 10)\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        data_loader = train_loader\n",
    "\n",
    "        for i, (images, labels) in enumerate(data_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            probs = [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, outputs)]\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward + optimize only if in training phase\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Statistics\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # Calculate entropy with epsilon\n",
    "            softmax_outputs = F.softmax(outputs, dim=1)\n",
    "            epsilon = 1e-10  # Small epsilon value to avoid zero probabilities\n",
    "            entropy = -torch.sum(softmax_outputs * torch.log2(softmax_outputs + epsilon), dim=1).mean()\n",
    "\n",
    "            # Log scalars\n",
    "            writer.add_scalar('training loss',\n",
    "                              loss.item(),\n",
    "                              epoch * len(train_loader) + i)\n",
    "            writer.add_scalar('entropy',\n",
    "                              entropy.item(),\n",
    "                              epoch * len(train_loader) + i)\n",
    "            writer.add_scalar('learning rate',\n",
    "                              np.array(scheduler.get_last_lr()),\n",
    "                              epoch * len(train_loader) + i)\n",
    "\n",
    "            #prints the stats every 20 steps (20 batches performed)\n",
    "            if (i + 1) % int(train_total_steps / 8) == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{train_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "                # Log image predictions\n",
    "                selected_indices = random.sample(range(len(images)), 4)  # Select 4 random indices\n",
    "                selected_images = images[selected_indices]\n",
    "                selected_labels = labels[selected_indices]\n",
    "                selected_preds = preds[selected_indices]\n",
    "                selected_probs = [probs[i] for i in selected_indices]\n",
    "                writer.add_figure('predictions vs. actuals',\n",
    "                                  plot_classes_preds(selected_images, selected_labels, selected_preds, selected_probs),\n",
    "                                  global_step=epoch * len(train_loader) + i)\n",
    "\n",
    "        epoch_loss = running_loss / len(data_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(data_loader.dataset)\n",
    "\n",
    "        print('{} Epoch {} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "            'Train phase - ', epoch + 1, epoch_loss, epoch_acc))\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_dynamic_network(num_features, num_classes, num_layers=0, num_neurons=1):\n",
    "    layers = []\n",
    "    # Input layer to first hidden layer\n",
    "    if num_layers > 0:\n",
    "        layers.append(nn.Linear(num_features, num_neurons))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "    # Additional hidden layers\n",
    "    for _ in range(1, num_layers):\n",
    "        layers.append(nn.Linear(num_neurons, num_neurons))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "    # Always include the final specified layer\n",
    "    layers.append(nn.Linear(num_neurons if num_layers > 0 else num_features, num_classes))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ],
   "id": "d1388a73ca0b9760",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training Setup",
   "id": "596398e0642aca56"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T23:25:13.231074Z",
     "start_time": "2024-04-30T23:25:13.222940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/TSR-SGD')\n",
    "\n",
    "# Setting device for the computation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 32\n",
    "batch_size = 100\n",
    "learning_rate = 0.05\n",
    "step_size = 2  # After how many epochs to apply the decay rate\n",
    "decay_rate = 0.9  # new_lr = Decay rate * learning rate\n",
    "\n",
    "num_layers = 1\n",
    "num_neurons = 100"
   ],
   "id": "50d7d9e3cbf71956",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setting up the model using ResNet152 as backbone",
   "id": "7d53679acd744b28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T23:25:14.294993Z",
     "start_time": "2024-04-30T23:25:13.232051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model initialization\n",
    "model = torchvision.models.resnet152(weights=ResNet152_Weights.IMAGENET1K_V2)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "# Define the layers you want to add\n",
    "model.fc = create_dynamic_network(model.fc.in_features, 43, num_layers=num_layers, num_neurons=num_neurons)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function, optimizer, etc.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.fc.parameters(), lr=learning_rate)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=decay_rate)"
   ],
   "id": "d53ebf690ba17e92",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train the model",
   "id": "6a9c4fc45c415b8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T23:50:14.505458Z",
     "start_time": "2024-04-30T23:25:14.296005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train model\n",
    "trained_model = train_model(device=device, model=model, criterion=criterion, optimizer=optimizer, scheduler=scheduler,\n",
    "                            train_loader=train_loader, num_epochs=num_epochs)"
   ],
   "id": "731771455515444c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Epoch 1/32\n",
      "----------\n",
      "Epoch [1/32], Step [49/393], Loss: 3.0118\n",
      "Epoch [1/32], Step [98/393], Loss: 2.5246\n",
      "Epoch [1/32], Step [147/393], Loss: 2.0781\n",
      "Epoch [1/32], Step [196/393], Loss: 1.7441\n",
      "Epoch [1/32], Step [245/393], Loss: 1.8026\n",
      "Epoch [1/32], Step [294/393], Loss: 1.5227\n",
      "Epoch [1/32], Step [343/393], Loss: 1.5133\n",
      "Epoch [1/32], Step [392/393], Loss: 1.3404\n",
      "Train phase -  Epoch 1 Loss: 2.1577 Acc: 0.4582\n",
      "----------\n",
      "Epoch 2/32\n",
      "----------\n",
      "Epoch [2/32], Step [49/393], Loss: 1.2185\n",
      "Epoch [2/32], Step [98/393], Loss: 1.3349\n",
      "Epoch [2/32], Step [147/393], Loss: 1.1783\n",
      "Epoch [2/32], Step [196/393], Loss: 1.0577\n",
      "Epoch [2/32], Step [245/393], Loss: 1.2048\n",
      "Epoch [2/32], Step [294/393], Loss: 0.8586\n",
      "Epoch [2/32], Step [343/393], Loss: 1.0823\n",
      "Epoch [2/32], Step [392/393], Loss: 1.0004\n",
      "Train phase -  Epoch 2 Loss: 1.1667 Acc: 0.6665\n",
      "----------\n",
      "Epoch 3/32\n",
      "----------\n",
      "Epoch [3/32], Step [49/393], Loss: 0.9084\n",
      "Epoch [3/32], Step [98/393], Loss: 0.8558\n",
      "Epoch [3/32], Step [147/393], Loss: 1.0659\n",
      "Epoch [3/32], Step [196/393], Loss: 0.9871\n",
      "Epoch [3/32], Step [245/393], Loss: 0.9490\n",
      "Epoch [3/32], Step [294/393], Loss: 0.8461\n",
      "Epoch [3/32], Step [343/393], Loss: 0.6942\n",
      "Epoch [3/32], Step [392/393], Loss: 0.8597\n",
      "Train phase -  Epoch 3 Loss: 0.8888 Acc: 0.7438\n",
      "----------\n",
      "Epoch 4/32\n",
      "----------\n",
      "Epoch [4/32], Step [49/393], Loss: 0.7198\n",
      "Epoch [4/32], Step [98/393], Loss: 0.6668\n",
      "Epoch [4/32], Step [147/393], Loss: 0.7971\n",
      "Epoch [4/32], Step [196/393], Loss: 0.8637\n",
      "Epoch [4/32], Step [245/393], Loss: 0.6974\n",
      "Epoch [4/32], Step [294/393], Loss: 0.6163\n",
      "Epoch [4/32], Step [343/393], Loss: 0.6453\n",
      "Epoch [4/32], Step [392/393], Loss: 0.5966\n",
      "Train phase -  Epoch 4 Loss: 0.7467 Acc: 0.7806\n",
      "----------\n",
      "Epoch 5/32\n",
      "----------\n",
      "Epoch [5/32], Step [49/393], Loss: 0.6358\n",
      "Epoch [5/32], Step [98/393], Loss: 0.7096\n",
      "Epoch [5/32], Step [147/393], Loss: 0.7322\n",
      "Epoch [5/32], Step [196/393], Loss: 0.5661\n",
      "Epoch [5/32], Step [245/393], Loss: 0.8550\n",
      "Epoch [5/32], Step [294/393], Loss: 0.5164\n",
      "Epoch [5/32], Step [343/393], Loss: 0.5109\n",
      "Epoch [5/32], Step [392/393], Loss: 0.6227\n",
      "Train phase -  Epoch 5 Loss: 0.6428 Acc: 0.8113\n",
      "----------\n",
      "Epoch 6/32\n",
      "----------\n",
      "Epoch [6/32], Step [49/393], Loss: 0.5579\n",
      "Epoch [6/32], Step [98/393], Loss: 0.5248\n",
      "Epoch [6/32], Step [147/393], Loss: 0.5589\n",
      "Epoch [6/32], Step [196/393], Loss: 0.7019\n",
      "Epoch [6/32], Step [245/393], Loss: 0.6803\n",
      "Epoch [6/32], Step [294/393], Loss: 0.4433\n",
      "Epoch [6/32], Step [343/393], Loss: 0.4817\n",
      "Epoch [6/32], Step [392/393], Loss: 0.5167\n",
      "Train phase -  Epoch 6 Loss: 0.5785 Acc: 0.8298\n",
      "----------\n",
      "Epoch 7/32\n",
      "----------\n",
      "Epoch [7/32], Step [49/393], Loss: 0.5956\n",
      "Epoch [7/32], Step [98/393], Loss: 0.6543\n",
      "Epoch [7/32], Step [147/393], Loss: 0.4870\n",
      "Epoch [7/32], Step [196/393], Loss: 0.4020\n",
      "Epoch [7/32], Step [245/393], Loss: 0.6064\n",
      "Epoch [7/32], Step [294/393], Loss: 0.5713\n",
      "Epoch [7/32], Step [343/393], Loss: 0.5861\n",
      "Epoch [7/32], Step [392/393], Loss: 0.6968\n",
      "Train phase -  Epoch 7 Loss: 0.5301 Acc: 0.8411\n",
      "----------\n",
      "Epoch 8/32\n",
      "----------\n",
      "Epoch [8/32], Step [49/393], Loss: 0.4270\n",
      "Epoch [8/32], Step [98/393], Loss: 0.6257\n",
      "Epoch [8/32], Step [147/393], Loss: 0.5047\n",
      "Epoch [8/32], Step [196/393], Loss: 0.4364\n",
      "Epoch [8/32], Step [245/393], Loss: 0.5102\n",
      "Epoch [8/32], Step [294/393], Loss: 0.4790\n",
      "Epoch [8/32], Step [343/393], Loss: 0.4994\n",
      "Epoch [8/32], Step [392/393], Loss: 0.4049\n",
      "Train phase -  Epoch 8 Loss: 0.4933 Acc: 0.8521\n",
      "----------\n",
      "Epoch 9/32\n",
      "----------\n",
      "Epoch [9/32], Step [49/393], Loss: 0.3758\n",
      "Epoch [9/32], Step [98/393], Loss: 0.5458\n",
      "Epoch [9/32], Step [147/393], Loss: 0.4102\n",
      "Epoch [9/32], Step [196/393], Loss: 0.5287\n",
      "Epoch [9/32], Step [245/393], Loss: 0.4610\n",
      "Epoch [9/32], Step [294/393], Loss: 0.3719\n",
      "Epoch [9/32], Step [343/393], Loss: 0.4147\n",
      "Epoch [9/32], Step [392/393], Loss: 0.5151\n",
      "Train phase -  Epoch 9 Loss: 0.4629 Acc: 0.8605\n",
      "----------\n",
      "Epoch 10/32\n",
      "----------\n",
      "Epoch [10/32], Step [49/393], Loss: 0.3853\n",
      "Epoch [10/32], Step [98/393], Loss: 0.3838\n",
      "Epoch [10/32], Step [147/393], Loss: 0.4390\n",
      "Epoch [10/32], Step [196/393], Loss: 0.4469\n",
      "Epoch [10/32], Step [245/393], Loss: 0.5537\n",
      "Epoch [10/32], Step [294/393], Loss: 0.4847\n",
      "Epoch [10/32], Step [343/393], Loss: 0.4270\n",
      "Epoch [10/32], Step [392/393], Loss: 0.4440\n",
      "Train phase -  Epoch 10 Loss: 0.4405 Acc: 0.8679\n",
      "----------\n",
      "Epoch 11/32\n",
      "----------\n",
      "Epoch [11/32], Step [49/393], Loss: 0.3933\n",
      "Epoch [11/32], Step [98/393], Loss: 0.4183\n",
      "Epoch [11/32], Step [147/393], Loss: 0.3727\n",
      "Epoch [11/32], Step [196/393], Loss: 0.3234\n",
      "Epoch [11/32], Step [245/393], Loss: 0.3776\n",
      "Epoch [11/32], Step [294/393], Loss: 0.3532\n",
      "Epoch [11/32], Step [343/393], Loss: 0.3721\n",
      "Epoch [11/32], Step [392/393], Loss: 0.6945\n",
      "Train phase -  Epoch 11 Loss: 0.4194 Acc: 0.8751\n",
      "----------\n",
      "Epoch 12/32\n",
      "----------\n",
      "Epoch [12/32], Step [49/393], Loss: 0.3099\n",
      "Epoch [12/32], Step [98/393], Loss: 0.5223\n",
      "Epoch [12/32], Step [147/393], Loss: 0.2869\n",
      "Epoch [12/32], Step [196/393], Loss: 0.4384\n",
      "Epoch [12/32], Step [245/393], Loss: 0.4766\n",
      "Epoch [12/32], Step [294/393], Loss: 0.3871\n",
      "Epoch [12/32], Step [343/393], Loss: 0.4980\n",
      "Epoch [12/32], Step [392/393], Loss: 0.4167\n",
      "Train phase -  Epoch 12 Loss: 0.4019 Acc: 0.8779\n",
      "----------\n",
      "Epoch 13/32\n",
      "----------\n",
      "Epoch [13/32], Step [49/393], Loss: 0.4759\n",
      "Epoch [13/32], Step [98/393], Loss: 0.4477\n",
      "Epoch [13/32], Step [147/393], Loss: 0.4265\n",
      "Epoch [13/32], Step [196/393], Loss: 0.2775\n",
      "Epoch [13/32], Step [245/393], Loss: 0.2921\n",
      "Epoch [13/32], Step [294/393], Loss: 0.4220\n",
      "Epoch [13/32], Step [343/393], Loss: 0.3204\n",
      "Epoch [13/32], Step [392/393], Loss: 0.4277\n",
      "Train phase -  Epoch 13 Loss: 0.3900 Acc: 0.8825\n",
      "----------\n",
      "Epoch 14/32\n",
      "----------\n",
      "Epoch [14/32], Step [49/393], Loss: 0.3850\n",
      "Epoch [14/32], Step [98/393], Loss: 0.3478\n",
      "Epoch [14/32], Step [147/393], Loss: 0.4418\n",
      "Epoch [14/32], Step [196/393], Loss: 0.3478\n",
      "Epoch [14/32], Step [245/393], Loss: 0.3616\n",
      "Epoch [14/32], Step [294/393], Loss: 0.3182\n",
      "Epoch [14/32], Step [343/393], Loss: 0.3922\n",
      "Epoch [14/32], Step [392/393], Loss: 0.3699\n",
      "Train phase -  Epoch 14 Loss: 0.3766 Acc: 0.8844\n",
      "----------\n",
      "Epoch 15/32\n",
      "----------\n",
      "Epoch [15/32], Step [49/393], Loss: 0.4833\n",
      "Epoch [15/32], Step [98/393], Loss: 0.4039\n",
      "Epoch [15/32], Step [147/393], Loss: 0.4169\n",
      "Epoch [15/32], Step [196/393], Loss: 0.3426\n",
      "Epoch [15/32], Step [245/393], Loss: 0.2854\n",
      "Epoch [15/32], Step [294/393], Loss: 0.3268\n",
      "Epoch [15/32], Step [343/393], Loss: 0.3269\n",
      "Epoch [15/32], Step [392/393], Loss: 0.3150\n",
      "Train phase -  Epoch 15 Loss: 0.3650 Acc: 0.8887\n",
      "----------\n",
      "Epoch 16/32\n",
      "----------\n",
      "Epoch [16/32], Step [49/393], Loss: 0.3191\n",
      "Epoch [16/32], Step [98/393], Loss: 0.3693\n",
      "Epoch [16/32], Step [147/393], Loss: 0.2285\n",
      "Epoch [16/32], Step [196/393], Loss: 0.3097\n",
      "Epoch [16/32], Step [245/393], Loss: 0.3315\n",
      "Epoch [16/32], Step [294/393], Loss: 0.4309\n",
      "Epoch [16/32], Step [343/393], Loss: 0.4001\n",
      "Epoch [16/32], Step [392/393], Loss: 0.4585\n",
      "Train phase -  Epoch 16 Loss: 0.3529 Acc: 0.8926\n",
      "----------\n",
      "Epoch 17/32\n",
      "----------\n",
      "Epoch [17/32], Step [49/393], Loss: 0.3352\n",
      "Epoch [17/32], Step [98/393], Loss: 0.3301\n",
      "Epoch [17/32], Step [147/393], Loss: 0.2672\n",
      "Epoch [17/32], Step [196/393], Loss: 0.3296\n",
      "Epoch [17/32], Step [245/393], Loss: 0.3434\n",
      "Epoch [17/32], Step [294/393], Loss: 0.3449\n",
      "Epoch [17/32], Step [343/393], Loss: 0.4750\n",
      "Epoch [17/32], Step [392/393], Loss: 0.4153\n",
      "Train phase -  Epoch 17 Loss: 0.3397 Acc: 0.8967\n",
      "----------\n",
      "Epoch 18/32\n",
      "----------\n",
      "Epoch [18/32], Step [49/393], Loss: 0.3389\n",
      "Epoch [18/32], Step [98/393], Loss: 0.2854\n",
      "Epoch [18/32], Step [147/393], Loss: 0.3166\n",
      "Epoch [18/32], Step [196/393], Loss: 0.3230\n",
      "Epoch [18/32], Step [245/393], Loss: 0.4473\n",
      "Epoch [18/32], Step [294/393], Loss: 0.3619\n",
      "Epoch [18/32], Step [343/393], Loss: 0.3253\n",
      "Epoch [18/32], Step [392/393], Loss: 0.3344\n",
      "Train phase -  Epoch 18 Loss: 0.3372 Acc: 0.8965\n",
      "----------\n",
      "Epoch 19/32\n",
      "----------\n",
      "Epoch [19/32], Step [49/393], Loss: 0.4109\n",
      "Epoch [19/32], Step [98/393], Loss: 0.3702\n",
      "Epoch [19/32], Step [147/393], Loss: 0.3005\n",
      "Epoch [19/32], Step [196/393], Loss: 0.2463\n",
      "Epoch [19/32], Step [245/393], Loss: 0.2423\n",
      "Epoch [19/32], Step [294/393], Loss: 0.1653\n",
      "Epoch [19/32], Step [343/393], Loss: 0.3441\n",
      "Epoch [19/32], Step [392/393], Loss: 0.3517\n",
      "Train phase -  Epoch 19 Loss: 0.3262 Acc: 0.9020\n",
      "----------\n",
      "Epoch 20/32\n",
      "----------\n",
      "Epoch [20/32], Step [49/393], Loss: 0.2664\n",
      "Epoch [20/32], Step [98/393], Loss: 0.2479\n",
      "Epoch [20/32], Step [147/393], Loss: 0.3005\n",
      "Epoch [20/32], Step [196/393], Loss: 0.2407\n",
      "Epoch [20/32], Step [245/393], Loss: 0.3008\n",
      "Epoch [20/32], Step [294/393], Loss: 0.3451\n",
      "Epoch [20/32], Step [343/393], Loss: 0.3115\n",
      "Epoch [20/32], Step [392/393], Loss: 0.3692\n",
      "Train phase -  Epoch 20 Loss: 0.3271 Acc: 0.9001\n",
      "----------\n",
      "Epoch 21/32\n",
      "----------\n",
      "Epoch [21/32], Step [49/393], Loss: 0.2814\n",
      "Epoch [21/32], Step [98/393], Loss: 0.2781\n",
      "Epoch [21/32], Step [147/393], Loss: 0.3394\n",
      "Epoch [21/32], Step [196/393], Loss: 0.3006\n",
      "Epoch [21/32], Step [245/393], Loss: 0.2740\n",
      "Epoch [21/32], Step [294/393], Loss: 0.3916\n",
      "Epoch [21/32], Step [343/393], Loss: 0.3514\n",
      "Epoch [21/32], Step [392/393], Loss: 0.2670\n",
      "Train phase -  Epoch 21 Loss: 0.3169 Acc: 0.9044\n",
      "----------\n",
      "Epoch 22/32\n",
      "----------\n",
      "Epoch [22/32], Step [49/393], Loss: 0.3149\n",
      "Epoch [22/32], Step [98/393], Loss: 0.2815\n",
      "Epoch [22/32], Step [147/393], Loss: 0.3957\n",
      "Epoch [22/32], Step [196/393], Loss: 0.3529\n",
      "Epoch [22/32], Step [245/393], Loss: 0.3174\n",
      "Epoch [22/32], Step [294/393], Loss: 0.6394\n",
      "Epoch [22/32], Step [343/393], Loss: 0.2758\n",
      "Epoch [22/32], Step [392/393], Loss: 0.3863\n",
      "Train phase -  Epoch 22 Loss: 0.3074 Acc: 0.9068\n",
      "----------\n",
      "Epoch 23/32\n",
      "----------\n",
      "Epoch [23/32], Step [49/393], Loss: 0.3454\n",
      "Epoch [23/32], Step [98/393], Loss: 0.1986\n",
      "Epoch [23/32], Step [147/393], Loss: 0.4659\n",
      "Epoch [23/32], Step [196/393], Loss: 0.2525\n",
      "Epoch [23/32], Step [245/393], Loss: 0.2856\n",
      "Epoch [23/32], Step [294/393], Loss: 0.3749\n",
      "Epoch [23/32], Step [343/393], Loss: 0.4135\n",
      "Epoch [23/32], Step [392/393], Loss: 0.3293\n",
      "Train phase -  Epoch 23 Loss: 0.3119 Acc: 0.9060\n",
      "----------\n",
      "Epoch 24/32\n",
      "----------\n",
      "Epoch [24/32], Step [49/393], Loss: 0.3257\n",
      "Epoch [24/32], Step [98/393], Loss: 0.2784\n",
      "Epoch [24/32], Step [147/393], Loss: 0.4453\n",
      "Epoch [24/32], Step [196/393], Loss: 0.3533\n",
      "Epoch [24/32], Step [245/393], Loss: 0.3467\n",
      "Epoch [24/32], Step [294/393], Loss: 0.2592\n",
      "Epoch [24/32], Step [343/393], Loss: 0.3491\n",
      "Epoch [24/32], Step [392/393], Loss: 0.3161\n",
      "Train phase -  Epoch 24 Loss: 0.3018 Acc: 0.9065\n",
      "----------\n",
      "Epoch 25/32\n",
      "----------\n",
      "Epoch [25/32], Step [49/393], Loss: 0.2708\n",
      "Epoch [25/32], Step [98/393], Loss: 0.2937\n",
      "Epoch [25/32], Step [147/393], Loss: 0.2755\n",
      "Epoch [25/32], Step [196/393], Loss: 0.2938\n",
      "Epoch [25/32], Step [245/393], Loss: 0.3355\n",
      "Epoch [25/32], Step [294/393], Loss: 0.3175\n",
      "Epoch [25/32], Step [343/393], Loss: 0.2552\n",
      "Epoch [25/32], Step [392/393], Loss: 0.5071\n",
      "Train phase -  Epoch 25 Loss: 0.2985 Acc: 0.9107\n",
      "----------\n",
      "Epoch 26/32\n",
      "----------\n",
      "Epoch [26/32], Step [49/393], Loss: 0.1964\n",
      "Epoch [26/32], Step [98/393], Loss: 0.3146\n",
      "Epoch [26/32], Step [147/393], Loss: 0.3164\n",
      "Epoch [26/32], Step [196/393], Loss: 0.2756\n",
      "Epoch [26/32], Step [245/393], Loss: 0.2576\n",
      "Epoch [26/32], Step [294/393], Loss: 0.2735\n",
      "Epoch [26/32], Step [343/393], Loss: 0.2487\n",
      "Epoch [26/32], Step [392/393], Loss: 0.3255\n",
      "Train phase -  Epoch 26 Loss: 0.2907 Acc: 0.9105\n",
      "----------\n",
      "Epoch 27/32\n",
      "----------\n",
      "Epoch [27/32], Step [49/393], Loss: 0.2975\n",
      "Epoch [27/32], Step [98/393], Loss: 0.3196\n",
      "Epoch [27/32], Step [147/393], Loss: 0.2829\n",
      "Epoch [27/32], Step [196/393], Loss: 0.2738\n",
      "Epoch [27/32], Step [245/393], Loss: 0.2735\n",
      "Epoch [27/32], Step [294/393], Loss: 0.2616\n",
      "Epoch [27/32], Step [343/393], Loss: 0.3877\n",
      "Epoch [27/32], Step [392/393], Loss: 0.4383\n",
      "Train phase -  Epoch 27 Loss: 0.2972 Acc: 0.9086\n",
      "----------\n",
      "Epoch 28/32\n",
      "----------\n",
      "Epoch [28/32], Step [49/393], Loss: 0.2461\n",
      "Epoch [28/32], Step [98/393], Loss: 0.1360\n",
      "Epoch [28/32], Step [147/393], Loss: 0.2981\n",
      "Epoch [28/32], Step [196/393], Loss: 0.2689\n",
      "Epoch [28/32], Step [245/393], Loss: 0.3237\n",
      "Epoch [28/32], Step [294/393], Loss: 0.2252\n",
      "Epoch [28/32], Step [343/393], Loss: 0.3278\n",
      "Epoch [28/32], Step [392/393], Loss: 0.4097\n",
      "Train phase -  Epoch 28 Loss: 0.2828 Acc: 0.9142\n",
      "----------\n",
      "Epoch 29/32\n",
      "----------\n",
      "Epoch [29/32], Step [49/393], Loss: 0.3268\n",
      "Epoch [29/32], Step [98/393], Loss: 0.3401\n",
      "Epoch [29/32], Step [147/393], Loss: 0.3096\n",
      "Epoch [29/32], Step [196/393], Loss: 0.2096\n",
      "Epoch [29/32], Step [245/393], Loss: 0.1710\n",
      "Epoch [29/32], Step [294/393], Loss: 0.3361\n",
      "Epoch [29/32], Step [343/393], Loss: 0.3390\n",
      "Epoch [29/32], Step [392/393], Loss: 0.2625\n",
      "Train phase -  Epoch 29 Loss: 0.2799 Acc: 0.9124\n",
      "----------\n",
      "Epoch 30/32\n",
      "----------\n",
      "Epoch [30/32], Step [49/393], Loss: 0.2569\n",
      "Epoch [30/32], Step [98/393], Loss: 0.1814\n",
      "Epoch [30/32], Step [147/393], Loss: 0.2869\n",
      "Epoch [30/32], Step [196/393], Loss: 0.2224\n",
      "Epoch [30/32], Step [245/393], Loss: 0.3685\n",
      "Epoch [30/32], Step [294/393], Loss: 0.2480\n",
      "Epoch [30/32], Step [343/393], Loss: 0.3295\n",
      "Epoch [30/32], Step [392/393], Loss: 0.2437\n",
      "Train phase -  Epoch 30 Loss: 0.2817 Acc: 0.9149\n",
      "----------\n",
      "Epoch 31/32\n",
      "----------\n",
      "Epoch [31/32], Step [49/393], Loss: 0.2571\n",
      "Epoch [31/32], Step [98/393], Loss: 0.2903\n",
      "Epoch [31/32], Step [147/393], Loss: 0.2409\n",
      "Epoch [31/32], Step [196/393], Loss: 0.2872\n",
      "Epoch [31/32], Step [245/393], Loss: 0.5549\n",
      "Epoch [31/32], Step [294/393], Loss: 0.2040\n",
      "Epoch [31/32], Step [343/393], Loss: 0.3246\n",
      "Epoch [31/32], Step [392/393], Loss: 0.2000\n",
      "Train phase -  Epoch 31 Loss: 0.2811 Acc: 0.9144\n",
      "----------\n",
      "Epoch 32/32\n",
      "----------\n",
      "Epoch [32/32], Step [49/393], Loss: 0.2062\n",
      "Epoch [32/32], Step [98/393], Loss: 0.2905\n",
      "Epoch [32/32], Step [147/393], Loss: 0.1955\n",
      "Epoch [32/32], Step [196/393], Loss: 0.1587\n",
      "Epoch [32/32], Step [245/393], Loss: 0.3136\n",
      "Epoch [32/32], Step [294/393], Loss: 0.3160\n",
      "Epoch [32/32], Step [343/393], Loss: 0.3224\n",
      "Epoch [32/32], Step [392/393], Loss: 0.3291\n",
      "Train phase -  Epoch 32 Loss: 0.2755 Acc: 0.9167\n",
      "Training complete in 25m 0s\n"
     ]
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Saving the trained model",
   "id": "e4d9605ef361a0d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T23:50:14.908127Z",
     "start_time": "2024-04-30T23:50:14.507253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('Finished Training')\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "PATH = './models/trained_model.pth'\n",
    "torch.save(trained_model, PATH)"
   ],
   "id": "9f5f26cdefec21bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Loading the model",
   "id": "6f3fea444bec58bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T23:50:15.106916Z",
     "start_time": "2024-04-30T23:50:14.909181Z"
    }
   },
   "cell_type": "code",
   "source": "trained_model = torch.load('./models/trained_model.pth', map_location=device)",
   "id": "fb1aa946a282ac0f",
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluating the model",
   "id": "9e3a434c2806a2e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T23:55:04.741787Z",
     "start_time": "2024-04-30T23:55:04.738291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_model(model, dataloader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_corrects = 0\n",
    "\n",
    "    # Disable gradient calculation to speed up the process and reduce memory usage\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass to get output/logits\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Get predictions from the maximum value\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # Increment the correct predictions count\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # Optionally print progress every 250 batches\n",
    "            if (i + 1) % 250 == 0:\n",
    "                print(f'Evaluating: [{i + 1}/{len(dataloader)}],  Correct classified: {running_corrects}/{i + 1}')\n",
    "\n",
    "    # Calculate the accuracy by dividing the number of correct predictions by the dataset size\n",
    "    test_acc = running_corrects.double() / len(dataloader)\n",
    "    print(f'Test Acc: {test_acc:.4f}, Correct classified: {running_corrects}/{len(dataloader)}')\n",
    "\n",
    "    return test_acc"
   ],
   "id": "6488b43f7e58c5d",
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T23:57:54.612383Z",
     "start_time": "2024-04-30T23:55:05.737553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_loader = DataLoader(test_dataset, shuffle=True)\n",
    "\n",
    "test_model(trained_model, test_loader, device)"
   ],
   "id": "5b3a78850fc2ef92",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: [250/12630],  Correct classified: 5/250\n",
      "Evaluating: [500/12630],  Correct classified: 7/500\n",
      "Evaluating: [750/12630],  Correct classified: 18/750\n",
      "Evaluating: [1000/12630],  Correct classified: 28/1000\n",
      "Evaluating: [1250/12630],  Correct classified: 31/1250\n",
      "Evaluating: [1500/12630],  Correct classified: 38/1500\n",
      "Evaluating: [1750/12630],  Correct classified: 46/1750\n",
      "Evaluating: [2000/12630],  Correct classified: 52/2000\n",
      "Evaluating: [2250/12630],  Correct classified: 60/2250\n",
      "Evaluating: [2500/12630],  Correct classified: 67/2500\n",
      "Evaluating: [2750/12630],  Correct classified: 75/2750\n",
      "Evaluating: [3000/12630],  Correct classified: 80/3000\n",
      "Evaluating: [3250/12630],  Correct classified: 90/3250\n",
      "Evaluating: [3500/12630],  Correct classified: 95/3500\n",
      "Evaluating: [3750/12630],  Correct classified: 102/3750\n",
      "Evaluating: [4000/12630],  Correct classified: 108/4000\n",
      "Evaluating: [4250/12630],  Correct classified: 114/4250\n",
      "Evaluating: [4500/12630],  Correct classified: 121/4500\n",
      "Evaluating: [4750/12630],  Correct classified: 132/4750\n",
      "Evaluating: [5000/12630],  Correct classified: 139/5000\n",
      "Evaluating: [5250/12630],  Correct classified: 146/5250\n",
      "Evaluating: [5500/12630],  Correct classified: 152/5500\n",
      "Evaluating: [5750/12630],  Correct classified: 155/5750\n",
      "Evaluating: [6000/12630],  Correct classified: 157/6000\n",
      "Evaluating: [6250/12630],  Correct classified: 159/6250\n",
      "Evaluating: [6500/12630],  Correct classified: 164/6500\n",
      "Evaluating: [6750/12630],  Correct classified: 170/6750\n",
      "Evaluating: [7000/12630],  Correct classified: 176/7000\n",
      "Evaluating: [7250/12630],  Correct classified: 182/7250\n",
      "Evaluating: [7500/12630],  Correct classified: 185/7500\n",
      "Evaluating: [7750/12630],  Correct classified: 191/7750\n",
      "Evaluating: [8000/12630],  Correct classified: 197/8000\n",
      "Evaluating: [8250/12630],  Correct classified: 206/8250\n",
      "Evaluating: [8500/12630],  Correct classified: 211/8500\n",
      "Evaluating: [8750/12630],  Correct classified: 219/8750\n",
      "Evaluating: [9000/12630],  Correct classified: 224/9000\n",
      "Evaluating: [9250/12630],  Correct classified: 232/9250\n",
      "Evaluating: [9500/12630],  Correct classified: 242/9500\n",
      "Evaluating: [9750/12630],  Correct classified: 244/9750\n",
      "Evaluating: [10000/12630],  Correct classified: 249/10000\n",
      "Evaluating: [10250/12630],  Correct classified: 259/10250\n",
      "Evaluating: [10500/12630],  Correct classified: 269/10500\n",
      "Evaluating: [10750/12630],  Correct classified: 278/10750\n",
      "Evaluating: [11000/12630],  Correct classified: 281/11000\n",
      "Evaluating: [11250/12630],  Correct classified: 292/11250\n",
      "Evaluating: [11500/12630],  Correct classified: 293/11500\n",
      "Evaluating: [11750/12630],  Correct classified: 297/11750\n",
      "Evaluating: [12000/12630],  Correct classified: 305/12000\n",
      "Evaluating: [12250/12630],  Correct classified: 313/12250\n",
      "Evaluating: [12500/12630],  Correct classified: 320/12500\n",
      "Test Acc: 0.0258, Correct classified: 326/12630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0258, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 111
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
