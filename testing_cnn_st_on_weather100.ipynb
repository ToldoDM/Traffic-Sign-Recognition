{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53bb43bad4c869b9",
   "metadata": {
    "id": "53bb43bad4c869b9"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "P_5LiGFJ3Akj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P_5LiGFJ3Akj",
    "outputId": "3f140c27-1fc6-458e-f504-2e9ba18ac4ff",
    "ExecuteTime": {
     "end_time": "2024-07-04T17:17:11.822780Z",
     "start_time": "2024-07-04T17:17:08.945350Z"
    }
   },
   "source": [
    "# Package installation for colab\n",
    "!pip install -U albumentations\n",
    "!pip install split-folders"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (1.4.10)\r\n",
      "Requirement already satisfied: numpy<2,>=1.24.4 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from albumentations) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.10.0 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from albumentations) (1.13.0)\r\n",
      "Requirement already satisfied: scikit-image>=0.21.0 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from albumentations) (0.23.2)\r\n",
      "Requirement already satisfied: PyYAML in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from albumentations) (6.0.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from albumentations) (4.11.0)\r\n",
      "Requirement already satisfied: scikit-learn>=1.3.2 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from albumentations) (1.4.2)\r\n",
      "Requirement already satisfied: pydantic>=2.7.0 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from albumentations) (2.7.1)\r\n",
      "Requirement already satisfied: albucore>=0.0.11 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from albumentations) (0.0.12)\r\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from albumentations) (4.9.0.80)\r\n",
      "Requirement already satisfied: tomli>=2.0.1 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from albucore>=0.0.11->albumentations) (2.0.1)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from pydantic>=2.7.0->albumentations) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from pydantic>=2.7.0->albumentations) (2.18.2)\r\n",
      "Requirement already satisfied: networkx>=2.8 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (3.3)\r\n",
      "Requirement already satisfied: pillow>=9.1 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (10.3.0)\r\n",
      "Requirement already satisfied: imageio>=2.33 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (2.34.1)\r\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (2024.5.3)\r\n",
      "Requirement already satisfied: packaging>=21 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (24.0)\r\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (0.4)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from scikit-learn>=1.3.2->albumentations) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (from scikit-learn>=1.3.2->albumentations) (3.5.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: split-folders in /home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages (0.5.1)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "initial_id",
    "outputId": "e7310ded-f8e0-4409-856d-570280543fd0",
    "ExecuteTime": {
     "end_time": "2024-07-04T17:17:14.896567Z",
     "start_time": "2024-07-04T17:17:11.823810Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.models import ResNet152_Weights, ResNet50_Weights\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import splitfolders\n",
    "import data_augmenter"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "6CCfxseGNAtf",
   "metadata": {
    "id": "6CCfxseGNAtf",
    "ExecuteTime": {
     "end_time": "2024-07-04T17:17:14.901925Z",
     "start_time": "2024-07-04T17:17:14.897985Z"
    }
   },
   "source": [
    "# For tensorboard plotting\n",
    "def plot_classes_preds(images, labels, preds, probs):\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx + 1, xticks=[], yticks=[])\n",
    "        norm_img = cv2.normalize(images[idx].cpu().numpy(), None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "        rgb_img = np.transpose(norm_img, (1, 2, 0)).astype(np.uint8)\n",
    "        plt.imshow(rgb_img)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            preds[idx],\n",
    "            probs[idx] * 100.0,\n",
    "            labels[idx]),\n",
    "            color=(\"green\" if preds[idx] == labels[idx].item() else \"red\"))\n",
    "    return fig"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "9ax_0x5Pn-MF",
   "metadata": {
    "id": "9ax_0x5Pn-MF"
   },
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "id": "ba26bd6e55fb8bae",
   "metadata": {
    "id": "ba26bd6e55fb8bae",
    "ExecuteTime": {
     "end_time": "2024-07-04T17:17:14.907652Z",
     "start_time": "2024-07-04T17:17:14.902944Z"
    }
   },
   "source": [
    "# For replication\n",
    "np.random.seed(123)  \n",
    "torch.manual_seed(123)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "fe6a3582e4f42eab",
   "metadata": {
    "id": "fe6a3582e4f42eab"
   },
   "source": "## Downloading the dataset"
  },
  {
   "cell_type": "code",
   "id": "dc01ba7b79c1896f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dc01ba7b79c1896f",
    "outputId": "49b1ba35-3fdc-4b67-bff7-1ab2395cc55d",
    "ExecuteTime": {
     "end_time": "2024-07-04T17:17:17.918947Z",
     "start_time": "2024-07-04T17:17:14.908621Z"
    }
   },
   "source": [
    "def download_file(url, file_name):\n",
    "    if not os.path.exists('dataset/' + file_name):\n",
    "        with urllib.request.urlopen(url) as response, open('dataset/' + file_name, 'wb') as out_file:\n",
    "            content_length = int(response.headers['Content-Length'])\n",
    "            with tqdm(total=content_length, unit='B', unit_scale=True, desc=url.split('/')[-1]) as pbar:\n",
    "                while True:\n",
    "                    chunk = response.read(1024)\n",
    "                    if not chunk:\n",
    "                        break\n",
    "                    out_file.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "    else:\n",
    "        print(f\"{file_name} already exists.\")\n",
    "\n",
    "\n",
    "os.makedirs(\"dataset/\", exist_ok=True)\n",
    "# Check if the directory exists before trying to delete it\n",
    "if os.path.exists('dataset/GTSRB/'):\n",
    "    shutil.rmtree('dataset/GTSRB/')\n",
    "    print(\"The folder 'dataset/GTSRB/' has been deleted successfully.\")\n",
    "else:\n",
    "    print(\"The folder 'dataset/GTSRB/' does not exist.\")\n",
    "# Training\n",
    "download_file('https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Training_Images.zip',\n",
    "              'GTSRB_Final_Training_Images.zip')\n",
    "# Testing\n",
    "download_file('https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_Images.zip',\n",
    "              'GTSRB_Final_Test_Images.zip')\n",
    "# Ground truth for testing\n",
    "download_file('https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_GT.zip',\n",
    "              'GTSRB_Final_Test_GT.zip')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder 'dataset/GTSRB/' has been deleted successfully.\n",
      "GTSRB_Final_Training_Images.zip already exists.\n",
      "GTSRB_Final_Test_Images.zip already exists.\n",
      "GTSRB_Final_Test_GT.zip already exists.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "f905d089f305e5b7",
   "metadata": {
    "id": "f905d089f305e5b7"
   },
   "source": "## Extracting zip files"
  },
  {
   "cell_type": "code",
   "id": "3de5a631fc79d888",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3de5a631fc79d888",
    "outputId": "a47e1c83-caad-4385-c493-28c72f17ddb1",
    "ExecuteTime": {
     "end_time": "2024-07-04T17:17:25.658319Z",
     "start_time": "2024-07-04T17:17:17.920331Z"
    }
   },
   "source": [
    "def extract_file(file_name):\n",
    "    with zipfile.ZipFile(f\"dataset/{file_name}\", 'r') as zip_ref:\n",
    "        file_list = zip_ref.namelist()\n",
    "        with tqdm(total=len(file_list), desc=\"Extracting\") as pbar:\n",
    "            for file in file_list:\n",
    "                zip_ref.extract(file, 'dataset/')\n",
    "                pbar.update(1)\n",
    "\n",
    "\n",
    "extract_file('GTSRB_Final_Training_Images.zip')\n",
    "extract_file('GTSRB_Final_Test_Images.zip')\n",
    "extract_file('GTSRB_Final_Test_GT.zip')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting: 100%|██████████| 39299/39299 [00:05<00:00, 7088.00it/s]\n",
      "Extracting: 100%|██████████| 12635/12635 [00:01<00:00, 6797.96it/s]\n",
      "Extracting: 100%|██████████| 1/1 [00:00<00:00, 550.14it/s]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "323d7c1bb67c1550",
   "metadata": {
    "id": "323d7c1bb67c1550"
   },
   "source": "## Loading ground truth annotations for the testing set"
  },
  {
   "cell_type": "code",
   "id": "468db0ee5cdf9840",
   "metadata": {
    "id": "468db0ee5cdf9840",
    "ExecuteTime": {
     "end_time": "2024-07-04T17:17:25.679729Z",
     "start_time": "2024-07-04T17:17:25.659367Z"
    }
   },
   "source": [
    "def csv_loader(csv_path):\n",
    "    data = np.loadtxt(csv_path,delimiter=\";\", dtype=str, skiprows=1)\n",
    "    return data\n",
    "\n",
    "# Loading the ground truth annotations for testing set\n",
    "annotations = csv_loader('./dataset/GT-final_test.csv')\n",
    "\n",
    "# Sort the annotations\n",
    "annotations = annotations[:, [0, 7]]\n",
    "num_samples = len(annotations)\n",
    "\n",
    "#Column 0: image filename - Column 1: classid\n",
    "annotations = annotations[annotations[:, 1].astype(int).argsort()]"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "b8b6074543013fbb",
   "metadata": {
    "id": "b8b6074543013fbb"
   },
   "source": "## Making training data structure"
  },
  {
   "cell_type": "code",
   "id": "61a83a9b0d1825e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61a83a9b0d1825e4",
    "outputId": "bb698c3b-ee90-4bdb-a9ec-7e91f0e0027b",
    "ExecuteTime": {
     "end_time": "2024-07-04T17:17:25.734744Z",
     "start_time": "2024-07-04T17:17:25.681530Z"
    }
   },
   "source": [
    "def move_directories(source, destination):\n",
    "    if not os.path.exists(destination):\n",
    "        os.makedirs(destination)\n",
    "    # Get a list of all directories in the source directory\n",
    "    directories = [d for d in os.listdir(source) if os.path.isdir(os.path.join(source, d))]\n",
    "\n",
    "    # Move each directory to the destination\n",
    "    for directory in tqdm(directories):\n",
    "        source_path = os.path.join(source, directory)\n",
    "        destination_path = os.path.join(destination, directory)\n",
    "        shutil.move(source_path, destination_path)\n",
    "\n",
    "\n",
    "# Example: ./dataset/GTSRB/train/class_id/image\n",
    "move_directories(\"./dataset/GTSRB/Final_Training/Images\", \"./dataset/GTSRB/train\")\n",
    "shutil.rmtree(\"./dataset/GTSRB/Final_Training\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:00<00:00, 28224.58it/s]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "5172c68f192d26ae",
   "metadata": {
    "id": "5172c68f192d26ae"
   },
   "source": "## Making test data structure from the CSV annotations"
  },
  {
   "cell_type": "code",
   "id": "df556df8aaf39e91",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "df556df8aaf39e91",
    "outputId": "fb5ccd24-86c7-4c70-acff-be8f8cb0a4ff",
    "ExecuteTime": {
     "end_time": "2024-07-04T17:17:26.152970Z",
     "start_time": "2024-07-04T17:17:25.735628Z"
    }
   },
   "source": [
    "for class_id in tqdm(np.unique(annotations[:, 1]), desc='Class_ID'):\n",
    "    newpath = './dataset/GTSRB/test/' + class_id.zfill(5)\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    for image_filename in annotations[annotations[:, 1] == class_id]:\n",
    "        shutil.move('./dataset/GTSRB/Final_Test/Images/' + image_filename[0], newpath + '/' + image_filename[0])\n",
    "\n",
    "shutil.rmtree(\"./dataset/GTSRB/Final_Test\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class_ID: 100%|██████████| 43/43 [00:00<00:00, 104.87it/s]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "JWpVwjN6nUFH",
   "metadata": {
    "id": "JWpVwjN6nUFH"
   },
   "source": [
    "## Merging the dataset and then split into train and test\n",
    "- creating **plain** dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "690Ap50knZxO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "690Ap50knZxO",
    "outputId": "f8edb2d3-28d1-4d0b-bf88-541ab4e04add",
    "ExecuteTime": {
     "end_time": "2024-07-04T17:17:34.616927Z",
     "start_time": "2024-07-04T17:17:26.153854Z"
    }
   },
   "source": [
    "def merge(source_folder, destination_folder):\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "\n",
    "    # Get the total number of files and directories in the source folder\n",
    "    total_items = sum([len(files) + len(dirs) for root, dirs, files in os.walk(source_folder)])\n",
    "\n",
    "    progress = tqdm(total=total_items, desc='Moving: ' + source_folder + ' --> ' + destination_folder, position=0, leave=True)\n",
    "\n",
    "    # Iterate over all files and subdirectories in the source folder\n",
    "    for root, dirs, files in os.walk(source_folder):\n",
    "        for item in files + dirs:\n",
    "            source_item = os.path.join(root, item)\n",
    "            destination_item = os.path.join(destination_folder, os.path.relpath(source_item, source_folder))\n",
    "\n",
    "            # If the item is a file, copy it to the destination folder\n",
    "            if os.path.isfile(source_item):\n",
    "                shutil.move(source_item, destination_item)\n",
    "            # If the item is a directory, create it in the destination folder\n",
    "            elif os.path.isdir(source_item):\n",
    "                os.makedirs(destination_item, exist_ok=True)\n",
    "\n",
    "            progress.update(1) \n",
    "\n",
    "    progress.close()  \n",
    "\n",
    "def merge_folders(source_folders, target_folder):\n",
    "    for sf in source_folders:\n",
    "        merge(sf, target_folder)\n",
    "        shutil.rmtree(sf)\n",
    "\n",
    "# Temporary directory to store the merged dataset\n",
    "merged_dir = \"./dataset/GTSRB/merged\"\n",
    "\n",
    "# Merge train and test set\n",
    "merge_folders(['./dataset/GTSRB/train','./dataset/GTSRB/test'], merged_dir)\n",
    "\n",
    "# Training 70%\n",
    "# Testing 30%\n",
    "splitfolders.ratio(merged_dir, output=\"./dataset/GTSRB/plain\", seed=123, ratio=(.7,0, 0.3),move=False)\n",
    "\n",
    "# Clear temporary files\n",
    "shutil.rmtree('./dataset/GTSRB/plain/val')\n",
    "os.remove('./dataset/GTSRB/Readme-Images-Final-test.txt')\n",
    "os.remove('./dataset/GTSRB/Readme-Images.txt')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moving: ./dataset/GTSRB/train --> ./dataset/GTSRB/merged: 100%|██████████| 39295/39295 [00:01<00:00, 20384.64it/s]\n",
      "Moving: ./dataset/GTSRB/test --> ./dataset/GTSRB/merged: 100%|██████████| 12673/12673 [00:00<00:00, 22478.17it/s]\n",
      "Copying files: 51882 files [00:05, 8763.39 files/s]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "5650b2ca72f0f486",
   "metadata": {
    "id": "5650b2ca72f0f486"
   },
   "source": [
    "## Adding weather conditions to the merged dataset and then split into train and test\n",
    "- creating **50% Weather** or **100% Weather** dataset depending on fixed probability set below"
   ]
  },
  {
   "cell_type": "code",
   "id": "b0fb0b549a431a3d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0fb0b549a431a3d",
    "outputId": "0800184c-2b50-484c-ed37-7ff16d3fb9bd",
    "ExecuteTime": {
     "end_time": "2024-07-04T17:18:40.587749Z",
     "start_time": "2024-07-04T17:17:34.617753Z"
    }
   },
   "source": [
    "# For weather augmentation\n",
    "da = data_augmenter.DataAugmenter(dataset_path='./dataset/GTSRB/')\n",
    "da.load_images(folder_to_load='merged')\n",
    "da.add_weather_effects(prob_per_class=1)\n",
    "\n",
    "# Training 70 %\n",
    "# Testing 30 %\n",
    "splitfolders.ratio(merged_dir, output=\"./dataset/GTSRB/weather\", seed=123, ratio=(.7, 0, 0.3), move=True)\n",
    "\n",
    "# Clear temporary files\n",
    "shutil.rmtree('./dataset/GTSRB/weather/val')\n",
    "shutil.rmtree(merged_dir)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found: , 00016, 00033, 00022, 00011, 00003, 00039, 00034, 00035, 00028, 00036, 00037, 00015, 00027, 00038, 00014, 00026, 00019, 00007, 00029, 00005, 00008, 00021, 00025, 00032, 00002, 00000, 00024, 00009, 00012, 00023, 00013, 00041, 00030, 00017, 00006, 00042, 00018, 00040, 00020, 00010, 00031, 00004, 00001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading classes: 100%|██████████| 43/43 [00:02<00:00, 20.81it/s]\n",
      "Add weather effects with probability 1: 100%|██████████| 43/43 [01:01<00:00,  1.43s/it]\n",
      "Copying files: 51882 files [00:02, 22812.54 files/s]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "6dcd860b78a2678d",
   "metadata": {
    "id": "6dcd860b78a2678d"
   },
   "source": [
    "## Set dataset paths"
   ]
  },
  {
   "cell_type": "code",
   "id": "20f8c403f5b816ee",
   "metadata": {
    "id": "20f8c403f5b816ee",
    "ExecuteTime": {
     "end_time": "2024-07-04T17:18:40.590788Z",
     "start_time": "2024-07-04T17:18:40.588577Z"
    }
   },
   "source": [
    "plain_train_dir = './dataset/GTSRB/plain/train'\n",
    "plain_test_dir = './dataset/GTSRB/plain/test'\n",
    "\n",
    "weather_train_dir = './dataset/GTSRB/weather/train'\n",
    "weather_test_dir = './dataset/GTSRB/weather/test'"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "d2e92b663416be6f",
   "metadata": {
    "id": "d2e92b663416be6f"
   },
   "source": [
    "# Parameters setup"
   ]
  },
  {
   "cell_type": "code",
   "id": "e9b3ab9f505b0242",
   "metadata": {
    "id": "e9b3ab9f505b0242",
    "ExecuteTime": {
     "end_time": "2024-07-04T17:18:40.661736Z",
     "start_time": "2024-07-04T17:18:40.591621Z"
    }
   },
   "source": [
    "# Setting device for the computation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Change this to select between plain or weather dataset\n",
    "# train_dir = plain_train_dir/weather_train_dir\n",
    "# test_dir = plain_test_dir/weather_test_dir\n",
    "train_dir = plain_train_dir\n",
    "test_dir = plain_test_dir\n",
    "\n",
    "adam_hyperparams = {\n",
    "    \"num_epochs\": 15,\n",
    "    \"batch_size\": 64,\n",
    "    #optimizer\n",
    "    \"opt\": \"adam\",\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"beta_1\": 0.9,\n",
    "    \"beta_2\": 0.999,\n",
    "    \"eps\": 1e-8,\n",
    "    \"weight_decay\": 0,\n",
    "    \"momentum\": 0,\n",
    "    #scheduler\n",
    "    \"decay_rate\": 0.5,\n",
    "}\n",
    "\n",
    "runs_arguments = [\n",
    "    {'type': 'P', 'cnn': 'CNN'},\n",
    "    {'type': 'P', 'cnn': 'CNN_ST'},\n",
    "    {'type': 'W-FULL', 'cnn': 'CNN'},\n",
    "    {'type': 'W-FULL', 'cnn': 'CNN_ST'},\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Method for loading the training/testing set",
   "id": "43450b2755811a9b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:18:40.667292Z",
     "start_time": "2024-07-04T17:18:40.662643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_train_test_dir(train_dir, test_dir):\n",
    "    \n",
    "    # Define the transformations to make the image fit into the CNN\n",
    "    custom_cnn_transform1 = transforms.Compose([\n",
    "        transforms.Resize((48, 48)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    # Load the dataset\n",
    "    train_dataset = datasets.ImageFolder(train_dir, transform=custom_cnn_transform1)\n",
    "    \n",
    "    # Concatenate all images into a single tensor\n",
    "    images = torch.stack([img for img, _ in train_dataset], dim=0)\n",
    "    \n",
    "    # Calculate mean and std across all images and channels\n",
    "    mean = torch.mean(images, dim=(0, 2, 3))\n",
    "    std = torch.std(images, dim=(0, 2, 3))\n",
    "    \n",
    "    # Redefine the transformation adding the normalization\n",
    "    custom_cnn_transform2 = transforms.Compose([\n",
    "        transforms.Resize((48, 48)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "    \n",
    "    # Load dataset\n",
    "    train_dataset = datasets.ImageFolder(train_dir, transform=custom_cnn_transform2)\n",
    "    test_dataset = datasets.ImageFolder(test_dir, transform=custom_cnn_transform1)\n",
    "    \n",
    "    return train_dataset, test_dataset"
   ],
   "id": "4df51815de9ba5bb",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "G7CE0M9SWeZj",
   "metadata": {
    "id": "G7CE0M9SWeZj"
   },
   "source": "# Defining the CNNs from the paper"
  },
  {
   "cell_type": "markdown",
   "id": "waE26jiDoZ32",
   "metadata": {
    "id": "waE26jiDoZ32"
   },
   "source": "## Basic Convolutional Neural Network"
  },
  {
   "cell_type": "code",
   "id": "r56vEoqhogaG",
   "metadata": {
    "id": "r56vEoqhogaG",
    "ExecuteTime": {
     "end_time": "2024-07-04T17:18:40.677657Z",
     "start_time": "2024-07-04T17:18:40.667993Z"
    }
   },
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Expected input as 48x48\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=200, kernel_size=7, stride=1, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.local_norm = nn.LocalResponseNorm(size=5)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=200, out_channels=250, kernel_size=4, stride=1, padding=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=250, out_channels=350, kernel_size=4, stride=1, padding=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=350 * 6 * 6, out_features=400)\n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=400, out_features=43)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.local_norm(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.local_norm(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.local_norm(x)\n",
    "\n",
    "        # Flatten the output from conv1\n",
    "        x = x.view(-1, 350 * 6 * 6)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "6jKXkY8lft6q",
   "metadata": {
    "id": "6jKXkY8lft6q"
   },
   "source": "## CNN with 3 spatial transformer units"
  },
  {
   "cell_type": "code",
   "id": "eAwI5trgfxqg",
   "metadata": {
    "id": "eAwI5trgfxqg",
    "ExecuteTime": {
     "end_time": "2024-07-04T17:18:40.688760Z",
     "start_time": "2024-07-04T17:18:40.678388Z"
    }
   },
   "source": [
    "class CNN_ST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Expected input as 48x48\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=200, kernel_size=7, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=200, out_channels=250, kernel_size=4, stride=1, padding=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=250, out_channels=350, kernel_size=4, stride=1, padding=2)\n",
    "\n",
    "        self.local_norm = nn.LocalResponseNorm(size=5)\n",
    "\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=350 * 6 * 6, out_features=400)\n",
    "        self.fc2 = nn.Linear(in_features=400, out_features=43)\n",
    "\n",
    "        # Spatial transformer block 1\n",
    "        self.loc1 = nn.Sequential(\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(in_channels=3, out_channels=250, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(in_channels=250, out_channels=250, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.fc_loc1 = nn.Sequential(\n",
    "            nn.Linear(250 * 6 * 6, 250),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(250, 6)\n",
    "        )\n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc1[2].weight.data.zero_()\n",
    "        self.fc_loc1[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "        # Spatial transformer block 2\n",
    "        self.loc2 = nn.Sequential(\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(in_channels=200, out_channels=150, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(in_channels=150, out_channels=200, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.fc_loc2 = nn.Sequential(\n",
    "            nn.Linear(200 * 2 * 2, 300),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(300, 6)\n",
    "        )\n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc2[2].weight.data.zero_()\n",
    "        self.fc_loc2[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "        # Spatial transformer block 3\n",
    "        self.loc3 = nn.Sequential(\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(in_channels=250, out_channels=150, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(in_channels=150, out_channels=200, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.fc_loc3 = nn.Sequential(\n",
    "            nn.Linear(200 * 1 * 1, 300),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(300, 6)\n",
    "        )\n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc3[2].weight.data.zero_()\n",
    "        self.fc_loc3[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "    def stn1(self, x):\n",
    "        xs = self.loc1(x)\n",
    "        xs = xs.view(-1, 250 * 6 * 6)\n",
    "        theta = self.fc_loc1(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "\n",
    "        grid = F.affine_grid(theta, x.size(), align_corners=False)\n",
    "        x = F.grid_sample(x, grid, align_corners=False)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def stn2(self, x):\n",
    "        xs = self.loc2(x)\n",
    "        xs = xs.view(-1, 200 * 2 * 2)\n",
    "        theta = self.fc_loc2(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "\n",
    "        grid = F.affine_grid(theta, x.size(), align_corners=False)\n",
    "        x = F.grid_sample(x, grid, align_corners=False)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def stn3(self, x):\n",
    "        xs = self.loc3(x)\n",
    "        xs = xs.view(-1, 200 * 1 * 1)\n",
    "        theta = self.fc_loc3(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "\n",
    "        grid = F.affine_grid(theta, x.size(), align_corners=False)\n",
    "        x = F.grid_sample(x, grid, align_corners=False)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Spatial transformer 1\n",
    "        x = self.stn1(x)\n",
    "\n",
    "        # CNN block 1\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.local_norm(x)\n",
    "\n",
    "        # Spatial transformer 2\n",
    "        x = self.stn2(x)\n",
    "\n",
    "        # CNN block 2\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.local_norm(x)\n",
    "\n",
    "        # Spatial transformer 3\n",
    "        x = self.stn3(x)\n",
    "\n",
    "        # CNN block 3\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.local_norm(x)\n",
    "\n",
    "        # Flatten the output for dense layers\n",
    "        x = x.view(-1, 350 * 6 * 6)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "6cpZCj8mowXh",
   "metadata": {
    "id": "6cpZCj8mowXh"
   },
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "id": "RIiC4t_jWeBV",
   "metadata": {
    "id": "RIiC4t_jWeBV",
    "ExecuteTime": {
     "end_time": "2024-07-04T17:18:40.696697Z",
     "start_time": "2024-07-04T17:18:40.689533Z"
    }
   },
   "source": [
    "def test_model(trained_model, test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        trained_model.eval()\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = trained_model(images)\n",
    "            softmax_outputs = F.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(softmax_outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = float(correct / total)\n",
    "    return test_accuracy"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "trtHqdVcWrbV",
   "metadata": {
    "id": "trtHqdVcWrbV"
   },
   "source": [
    "## Model setting"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T17:20:27.277693Z",
     "start_time": "2024-07-04T17:18:40.697408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# All the stats will be stored in ./runs and they will be readed by tensorboard\n",
    "# runs_arguments = [\n",
    "#     {'type': 'P', 'cnn': 'CNN'},\n",
    "#     {'type': 'P', 'cnn': 'CNN_ST'},\n",
    "#     {'type': 'W-FULL', 'cnn': 'CNN'},\n",
    "#     {'type': 'W-FULL', 'cnn': 'CNN_ST'},\n",
    "# ]\n",
    "for r_args in tqdm(runs_arguments):\n",
    "    dataset_type = r_args['type']\n",
    "    model_architecture = r_args['cnn']\n",
    "\n",
    "    # If type is P I want to test into W and vice versa\n",
    "    train_dir = weather_train_dir if dataset_type == 'P' else plain_train_dir\n",
    "    test_dir =  weather_test_dir if dataset_type == 'P' else plain_test_dir\n",
    "    train_dataset, test_dataset = load_train_test_dir(train_dir, test_dir)\n",
    "\n",
    "    # Compute class weights\n",
    "    train_size = len(train_dataset)\n",
    "    number_of_classes = len(train_dataset.classes)\n",
    "    train_samples_per_class = np.bincount(train_dataset.targets)\n",
    "    class_weights = (train_size / (number_of_classes * train_samples_per_class))\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "    hyperparams = adam_hyperparams\n",
    "    model_name = f'{dataset_type}_{model_architecture}_{hyperparams[\"opt\"]}'\n",
    "    writer = SummaryWriter(f'runs/{model_name}')\n",
    "\n",
    "    # Create DataLoader instances for training and validation\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=hyperparams[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=0)\n",
    "\n",
    "    # Model initialization\n",
    "    # model = CNN() if model_architecture == 'CNN' else CNN_ST()\n",
    "    model = torch.load(f'./models/trained_model_{model_name}_final.pth')\n",
    "    model.to(device)\n",
    "\n",
    "    ta = test_model(trained_model=model, test_loader=test_loader)\n",
    "    \n",
    "    plot_name = 'Training/Weather-full Accuracy' if dataset_type == 'P' else 'Training/Plain Accuracy'\n",
    "    writer.add_scalar(plot_name, ta)"
   ],
   "id": "SpS12uKCWvbE",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]/home/toldo/virtual_envs/Traffic-Sign-Recognition/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "100%|██████████| 4/4 [01:46<00:00, 26.64s/it]\n"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
